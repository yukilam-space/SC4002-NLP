{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d01884a1",
   "metadata": {},
   "source": [
    "# Part 0. Dataset Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99fbb161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff174062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install SpaCy. See the docs at https://spacy.io for more information.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# For tokenization\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m TEXT = \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspacy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_language\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43men_core_web_sm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_lengths\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# For multi-class classification labels\u001b[39;00m\n\u001b[32m      4\u001b[39m LABEL = data.LabelField()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SC4002-NLP\\venv\\Lib\\site-packages\\torchtext\\data\\field.py:163\u001b[39m, in \u001b[36mField.__init__\u001b[39m\u001b[34m(self, sequential, use_vocab, init_token, eos_token, fix_length, dtype, preprocessing, postprocessing, lower, tokenize, tokenizer_language, include_lengths, batch_first, pad_token, unk_token, pad_first, truncate_first, stop_words, is_target)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# store params to construct tokenizer for serialization\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m# in case the tokenizer isn't picklable (e.g. spacy)\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer_args = (tokenize, tokenizer_language)\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenize = \u001b[43mget_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_language\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;28mself\u001b[39m.include_lengths = include_lengths\n\u001b[32m    165\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_first = batch_first\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SC4002-NLP\\venv\\Lib\\site-packages\\torchtext\\data\\utils.py:113\u001b[39m, in \u001b[36mget_tokenizer\u001b[39m\u001b[34m(tokenizer, language)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer == \u001b[33m\"\u001b[39m\u001b[33mspacy\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n\u001b[32m    114\u001b[39m         spacy = spacy.load(language)\n\u001b[32m    115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m partial(_spacy_tokenize, spacy=spacy)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "\n",
    "# For tokenization\n",
    "TEXT = data.Field(tokenize=\"spacy\", tokenizer_language=\"en_core_web_sm\", include_lengths=True)\n",
    "# For multi-class classification labels\n",
    "LABEL = data.LabelField()\n",
    "# Load the TREC dataset\n",
    "train_data, test_data = datasets.TREC.splits(TEXT, LABEL, fine_grained=False)\n",
    "# The datasets.TREC class in torchtext is hardcoded to download and parse the full training set (Set 5) and the TREC 10 test set.\n",
    "# You don’t specify which subset to use — it always loads the largest one by default.\n",
    "print(vars(train_data.examples[0]))\n",
    "print(\"Train size:\", len(train_data.examples))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031eecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some examples\n",
    "\n",
    "for i in range(10):\n",
    "    random_index = random.randint(0,len(train_data))\n",
    "    print(' '.join(train_data.examples[random_index].text), train_data.examples[random_index].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ece6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "trains_data, valid_data = train_data.split(split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7925cf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View multiple samples\n",
    "for i in range(3):  # adjust the range as needed\n",
    "    print(vars(trains_data.examples[i]))\n",
    "    \n",
    "\n",
    "print(\"Train size (after spliting):\", len(trains_data.examples))\n",
    "print(\"Validation size:\", len(valid_data.examples))\n",
    "print(\"Test size:\", len(test_data.examples))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f7ecd",
   "metadata": {},
   "source": [
    "#  Part 1. Preparing Word Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8afb989",
   "metadata": {},
   "source": [
    "##  Question 1. Word Embedding (glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3ae8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and Vocabulary with GloVe\n",
    "TEXT.build_vocab(trains_data, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(trains_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1245db7",
   "metadata": {},
   "source": [
    "(a) What is the size of the vocabulary formed from your training data according to your tokeniza\n",
    "tion method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572996de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Check vocabulary size\n",
    "print(f\"Size of TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Size of LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbec89a",
   "metadata": {},
   "source": [
    "(b) We use OOV (out-of-vocabulary) to refer to those words appeared in the training data but\n",
    " not in the Word2vec (or Glove) dictionary. How many OOV words exist in your training data?\n",
    " What is the number of OOV words for each topic category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd48d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prepare GloVe vocabulary\n",
    "glove_vocab = set(TEXT.vocab.stoi.keys())\n",
    "\n",
    "# Track OOV words per label\n",
    "oov_words_per_label = defaultdict(set)\n",
    "# Track all OOV words in training data\n",
    "all_oov_words = set()\n",
    "\n",
    "# Iterate over training data\n",
    "for example in trains_data:\n",
    "    label = example.label\n",
    "    # Normalize words: lowercase + remove punctuation\n",
    "    words = [w.lower().strip(string.punctuation) for w in example.text]\n",
    "    for w in words:\n",
    "        if w not in glove_vocab:\n",
    "            oov_words_per_label[label].add(w)\n",
    "            all_oov_words.add(w)\n",
    "\n",
    "# Print unique OOV per label\n",
    "print(\"Unique OOV words per label:\")\n",
    "for label, words in oov_words_per_label.items():\n",
    "    print(f\"{label}: {len(words)}\")\n",
    "\n",
    "# Total unique OOV in training data\n",
    "print(f\"\\nTotal unique OOV words in training data: {len(all_oov_words)}\")\n",
    "\n",
    "# Optional: verify sum matches total\n",
    "sum_per_label = sum(len(words) for words in oov_words_per_label.values())\n",
    "print(f\"Sum of unique OOVs across labels (with overlaps): {sum_per_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4fae04",
   "metadata": {},
   "source": [
    " (c) The existence of the OOV words is one of the well-known limitations of Word2vec (or Glove).\n",
    " Without using any transformer-based language models (e.g., BERT, GPT, T5), what do you\n",
    " think is the best strategy to mitigate such limitation? Implement your solution in your source\n",
    " code. Show the corresponding code snippet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62713acb",
   "metadata": {},
   "source": [
    "the best strategy: subword averaging + random for frequent OOVs + <unk> for very rare OOVs.\n",
    "1. Uses GloVe embeddings for known words.\n",
    "\n",
    "2. For OOV words, tries subword (character n-gram) averaging.\n",
    "Break the OOV word into character n-grams (e.g., 3–6 letters).\n",
    "Take embeddings of subwords that exist in the pretrained vocabulary.\n",
    "Average those subword embeddings to represent the OOV word.\n",
    "\n",
    "3. Falls back to random vector for frequent OOVs — lets the model learn their meaning\n",
    "Helps avoid errors, but it does not carry semantic information\n",
    "\n",
    "4. <unk> vector for rare OOVs — prevents noise from too many low-frequency random vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfe1f3b",
   "metadata": {},
   "source": [
    "Use Subword Information (FastText-like)\n",
    "\n",
    "Pros:\n",
    "- Handles morphological variations (e.g., runner → run) and rare words well.\n",
    "\n",
    "- Uses known subwords to create a vector → better semantic representation than random vectors.\n",
    "\n",
    "- No need for additional large datasets; can work with existing embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ce874",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_dim = 100\n",
    "vocab = TEXT.vocab\n",
    "vectors = vocab.vectors.clone()\n",
    "glove_vocab = set(vocab.stoi.keys())\n",
    "\n",
    "# Subword parameters\n",
    "ngram_min = 3\n",
    "ngram_max = 6\n",
    "\n",
    "# Count word frequency in training data\n",
    "word_counter = Counter()\n",
    "for example in trains_data:\n",
    "    word_counter.update([w.lower().strip(string.punctuation) for w in example.text])\n",
    "\n",
    "# Threshold to consider a word “frequent” (adjustable)\n",
    "freq_threshold = 3\n",
    "\n",
    "# <unk> vector\n",
    "unk_vector = torch.zeros(embedding_dim)\n",
    "\n",
    "# Counters\n",
    "subword_count = 0\n",
    "random_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "def get_subwords(word, n_min=3, n_max=6):\n",
    "    word = f\"<{word.lower()}>\"\n",
    "    subwords = []\n",
    "    for n in range(n_min, n_max+1):\n",
    "        subwords += [word[i:i+n] for i in range(len(word)-n+1)]\n",
    "    return subwords\n",
    "\n",
    "def get_word_vector(word):\n",
    "    global subword_count, random_count, unk_count\n",
    "    w_clean = word.lower().strip(string.punctuation)\n",
    "    \n",
    "    if w_clean in glove_vocab:\n",
    "        return vectors[vocab.stoi[w_clean]]\n",
    "    \n",
    "    # Subword averaging\n",
    "    subwords = get_subwords(w_clean, ngram_min, ngram_max)\n",
    "    subword_vecs = [vectors[vocab.stoi[sg]] for sg in subwords if sg in glove_vocab]\n",
    "    if subword_vecs:\n",
    "        subword_count += 1\n",
    "        return torch.stack(subword_vecs).mean(0)\n",
    "    \n",
    "    # Random vector for frequent OOVs\n",
    "    if word_counter[w_clean] >= freq_threshold:\n",
    "        random_count += 1\n",
    "        return torch.randn(embedding_dim)\n",
    "    \n",
    "    # <unk> for rare OOVs\n",
    "    unk_count += 1\n",
    "    return unk_vector\n",
    "\n",
    "# Build embedding matrix\n",
    "embedding_matrix = {}\n",
    "for example in trains_data:\n",
    "    for w in example.text:\n",
    "        if w not in embedding_matrix:\n",
    "            embedding_matrix[w] = get_word_vector(w)\n",
    "\n",
    "# Report statistics\n",
    "total_oov = subword_count + random_count + unk_count\n",
    "print(f\"Total OOV words in training data handled: {total_oov}\")\n",
    "print(f\" - Handled by subword averaging: {subword_count}\")\n",
    "print(f\" - Handled by random vectors: {random_count}\")\n",
    "print(f\" - Handled by <unk> vector: {unk_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946ab715",
   "metadata": {},
   "source": [
    " (d) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f32ad0d",
   "metadata": {},
   "source": [
    "(i)  Select the 20 most frequent words from each topic category in the training set (removing\n",
    " stopwords if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c319411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "embedding_dim = 100\n",
    "vocab = TEXT.vocab\n",
    "vectors = vocab.vectors.clone()\n",
    "glove_vocab = set(vocab.stoi.keys())\n",
    "\n",
    "# Count word frequency per label\n",
    "label_word_counter = defaultdict(Counter)\n",
    "\n",
    "for example in trains_data:\n",
    "    label = example.label\n",
    "    words = [w.lower().strip(string.punctuation) for w in example.text]\n",
    "    words = [w for w in words if w and w not in stop_words]  # remove empty strings & stopwords\n",
    "    label_word_counter[label].update(words)\n",
    "\n",
    "# Select top 20 words per label\n",
    "top_words_per_label = {}\n",
    "for label, counter in label_word_counter.items():\n",
    "    top_words = [w for w, _ in counter.most_common(20)]\n",
    "    top_words_per_label[label] = top_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e8c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_words_per_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc5ddec",
   "metadata": {},
   "source": [
    "(ii) Retrieve their pretrained embeddings (from Word2Vec or GloVe).\n",
    " Project these embeddings into 2D space (using e.g., t-SNE or Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00462fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_words = []\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "\n",
    "for label, words in top_words_per_label.items():\n",
    "    for w in words:\n",
    "        w_clean = w.lower().strip(string.punctuation)\n",
    "        if not w_clean:\n",
    "            continue\n",
    "        if w_clean not in embedding_matrix:\n",
    "            continue  # skip words not in embedding_matrix\n",
    "        emb = embedding_matrix[w_clean]\n",
    "        all_words.append(w_clean)\n",
    "        all_embeddings.append(emb.numpy())\n",
    "        all_labels.append(label)\n",
    "\n",
    "all_embeddings = torch.tensor(all_embeddings).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bed08fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "emb_2d = tsne.fit_transform(all_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bc597e",
   "metadata": {},
   "source": [
    "(iii)  Plot the points in a scatter plot, color-coded by their topic category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36bde61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "# Assign a color per label\n",
    "unique_labels = list(top_words_per_label.keys())\n",
    "palette = sns.color_palette(\"hls\", len(unique_labels))\n",
    "label_color = {label: palette[i] for i, label in enumerate(unique_labels)}\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, label in enumerate(unique_labels):\n",
    "    idx = [j for j, l in enumerate(all_labels) if l == label]\n",
    "    plt.scatter(emb_2d[idx,0], emb_2d[idx,1], label=label, color=palette[i], s=100)\n",
    "\n",
    "# Annotate words\n",
    "for i, word in enumerate(all_words):\n",
    "    plt.text(emb_2d[i,0]+0.2, emb_2d[i,1]+0.2, word, fontsize=9)\n",
    "\n",
    "plt.title(\"t-SNE of Top 20 Words per Topic Category (GloVe embeddings)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
