{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading train_5500.label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\trec\\train_5500.label: 100%|██████████| 336k/336k [00:01<00:00, 243kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading TREC_10.label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\trec\\TREC_10.label: 100%|██████████| 23.4k/23.4k [00:00<00:00, 92.5kB/s]\n"
     ]
    }
   ],
   "source": [
    "# For tokenization\n",
    "TEXT = data.Field(tokenize='spacy',\n",
    "    tokenizer_language='en_core_web_sm',\n",
    "    batch_first=True,\n",
    "    fix_length=50,\n",
    "    lower=True,\n",
    "    unk_token='<unk>',\n",
    "    pad_token='<pad>'\n",
    "    )\n",
    "\n",
    "# For multi-class classification labels\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "# Load the TREC dataset\n",
    "train_data, test_data = datasets.TREC.splits(TEXT, LABEL, fine_grained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC', 'ABBR', 'DESC', 'HUM', 'NUM', 'ENTY'}\n"
     ]
    }
   ],
   "source": [
    "label_set = set()\n",
    "for i in train_data:\n",
    "    label_set.add(i.label)\n",
    "\n",
    "print(label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.6B.zip: 862MB [06:20, 2.27MB/s]                               \n",
      "100%|█████████▉| 399999/400000 [00:36<00:00, 10980.60it/s]\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "vocab = TEXT.vocab\n",
    "vectors = vocab.vectors.clone()\n",
    "glove_vocab = set(vocab.stoi.keys())\n",
    "\n",
    "# Subword parameters\n",
    "ngram_min = 3\n",
    "ngram_max = 6\n",
    "\n",
    "# Count word frequency in training data\n",
    "word_counter = Counter()\n",
    "for example in train_data:\n",
    "    word_counter.update([w.lower().strip(string.punctuation) for w in example.text])\n",
    "\n",
    "# Threshold to consider a word “frequent” (adjustable)\n",
    "freq_threshold = 3\n",
    "\n",
    "# <unk> vector\n",
    "unk_vector = torch.zeros(embedding_dim)\n",
    "\n",
    "def get_subwords(word, n_min=3, n_max=6):\n",
    "    word = f\"<{word.lower()}>\"\n",
    "    subwords = []\n",
    "    for n in range(n_min, n_max+1):\n",
    "        subwords += [word[i:i+n] for i in range(len(word)-n+1)]\n",
    "    return subwords\n",
    "\n",
    "def get_word_vector(word):\n",
    "    w_clean = word.lower().strip(string.punctuation)\n",
    "    \n",
    "    if w_clean in glove_vocab:\n",
    "        return vectors[vocab.stoi[w_clean]]\n",
    "    \n",
    "    # Subword averaging\n",
    "    subwords = get_subwords(w_clean, ngram_min, ngram_max)\n",
    "    subword_vecs = [vectors[vocab.stoi[sg]] for sg in subwords if sg in glove_vocab]\n",
    "    if subword_vecs:\n",
    "        return torch.stack(subword_vecs).mean(0)\n",
    "    \n",
    "    # Random vector for frequent OOVs\n",
    "    if word_counter[w_clean] >= freq_threshold:\n",
    "        return torch.randn(embedding_dim)\n",
    "    \n",
    "    # <unk> for rare OOVs\n",
    "    return unk_vector\n",
    "\n",
    "# Build embedding matrix\n",
    "embedding_matrix = {}\n",
    "for example in train_data:\n",
    "    for w in example.text:\n",
    "        if w not in embedding_matrix:\n",
    "            embedding_matrix[w] = get_word_vector(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer created with shape: torch.Size([8536, 100])\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab = TEXT.vocab\n",
    "\n",
    "# Create tensor for nn.Embedding\n",
    "embedding_matrix_tensor = torch.zeros(len(vocab), embedding_dim)\n",
    "for word, idx in vocab.stoi.items():\n",
    "    if word in embedding_matrix:\n",
    "        embedding_matrix_tensor[idx] = embedding_matrix[word]\n",
    "\n",
    "# Create embedding layer (learnable)\n",
    "embedding_layer = nn.Embedding.from_pretrained(embedding_matrix_tensor, freeze=False)\n",
    "print(\"Embedding layer created with shape:\", embedding_layer.weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class TrecDataset(Dataset):\n",
    "    def __init__(self, examples, vocab, label_vocab, max_len=50):\n",
    "        self.vocab = vocab\n",
    "        self.label_vocab = label_vocab\n",
    "        self.max_len = max_len\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        for ex in examples:\n",
    "            indices = [vocab.stoi[w] for w in ex.text if w in vocab.stoi]\n",
    "            # Pad/truncate\n",
    "            if len(indices) < max_len:\n",
    "                indices += [vocab.stoi['<pad>']] * (max_len - len(indices))\n",
    "            else:\n",
    "                indices = indices[:max_len]\n",
    "            self.data.append(indices)\n",
    "            self.labels.append(label_vocab.stoi[ex.label])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx]), torch.tensor(self.labels[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim, output_dim, rnn_type='RNN', dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if rnn_type == 'RNN':\n",
    "            self.rnn = nn.RNN(embedding_layer.embedding_dim, hidden_dim, batch_first=True)\n",
    "        elif rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_layer.embedding_dim, hidden_dim, batch_first=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_layer.embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        if isinstance(hidden, tuple):  # LSTM returns (h, c)\n",
    "            hidden = hidden[0]\n",
    "        sentence_repr = hidden.squeeze(0)\n",
    "        sentence_repr = self.dropout(sentence_repr)\n",
    "        logits = self.fc(sentence_repr)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_x)\n",
    "        loss = criterion(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            logits = model(batch_x)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Learning Rate: 0.001 ===\n",
      "\n",
      "--- Batch Size: 16 ---\n",
      "\n",
      ">> Fold 1 / 5\n",
      "Epoch 1/50 - Val Acc: 0.2081\n",
      "Epoch 2/50 - Val Acc: 0.2255\n",
      "Epoch 3/50 - Val Acc: 0.2200\n",
      "Epoch 4/50 - Val Acc: 0.2200\n",
      "Epoch 5/50 - Val Acc: 0.2081\n",
      "Epoch 6/50 - Val Acc: 0.2255\n",
      "Epoch 7/50 - Val Acc: 0.2273\n",
      "Epoch 8/50 - Val Acc: 0.2081\n",
      "Epoch 9/50 - Val Acc: 0.2200\n",
      "Epoch 10/50 - Val Acc: 0.2255\n",
      "Epoch 11/50 - Val Acc: 0.2200\n",
      "Epoch 12/50 - Val Acc: 0.2255\n",
      "Early stopping at epoch 12 for fold\n",
      "Best Val Acc for Fold 1: 0.2273\n",
      "\n",
      ">> Fold 2 / 5\n",
      "Epoch 1/50 - Val Acc: 0.2328\n",
      "Epoch 2/50 - Val Acc: 0.2420\n",
      "Epoch 3/50 - Val Acc: 0.2420\n",
      "Epoch 4/50 - Val Acc: 0.2026\n",
      "Epoch 5/50 - Val Acc: 0.2420\n",
      "Epoch 6/50 - Val Acc: 0.2026\n",
      "Epoch 7/50 - Val Acc: 0.2026\n",
      "Early stopping at epoch 7 for fold\n",
      "Best Val Acc for Fold 2: 0.2420\n",
      "\n",
      ">> Fold 3 / 5\n",
      "Epoch 1/50 - Val Acc: 0.2202\n",
      "Epoch 2/50 - Val Acc: 0.2202\n",
      "Epoch 3/50 - Val Acc: 0.1954\n",
      "Epoch 4/50 - Val Acc: 0.2202\n",
      "Epoch 5/50 - Val Acc: 0.2202\n",
      "Epoch 6/50 - Val Acc: 0.2202\n",
      "Early stopping at epoch 6 for fold\n",
      "Best Val Acc for Fold 3: 0.2202\n",
      "\n",
      ">> Fold 4 / 5\n",
      "Epoch 1/50 - Val Acc: 0.2211\n",
      "Epoch 2/50 - Val Acc: 0.2394\n",
      "Epoch 3/50 - Val Acc: 0.2211\n",
      "Epoch 4/50 - Val Acc: 0.2394\n",
      "Epoch 5/50 - Val Acc: 0.2211\n",
      "Epoch 6/50 - Val Acc: 0.2174\n",
      "Epoch 7/50 - Val Acc: 0.2211\n",
      "Early stopping at epoch 7 for fold\n",
      "Best Val Acc for Fold 4: 0.2394\n",
      "\n",
      ">> Fold 5 / 5\n",
      "Epoch 1/50 - Val Acc: 0.2165\n",
      "Epoch 2/50 - Val Acc: 0.2376\n",
      "Epoch 3/50 - Val Acc: 0.2376\n",
      "Epoch 4/50 - Val Acc: 0.2119\n",
      "Epoch 5/50 - Val Acc: 0.2165\n",
      "Epoch 6/50 - Val Acc: 0.2376\n",
      "Epoch 7/50 - Val Acc: 0.2119\n",
      "Early stopping at epoch 7 for fold\n",
      "Best Val Acc for Fold 5: 0.2376\n",
      "LR=0.001, BS=16, Avg CV Acc=0.2333\n",
      "\n",
      "--- Batch Size: 32 ---\n",
      "\n",
      ">> Fold 1 / 5\n",
      "Epoch 1/50 - Val Acc: 0.2081\n",
      "Epoch 2/50 - Val Acc: 0.2081\n",
      "Epoch 3/50 - Val Acc: 0.2081\n",
      "Epoch 4/50 - Val Acc: 0.2200\n",
      "Epoch 5/50 - Val Acc: 0.2255\n",
      "Epoch 6/50 - Val Acc: 0.2200\n",
      "Epoch 7/50 - Val Acc: 0.2081\n",
      "Epoch 8/50 - Val Acc: 0.2200\n",
      "Epoch 9/50 - Val Acc: 0.2081\n",
      "Epoch 10/50 - Val Acc: 0.2200\n",
      "Early stopping at epoch 10 for fold\n",
      "Best Val Acc for Fold 1: 0.2255\n",
      "\n",
      ">> Fold 2 / 5\n",
      "Epoch 1/50 - Val Acc: 0.2026\n",
      "Epoch 2/50 - Val Acc: 0.2026\n",
      "Epoch 3/50 - Val Acc: 0.2420\n",
      "Epoch 4/50 - Val Acc: 0.2420\n",
      "Epoch 5/50 - Val Acc: 0.2026\n",
      "Epoch 6/50 - Val Acc: 0.2420\n",
      "Epoch 7/50 - Val Acc: 0.2420\n",
      "Epoch 8/50 - Val Acc: 0.2026\n",
      "Early stopping at epoch 8 for fold\n",
      "Best Val Acc for Fold 2: 0.2420\n",
      "\n",
      ">> Fold 3 / 5\n",
      "Epoch 1/50 - Val Acc: 0.1954\n",
      "Epoch 2/50 - Val Acc: 0.1954\n",
      "Epoch 3/50 - Val Acc: 0.2431\n",
      "Epoch 4/50 - Val Acc: 0.2202\n",
      "Epoch 5/50 - Val Acc: 0.2202\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m best_fold_acc = \u001b[32m0\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     val_acc = evaluate(model, val_loader)\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion)\u001b[39m\n\u001b[32m      5\u001b[39m batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n\u001b[32m      6\u001b[39m optimizer.zero_grad()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m loss = criterion(logits, batch_y)\n\u001b[32m      9\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SC4002-NLP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SC4002-NLP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mRNNClassifier.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     15\u001b[39m     embedded = \u001b[38;5;28mself\u001b[39m.embedding(x)  \u001b[38;5;66;03m# (batch, seq_len, embed_dim)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     output, hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(hidden, \u001b[38;5;28mtuple\u001b[39m):  \u001b[38;5;66;03m# LSTM returns (h, c)\u001b[39;00m\n\u001b[32m     18\u001b[39m         hidden = hidden[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SC4002-NLP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SC4002-NLP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SC4002-NLP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:718\u001b[39m, in \u001b[36mRNN.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mRNN_TANH\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrnn_tanh\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    722\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    729\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    730\u001b[39m         result = _VF.rnn_relu(\n\u001b[32m    731\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    732\u001b[39m             hx,\n\u001b[32m   (...)\u001b[39m\u001b[32m    739\u001b[39m             \u001b[38;5;28mself\u001b[39m.batch_first,\n\u001b[32m    740\u001b[39m         )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "learning_rates = [1e-3, 5e-4, 1e-4, 5e-3, 1e-2]\n",
    "batch_sizes = [16, 32, 64, 128, 256]\n",
    "max_len = 50\n",
    "hidden_dim = 128\n",
    "num_classes = len(LABEL.vocab)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_acc = 0\n",
    "best_params = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\n=== Testing Learning Rate: {lr} ===\")\n",
    "    for bs in batch_sizes:\n",
    "        print(f\"\\n--- Batch Size: {bs} ---\")\n",
    "        fold_acc = []\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(kf.split(train_data.examples)):\n",
    "            print(f\"\\n>> Fold {fold_idx+1} / {kf.n_splits}\")\n",
    "            # Prepare datasets\n",
    "            train_subset = [train_data.examples[i] for i in train_idx]\n",
    "            val_subset = [train_data.examples[i] for i in val_idx]\n",
    "\n",
    "            train_dataset = TrecDataset(train_subset, vocab, LABEL.vocab, max_len)\n",
    "            val_dataset = TrecDataset(val_subset, vocab, LABEL.vocab, max_len)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=bs)\n",
    "\n",
    "            # Initialize model\n",
    "            model = RNNClassifier(embedding_layer, hidden_dim, num_classes).to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            # --- Training per fold ---\n",
    "            num_epochs = 50\n",
    "            patience = 5\n",
    "            epochs_no_improve = 0\n",
    "            best_fold_acc = 0\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "                val_acc = evaluate(model, val_loader)\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} - Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "                \n",
    "                # Early stopping\n",
    "                if val_acc > best_fold_acc:\n",
    "                    best_fold_acc = val_acc\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    if epochs_no_improve >= patience:\n",
    "                        print(f\"Early stopping at epoch {epoch+1} for fold\")\n",
    "                        break\n",
    "            print(f\"Best Val Acc for Fold {fold_idx+1}: {best_fold_acc:.4f}\")\n",
    "            fold_acc.append(best_fold_acc)  # record best val acc for this fold\n",
    "\n",
    "        avg_acc = np.mean(fold_acc)\n",
    "        print(f\"LR={lr}, BS={bs}, Avg CV Acc={avg_acc:.4f}\")\n",
    "        if avg_acc > best_acc:\n",
    "            best_acc = avg_acc\n",
    "            best_params = {'lr': lr, 'batch_size': bs}\n",
    "\n",
    "print(f\"\\n=== Best Hyperparameters Found ===\")\n",
    "print(f\"Learning Rate: {best_params['lr']}, Batch Size: {best_params['batch_size']}\")\n",
    "print(f\"CV Accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
