{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading train_5500.label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\trec\\train_5500.label: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 336k/336k [00:01<00:00, 276kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading TREC_10.label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\trec\\TREC_10.label: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23.4k/23.4k [00:00<00:00, 107kB/s] \n"
     ]
    }
   ],
   "source": [
    "# For tokenization\n",
    "TEXT = data.Field(tokenize='spacy',\n",
    "    tokenizer_language='en_core_web_sm',\n",
    "    batch_first=True,\n",
    "    fix_length=50,\n",
    "    lower=True,\n",
    "    unk_token='<unk>',\n",
    "    pad_token='<pad>'\n",
    "    )\n",
    "\n",
    "# For multi-class classification labels\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "# Load the TREC dataset\n",
    "train_data, test_data = datasets.TREC.splits(TEXT, LABEL, fine_grained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ABBR', 'LOC', 'ENTY', 'NUM', 'DESC', 'HUM'}\n"
     ]
    }
   ],
   "source": [
    "label_set = set()\n",
    "for i in train_data:\n",
    "    label_set.add(i.label)\n",
    "\n",
    "print(label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.6B.zip: 862MB [06:11, 2.32MB/s]                               \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 399999/400000 [00:10<00:00, 37352.49it/s]\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "vocab = TEXT.vocab\n",
    "vectors = vocab.vectors.clone()\n",
    "glove_vocab = set(vocab.stoi.keys())\n",
    "\n",
    "# Subword parameters\n",
    "ngram_min = 3\n",
    "ngram_max = 6\n",
    "\n",
    "# Count word frequency in training data\n",
    "word_counter = Counter()\n",
    "for example in train_data:\n",
    "    word_counter.update([w.lower().strip(string.punctuation) for w in example.text])\n",
    "\n",
    "# Threshold to consider a word ‚Äúfrequent‚Äù (adjustable)\n",
    "freq_threshold = 3\n",
    "\n",
    "# <unk> vector\n",
    "unk_vector = torch.zeros(embedding_dim)\n",
    "\n",
    "def get_subwords(word, n_min=3, n_max=6):\n",
    "    word = f\"<{word.lower()}>\"\n",
    "    subwords = []\n",
    "    for n in range(n_min, n_max+1):\n",
    "        subwords += [word[i:i+n] for i in range(len(word)-n+1)]\n",
    "    return subwords\n",
    "\n",
    "def get_word_vector(word):\n",
    "    w_clean = word.lower().strip(string.punctuation)\n",
    "    \n",
    "    if w_clean in glove_vocab:\n",
    "        return vectors[vocab.stoi[w_clean]]\n",
    "    \n",
    "    # Subword averaging\n",
    "    subwords = get_subwords(w_clean, ngram_min, ngram_max)\n",
    "    subword_vecs = [vectors[vocab.stoi[sg]] for sg in subwords if sg in glove_vocab]\n",
    "    if subword_vecs:\n",
    "        return torch.stack(subword_vecs).mean(0)\n",
    "    \n",
    "    # Random vector for frequent OOVs\n",
    "    if word_counter[w_clean] >= freq_threshold:\n",
    "        return torch.randn(embedding_dim)\n",
    "    \n",
    "    # <unk> for rare OOVs\n",
    "    return unk_vector\n",
    "\n",
    "# Build embedding matrix\n",
    "embedding_matrix = {}\n",
    "for example in train_data:\n",
    "    for w in example.text:\n",
    "        if w not in embedding_matrix:\n",
    "            embedding_matrix[w] = get_word_vector(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer created with shape: torch.Size([8536, 100])\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab = TEXT.vocab\n",
    "\n",
    "# Create tensor for nn.Embedding\n",
    "embedding_matrix_tensor = torch.zeros(len(vocab), embedding_dim)\n",
    "for word, idx in vocab.stoi.items():\n",
    "    if word in embedding_matrix:\n",
    "        embedding_matrix_tensor[idx] = embedding_matrix[word]\n",
    "\n",
    "# Create embedding layer (learnable)\n",
    "embedding_layer = nn.Embedding.from_pretrained(embedding_matrix_tensor, freeze=False)\n",
    "print(\"Embedding layer created with shape:\", embedding_layer.weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, copy, pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4416, Valid: 491, Test: 500\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1. Setup & Config\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume TEXT, LABEL, train_data, test_data, embedding_layer already exist\n",
    "train_data, valid_data = train_data.split(split_ratio=0.9, random_state=random.seed(SEED))\n",
    "print(f\"Train: {len(train_data)}, Valid: {len(valid_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "embedding_dim = embedding_layer.weight.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Simple RNN classifier (tanh)\n",
    "# -----------------------------\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim, num_layers, bidirectional, dropout, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        emb_dim = self.embedding.weight.size(1)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            nonlinearity=\"tanh\",      # simple RNN\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), num_classes)\n",
    "\n",
    "    def forward(self, text):\n",
    "        x = self.embedding(text)                     # [B, T, E]\n",
    "        outputs, hidden = self.rnn(x)                # hidden: [L*D, B, H]\n",
    "        if self.rnn.bidirectional:\n",
    "            last = torch.cat((hidden[-2], hidden[-1]), dim=1)  # [B, 2H]\n",
    "        else:\n",
    "            last = hidden[-1]                                   # [B, H]\n",
    "        return self.fc(self.dropout(last))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Train/eval 1 epoch\n",
    "# -----------------------------\n",
    "def epoch_run(iterator, model, criterion, optimizer=None):\n",
    "    model.train(optimizer is not None)\n",
    "    tot_loss, tot_correct, tot_count = 0.0, 0, 0\n",
    "    for batch in iterator:\n",
    "        text, labels = batch.text, batch.label\n",
    "        logits = model(text)\n",
    "        loss = criterion(logits, labels)\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)  # RNNs can explode\n",
    "            optimizer.step()\n",
    "        tot_loss += loss.item() * labels.size(0)\n",
    "        tot_correct += (logits.argmax(1) == labels).sum().item()\n",
    "        tot_count += labels.size(0)\n",
    "    return tot_loss / max(tot_count,1), tot_correct / max(tot_count,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing BATCH=32, LR=0.0005\n",
      "Epoch 01: train_acc=0.6003, val_acc=0.7943\n",
      "Epoch 02: train_acc=0.8252, val_acc=0.8411\n",
      "Epoch 03: train_acc=0.8705, val_acc=0.8839\n",
      "Epoch 04: train_acc=0.9017, val_acc=0.9063\n",
      "Epoch 05: train_acc=0.9237, val_acc=0.8900\n",
      "Epoch 06: train_acc=0.9386, val_acc=0.9022\n",
      "Early stopping\n",
      "\n",
      "üîç Testing BATCH=32, LR=0.001\n",
      "Epoch 01: train_acc=0.6771, val_acc=0.8534\n",
      "Epoch 02: train_acc=0.8490, val_acc=0.8778\n",
      "Epoch 03: train_acc=0.8961, val_acc=0.8982\n",
      "Epoch 04: train_acc=0.9214, val_acc=0.9124\n",
      "Epoch 05: train_acc=0.9450, val_acc=0.9022\n",
      "Epoch 06: train_acc=0.9656, val_acc=0.8982\n",
      "Early stopping\n",
      "\n",
      "üîç Testing BATCH=32, LR=0.002\n",
      "Epoch 01: train_acc=0.7124, val_acc=0.8228\n",
      "Epoch 02: train_acc=0.8594, val_acc=0.8676\n",
      "Epoch 03: train_acc=0.9128, val_acc=0.8656\n",
      "Epoch 04: train_acc=0.9402, val_acc=0.8574\n",
      "Early stopping\n",
      "\n",
      "üîç Testing BATCH=32, LR=0.003\n",
      "Epoch 01: train_acc=0.7251, val_acc=0.8513\n",
      "Epoch 02: train_acc=0.8449, val_acc=0.8493\n",
      "Epoch 03: train_acc=0.9096, val_acc=0.8676\n",
      "Epoch 04: train_acc=0.9126, val_acc=0.8676\n",
      "Epoch 05: train_acc=0.9656, val_acc=0.8839\n",
      "Epoch 06: train_acc=0.9801, val_acc=0.8697\n",
      "Epoch 07: train_acc=0.9835, val_acc=0.8758\n",
      "Early stopping\n",
      "\n",
      "üîç Testing BATCH=64, LR=0.0005\n",
      "Epoch 01: train_acc=0.5159, val_acc=0.6538\n",
      "Epoch 02: train_acc=0.7726, val_acc=0.8289\n",
      "Epoch 03: train_acc=0.8435, val_acc=0.8615\n",
      "Epoch 04: train_acc=0.8768, val_acc=0.8798\n",
      "Epoch 05: train_acc=0.9013, val_acc=0.8839\n",
      "Epoch 06: train_acc=0.9196, val_acc=0.8900\n",
      "Epoch 07: train_acc=0.9357, val_acc=0.8941\n",
      "Epoch 08: train_acc=0.9459, val_acc=0.9084\n",
      "Epoch 09: train_acc=0.9547, val_acc=0.8921\n",
      "Epoch 10: train_acc=0.9620, val_acc=0.8880\n",
      "Early stopping\n",
      "\n",
      "üîç Testing BATCH=64, LR=0.001\n",
      "Epoch 01: train_acc=0.6125, val_acc=0.7984\n",
      "Epoch 02: train_acc=0.8200, val_acc=0.8187\n",
      "Epoch 03: train_acc=0.8693, val_acc=0.8798\n",
      "Epoch 04: train_acc=0.9119, val_acc=0.8982\n",
      "Epoch 05: train_acc=0.9316, val_acc=0.8798\n",
      "Epoch 06: train_acc=0.9534, val_acc=0.8839\n",
      "Early stopping\n",
      "\n",
      "üîç Testing BATCH=64, LR=0.002\n",
      "Epoch 01: train_acc=0.6712, val_acc=0.8167\n",
      "Epoch 02: train_acc=0.8490, val_acc=0.8432\n",
      "Epoch 03: train_acc=0.8918, val_acc=0.8900\n",
      "Epoch 04: train_acc=0.9296, val_acc=0.8921\n",
      "Epoch 05: train_acc=0.9380, val_acc=0.8758\n",
      "Epoch 06: train_acc=0.9545, val_acc=0.8697\n",
      "Early stopping\n",
      "\n",
      "üîç Testing BATCH=64, LR=0.003\n",
      "Epoch 01: train_acc=0.6789, val_acc=0.8086\n",
      "Epoch 02: train_acc=0.8347, val_acc=0.8371\n",
      "Epoch 03: train_acc=0.9031, val_acc=0.8615\n",
      "Epoch 04: train_acc=0.9468, val_acc=0.8859\n",
      "Epoch 05: train_acc=0.9629, val_acc=0.8758\n",
      "Epoch 06: train_acc=0.9819, val_acc=0.8798\n",
      "Early stopping\n",
      "\n",
      "üîç Testing BATCH=128, LR=0.0005\n",
      "Epoch 01: train_acc=0.4287, val_acc=0.5255\n",
      "Epoch 02: train_acc=0.6606, val_acc=0.7699\n",
      "Epoch 03: train_acc=0.7702, val_acc=0.8187\n",
      "Epoch 04: train_acc=0.8277, val_acc=0.8350\n",
      "Epoch 05: train_acc=0.8551, val_acc=0.8778\n",
      "Epoch 06: train_acc=0.8807, val_acc=0.8839\n",
      "Epoch 07: train_acc=0.9001, val_acc=0.8839\n",
      "Epoch 08: train_acc=0.9169, val_acc=0.8982\n",
      "Epoch 09: train_acc=0.9287, val_acc=0.8921\n",
      "Epoch 10: train_acc=0.9361, val_acc=0.9104\n",
      "Epoch 11: train_acc=0.9504, val_acc=0.9063\n",
      "Epoch 12: train_acc=0.9567, val_acc=0.9022\n",
      "Early stopping\n",
      "\n",
      "üîç Testing BATCH=128, LR=0.001\n",
      "Epoch 01: train_acc=0.5188, val_acc=0.7393\n",
      "Epoch 02: train_acc=0.7584, val_acc=0.8289\n",
      "Epoch 03: train_acc=0.8438, val_acc=0.8737\n",
      "Epoch 04: train_acc=0.8832, val_acc=0.8758\n",
      "Epoch 05: train_acc=0.8995, val_acc=0.8697\n",
      "Epoch 06: train_acc=0.9282, val_acc=0.8921\n",
      "Epoch 07: train_acc=0.9450, val_acc=0.8921\n",
      "Epoch 08: train_acc=0.9547, val_acc=0.8819\n",
      "Early stopping\n",
      "\n",
      "üîç Testing BATCH=128, LR=0.002\n",
      "Epoch 01: train_acc=0.5736, val_acc=0.7658\n",
      "Epoch 02: train_acc=0.8184, val_acc=0.8452\n",
      "Epoch 03: train_acc=0.8854, val_acc=0.8697\n",
      "Epoch 04: train_acc=0.9103, val_acc=0.8697\n",
      "Epoch 05: train_acc=0.9226, val_acc=0.8737\n",
      "Epoch 06: train_acc=0.9488, val_acc=0.8758\n",
      "Epoch 07: train_acc=0.9669, val_acc=0.9002\n",
      "Epoch 08: train_acc=0.9803, val_acc=0.8961\n",
      "Epoch 09: train_acc=0.9885, val_acc=0.9022\n",
      "Epoch 10: train_acc=0.9939, val_acc=0.8880\n",
      "Epoch 11: train_acc=0.9941, val_acc=0.8859\n",
      "Early stopping\n",
      "\n",
      "üîç Testing BATCH=128, LR=0.003\n",
      "Epoch 01: train_acc=0.5956, val_acc=0.7678\n",
      "Epoch 02: train_acc=0.8111, val_acc=0.8635\n",
      "Epoch 03: train_acc=0.8721, val_acc=0.8391\n",
      "Epoch 04: train_acc=0.9241, val_acc=0.8819\n",
      "Epoch 05: train_acc=0.9543, val_acc=0.8778\n",
      "Epoch 06: train_acc=0.9712, val_acc=0.8819\n",
      "Early stopping\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     67\u001b[39m         model.load_state_dict(best_model_state)\n\u001b[32m     69\u001b[39m     results.append({\u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m: bs, \u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: lr, \u001b[33m\"\u001b[39m\u001b[33mval_acc\u001b[39m\u001b[33m\"\u001b[39m: best_val_acc})\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m df_results = \u001b[43mpd\u001b[49m.DataFrame(results).sort_values(\u001b[33m\"\u001b[39m\u001b[33mval_acc\u001b[39m\u001b[33m\"\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     72\u001b[39m best = df_results.iloc[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Grid search with early stopping that SAVES BEST WEIGHTS\n",
    "# -----------------------------\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.3\n",
    "NUM_CLASSES = len(LABEL.vocab)\n",
    "MAX_EPOCHS = 20\n",
    "PATIENCE = 3\n",
    "\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "batch_sizes = [32, 64, 128]\n",
    "learning_rates = [5e-4, 1e-3, 2e-3, 3e-3]\n",
    "\n",
    "results = []\n",
    "\n",
    "for bs, lr in itertools.product(batch_sizes, learning_rates):\n",
    "    print(f\"\\nüîç Testing BATCH={bs}, LR={lr}\")\n",
    "    train_iter, valid_iter = data.BucketIterator.splits(\n",
    "        (train_data, valid_data),\n",
    "        batch_size=bs,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # fresh embedding weights each trial\n",
    "    emb_layer = nn.Embedding.from_pretrained(\n",
    "        embedding_layer.weight.data.clone(), freeze=False, padding_idx=PAD_IDX\n",
    "    )\n",
    "    model = RNNClassifier(\n",
    "        embedding_layer=emb_layer,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        bidirectional=BIDIRECTIONAL,\n",
    "        dropout=DROPOUT,\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_acc = -1.0\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        train_loss, train_acc = epoch_run(train_iter, model, criterion, optimizer)\n",
    "        val_loss, val_acc = epoch_run(valid_iter, model, criterion, optimizer=None)\n",
    "        print(f\"Epoch {epoch:02d}: train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "        # ---- your early stopping block (with saving best) ----\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())  # keep BEST weights\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    # restore best weights before logging result (so this combo truly reflects its best)\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    results.append({\"batch_size\": bs, \"lr\": lr, \"val_acc\": best_val_acc})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Best configuration:\n",
      "batch_size    32.000000\n",
      "lr             0.001000\n",
      "val_acc        0.912424\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "All results (sorted):\n",
      "     batch_size      lr   val_acc\n",
      "0           32  0.0010  0.912424\n",
      "1          128  0.0005  0.910387\n",
      "2           64  0.0005  0.908350\n",
      "3           32  0.0005  0.906314\n",
      "4          128  0.0020  0.902240\n",
      "5           64  0.0010  0.898167\n",
      "6          128  0.0010  0.892057\n",
      "7           64  0.0020  0.892057\n",
      "8           64  0.0030  0.885947\n",
      "9           32  0.0030  0.883910\n",
      "10         128  0.0030  0.881874\n",
      "11          32  0.0020  0.867617\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results).sort_values(\"val_acc\", ascending=False)\n",
    "best = df_results.iloc[0]\n",
    "print(\"\\nüèÜ Best configuration:\")\n",
    "print(best)\n",
    "print(\"\\nAll results (sorted):\\n\", df_results.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Fix best batch size & lr from your previous grid search result `best` ---\n",
    "best_batch_size = int(best[\"batch_size\"])\n",
    "best_lr = float(best[\"lr\"])\n",
    "print(f\"\\n‚úÖ Using best hyperparams from LR/Batch search -> batch_size={best_batch_size}, lr={best_lr}\")\n",
    "\n",
    "# --- 2) Hidden-dim search using fixed (batch_size, lr) ---\n",
    "\n",
    "# Search space (tweak as you like)\n",
    "hidden_dims = [64, 96, 128, 192, 256, 384, 512]\n",
    "\n",
    "results_hd = []\n",
    "\n",
    "# Rebuild iterators ONCE with best batch size\n",
    "train_iter, valid_iter = data.BucketIterator.splits(\n",
    "    (train_data, valid_data),\n",
    "    batch_size=best_batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "for hd in hidden_dims:\n",
    "    print(f\"\\nüß™ Testing hidden_dim={hd}\")\n",
    "\n",
    "    # fresh embedding weights for each trial\n",
    "    emb_layer = nn.Embedding.from_pretrained(\n",
    "        embedding_layer.weight.data.clone(), freeze=False, padding_idx=PAD_IDX\n",
    "    )\n",
    "\n",
    "    model = RNNClassifier(\n",
    "        embedding_layer=emb_layer,\n",
    "        hidden_dim=hd,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        bidirectional=BIDIRECTIONAL,\n",
    "        dropout=DROPOUT,\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)\n",
    "\n",
    "    best_val_acc_hd = -1.0\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state_hd = None\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        train_loss, train_acc = epoch_run(train_iter, model, criterion, optimizer)\n",
    "        val_loss, val_acc = epoch_run(valid_iter, model, criterion, optimizer=None)\n",
    "        print(f\"Epoch {epoch:02d}: train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "        # Early stopping + save-best (your pattern)\n",
    "        if val_acc > best_val_acc_hd:\n",
    "            best_val_acc_hd = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state_hd = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    # Restore best for this hidden_dim before recording\n",
    "    if best_model_state_hd is not None:\n",
    "        model.load_state_dict(best_model_state_hd)\n",
    "\n",
    "    results_hd.append({\"hidden_dim\": hd, \"val_acc\": best_val_acc_hd})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) Pick best hidden dim, print & save variable ---\n",
    "df_hd = pd.DataFrame(results_hd).sort_values(\"val_acc\", ascending=False).reset_index(drop=True)\n",
    "best_hidden_dim = int(df_hd.loc[0, \"hidden_dim\"])\n",
    "\n",
    "print(\"\\nüèÜ Best hidden_dim configuration:\")\n",
    "print(df_hd.loc[0])\n",
    "print(\"\\nAll hidden_dim results (sorted):\")\n",
    "print(df_hd)\n",
    "\n",
    "# Variables now set for subsequent training:\n",
    "print(f\"\\nüëâ Final choice: best_batch_size={best_batch_size}, best_lr={best_lr}, best_hidden_dim={best_hidden_dim}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc4002-nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
