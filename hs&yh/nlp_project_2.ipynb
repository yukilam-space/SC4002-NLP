{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 0 & 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tokenization\n",
    "TEXT = data.Field(tokenize='spacy',\n",
    "    tokenizer_language='en_core_web_sm',\n",
    "    batch_first=True,\n",
    "    fix_length=50,\n",
    "    lower=True,\n",
    "    unk_token='<unk>',\n",
    "    pad_token='<pad>'\n",
    "    )\n",
    "\n",
    "# For multi-class classification labels\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "# Load the TREC dataset\n",
    "train_data, test_data = datasets.TREC.splits(TEXT, LABEL, fine_grained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DESC', 'LOC', 'ENTY', 'NUM', 'HUM', 'ABBR'}\n"
     ]
    }
   ],
   "source": [
    "label_set = set()\n",
    "for i in train_data:\n",
    "    label_set.add(i.label)\n",
    "\n",
    "print(label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "vocab = TEXT.vocab\n",
    "vectors = vocab.vectors.clone()\n",
    "glove_vocab = set(vocab.stoi.keys())\n",
    "\n",
    "# Subword parameters\n",
    "ngram_min = 3\n",
    "ngram_max = 6\n",
    "\n",
    "# Count word frequency in training data\n",
    "word_counter = Counter()\n",
    "for example in train_data:\n",
    "    word_counter.update([w.lower().strip(string.punctuation) for w in example.text])\n",
    "\n",
    "# Threshold to consider a word “frequent” (adjustable)\n",
    "freq_threshold = 3\n",
    "\n",
    "# <unk> vector\n",
    "unk_vector = torch.zeros(embedding_dim)\n",
    "\n",
    "def get_subwords(word, n_min=3, n_max=6):\n",
    "    word = f\"<{word.lower()}>\"\n",
    "    subwords = []\n",
    "    for n in range(n_min, n_max+1):\n",
    "        subwords += [word[i:i+n] for i in range(len(word)-n+1)]\n",
    "    return subwords\n",
    "\n",
    "def get_word_vector(word):\n",
    "    w_clean = word.lower().strip(string.punctuation)\n",
    "    \n",
    "    if w_clean in glove_vocab:\n",
    "        return vectors[vocab.stoi[w_clean]]\n",
    "    \n",
    "    # Subword averaging\n",
    "    subwords = get_subwords(w_clean, ngram_min, ngram_max)\n",
    "    subword_vecs = [vectors[vocab.stoi[sg]] for sg in subwords if sg in glove_vocab]\n",
    "    if subword_vecs:\n",
    "        return torch.stack(subword_vecs).mean(0)\n",
    "    \n",
    "    # Random vector for frequent OOVs\n",
    "    if word_counter[w_clean] >= freq_threshold:\n",
    "        return torch.randn(embedding_dim)\n",
    "    \n",
    "    # <unk> for rare OOVs\n",
    "    return unk_vector\n",
    "\n",
    "# Build embedding matrix\n",
    "embedding_matrix = {}\n",
    "for example in train_data:\n",
    "    for w in example.text:\n",
    "        if w not in embedding_matrix:\n",
    "            embedding_matrix[w] = get_word_vector(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer created with shape: torch.Size([8536, 100])\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab = TEXT.vocab\n",
    "\n",
    "# Create tensor for nn.Embedding\n",
    "embedding_matrix_tensor = torch.zeros(len(vocab), embedding_dim)\n",
    "for word, idx in vocab.stoi.items():\n",
    "    if word in embedding_matrix:\n",
    "        embedding_matrix_tensor[idx] = embedding_matrix[word]\n",
    "\n",
    "# Create embedding layer (learnable)\n",
    "embedding_layer = nn.Embedding.from_pretrained(embedding_matrix_tensor, freeze=False)\n",
    "print(\"Embedding layer created with shape:\", embedding_layer.weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, copy, pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "embedding_dim = embedding_layer.weight.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Simple RNN classifier (tanh)\n",
    "# -----------------------------\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim, num_layers, bidirectional, dropout, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        emb_dim = self.embedding.weight.size(1)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            nonlinearity=\"tanh\",      # simple RNN\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), num_classes)\n",
    "\n",
    "    def forward(self, text):\n",
    "        x = self.embedding(text)                     # [B, T, E]\n",
    "        outputs, hidden = self.rnn(x)                # hidden: [L*D, B, H]\n",
    "        if self.rnn.bidirectional:\n",
    "            last = torch.cat((hidden[-2], hidden[-1]), dim=1)  # [B, 2H]\n",
    "        else:\n",
    "            last = hidden[-1]                                   # [B, H]\n",
    "        return self.fc(self.dropout(last))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, copy, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext import data\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# -----------------------------\n",
    "# Reproducibility\n",
    "# -----------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -----------------------------\n",
    "# Generic helpers\n",
    "# -----------------------------\n",
    "def build_iters(batch_size, train_data, valid_data, device):\n",
    "    return data.BucketIterator.splits(\n",
    "        (train_data, valid_data),\n",
    "        batch_size=batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "def build_model(embedding_layer, pad_idx, hidden_dim, num_layers, bidirectional, dropout, num_classes, device):\n",
    "    emb_layer = nn.Embedding.from_pretrained(\n",
    "        embedding_layer.weight.data.clone(), freeze=False, padding_idx=pad_idx\n",
    "    )\n",
    "    model = RNNClassifier(\n",
    "        embedding_layer=emb_layer,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        bidirectional=bidirectional,\n",
    "        dropout=dropout,\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    return model\n",
    "\n",
    "def epoch_run(iterator, model, criterion, optimizer=None):\n",
    "    model.train(optimizer is not None)\n",
    "    tot_loss, tot_correct, tot_count = 0.0, 0, 0\n",
    "    for batch in iterator:\n",
    "        text, labels = batch.text, batch.label\n",
    "        logits = model(text)\n",
    "        loss = criterion(logits, labels)\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "        tot_loss += loss.item() * labels.size(0)\n",
    "        tot_correct += (logits.argmax(1) == labels).sum().item()\n",
    "        tot_count += labels.size(0)\n",
    "    return tot_loss / max(tot_count,1), tot_correct / max(tot_count,1)\n",
    "\n",
    "# -----------------------------\n",
    "# K-Fold training function\n",
    "# -----------------------------\n",
    "def train_kfold_config(k_folds, batch_size, lr, hidden_dim, *,\n",
    "                       num_layers, bidirectional, dropout, num_classes,\n",
    "                       full_dataset, pad_idx, device, embedding_layer,\n",
    "                       max_epochs=50, patience=3, seed=42):\n",
    "\n",
    "    set_seed(seed)  # 🔒 make results deterministic\n",
    "\n",
    "    # Convert torchtext dataset examples to indices for sklearn KFold\n",
    "    all_indices = np.arange(len(full_dataset))\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    fold_results = []\n",
    "    fold_models = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(all_indices), 1):\n",
    "        print(f\"\\n===== Fold {fold}/{k_folds} =====\")\n",
    "\n",
    "        # Create new Dataset splits\n",
    "        train_data = [full_dataset.examples[i] for i in train_idx]\n",
    "        valid_data = [full_dataset.examples[i] for i in val_idx]\n",
    "        train_data = data.Dataset(train_data, fields=full_dataset.fields)\n",
    "        valid_data = data.Dataset(valid_data, fields=full_dataset.fields)\n",
    "\n",
    "        # Build iterators\n",
    "        train_iter, valid_iter = build_iters(batch_size, train_data, valid_data, device)\n",
    "\n",
    "        # Build model, loss, optimizer\n",
    "        model = build_model(embedding_layer, pad_idx, hidden_dim, num_layers, bidirectional, dropout, num_classes, device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        best_val_acc = -1.0\n",
    "        epochs_no_improve = 0\n",
    "        best_state = None\n",
    "\n",
    "        # Train per fold\n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            train_loss, train_acc = epoch_run(train_iter, model, criterion, optimizer)\n",
    "            val_loss, val_acc = epoch_run(valid_iter, model, criterion, optimizer=None)\n",
    "            print(f\"Epoch {epoch:02d}: train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                epochs_no_improve = 0\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "\n",
    "        # Load best model\n",
    "        if best_state:\n",
    "            model.load_state_dict(best_state)\n",
    "\n",
    "        print(f\"Fold {fold} best val_acc={best_val_acc:.4f}\")\n",
    "        fold_results.append(best_val_acc)\n",
    "        fold_models.append(model)\n",
    "\n",
    "    # Average accuracy\n",
    "    mean_acc = np.mean(fold_results)\n",
    "    print(f\"\\n===== K-Fold Results =====\")\n",
    "    for i, acc in enumerate(fold_results, 1):\n",
    "        print(f\"Fold {i}: {acc:.4f}\")\n",
    "    print(f\"Average Val Accuracy: {mean_acc:.4f}\")\n",
    "\n",
    "    return mean_acc, fold_results, fold_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip here ownards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4416, Valid: 491, Test: 500\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1. Setup & Config\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume TEXT, LABEL, train_data, test_data, embedding_layer already exist\n",
    "train_data, valid_data = train_data.split(split_ratio=0.9, random_state=random.seed(SEED))\n",
    "print(f\"Train: {len(train_data)}, Valid: {len(valid_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "embedding_dim = embedding_layer.weight.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Train/eval 1 epoch\n",
    "# -----------------------------\n",
    "def epoch_run(iterator, model, criterion, optimizer=None):\n",
    "    model.train(optimizer is not None)\n",
    "    tot_loss, tot_correct, tot_count = 0.0, 0, 0\n",
    "    for batch in iterator:\n",
    "        text, labels = batch.text, batch.label\n",
    "        logits = model(text)\n",
    "        loss = criterion(logits, labels)\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)  # RNNs can explode\n",
    "            optimizer.step()\n",
    "        tot_loss += loss.item() * labels.size(0)\n",
    "        tot_correct += (logits.argmax(1) == labels).sum().item()\n",
    "        tot_count += labels.size(0)\n",
    "    return tot_loss / max(tot_count,1), tot_correct / max(tot_count,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Testing BATCH=32, LR=0.0005\n",
      "Epoch 01: train_acc=0.6003, val_acc=0.7943\n",
      "Epoch 02: train_acc=0.8252, val_acc=0.8411\n",
      "Epoch 03: train_acc=0.8705, val_acc=0.8839\n",
      "Epoch 04: train_acc=0.9017, val_acc=0.9063\n",
      "Epoch 05: train_acc=0.9237, val_acc=0.8900\n",
      "Epoch 06: train_acc=0.9386, val_acc=0.9022\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=32, LR=0.001\n",
      "Epoch 01: train_acc=0.6771, val_acc=0.8534\n",
      "Epoch 02: train_acc=0.8490, val_acc=0.8778\n",
      "Epoch 03: train_acc=0.8961, val_acc=0.8982\n",
      "Epoch 04: train_acc=0.9214, val_acc=0.9124\n",
      "Epoch 05: train_acc=0.9450, val_acc=0.9022\n",
      "Epoch 06: train_acc=0.9656, val_acc=0.8982\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=32, LR=0.002\n",
      "Epoch 01: train_acc=0.7124, val_acc=0.8228\n",
      "Epoch 02: train_acc=0.8594, val_acc=0.8676\n",
      "Epoch 03: train_acc=0.9128, val_acc=0.8656\n",
      "Epoch 04: train_acc=0.9402, val_acc=0.8574\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=32, LR=0.003\n",
      "Epoch 01: train_acc=0.7251, val_acc=0.8513\n",
      "Epoch 02: train_acc=0.8449, val_acc=0.8493\n",
      "Epoch 03: train_acc=0.9096, val_acc=0.8676\n",
      "Epoch 04: train_acc=0.9126, val_acc=0.8676\n",
      "Epoch 05: train_acc=0.9656, val_acc=0.8839\n",
      "Epoch 06: train_acc=0.9801, val_acc=0.8697\n",
      "Epoch 07: train_acc=0.9835, val_acc=0.8758\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=64, LR=0.0005\n",
      "Epoch 01: train_acc=0.5159, val_acc=0.6538\n",
      "Epoch 02: train_acc=0.7726, val_acc=0.8289\n",
      "Epoch 03: train_acc=0.8435, val_acc=0.8615\n",
      "Epoch 04: train_acc=0.8768, val_acc=0.8798\n",
      "Epoch 05: train_acc=0.9013, val_acc=0.8839\n",
      "Epoch 06: train_acc=0.9196, val_acc=0.8900\n",
      "Epoch 07: train_acc=0.9357, val_acc=0.8941\n",
      "Epoch 08: train_acc=0.9459, val_acc=0.9084\n",
      "Epoch 09: train_acc=0.9547, val_acc=0.8921\n",
      "Epoch 10: train_acc=0.9620, val_acc=0.8880\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=64, LR=0.001\n",
      "Epoch 01: train_acc=0.6125, val_acc=0.7984\n",
      "Epoch 02: train_acc=0.8200, val_acc=0.8187\n",
      "Epoch 03: train_acc=0.8693, val_acc=0.8798\n",
      "Epoch 04: train_acc=0.9119, val_acc=0.8982\n",
      "Epoch 05: train_acc=0.9316, val_acc=0.8798\n",
      "Epoch 06: train_acc=0.9534, val_acc=0.8839\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=64, LR=0.002\n",
      "Epoch 01: train_acc=0.6712, val_acc=0.8167\n",
      "Epoch 02: train_acc=0.8490, val_acc=0.8432\n",
      "Epoch 03: train_acc=0.8918, val_acc=0.8900\n",
      "Epoch 04: train_acc=0.9296, val_acc=0.8921\n",
      "Epoch 05: train_acc=0.9380, val_acc=0.8758\n",
      "Epoch 06: train_acc=0.9545, val_acc=0.8697\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=64, LR=0.003\n",
      "Epoch 01: train_acc=0.6789, val_acc=0.8086\n",
      "Epoch 02: train_acc=0.8347, val_acc=0.8371\n",
      "Epoch 03: train_acc=0.9031, val_acc=0.8615\n",
      "Epoch 04: train_acc=0.9468, val_acc=0.8859\n",
      "Epoch 05: train_acc=0.9629, val_acc=0.8758\n",
      "Epoch 06: train_acc=0.9819, val_acc=0.8798\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=128, LR=0.0005\n",
      "Epoch 01: train_acc=0.4287, val_acc=0.5255\n",
      "Epoch 02: train_acc=0.6606, val_acc=0.7699\n",
      "Epoch 03: train_acc=0.7702, val_acc=0.8187\n",
      "Epoch 04: train_acc=0.8277, val_acc=0.8350\n",
      "Epoch 05: train_acc=0.8551, val_acc=0.8778\n",
      "Epoch 06: train_acc=0.8807, val_acc=0.8839\n",
      "Epoch 07: train_acc=0.9001, val_acc=0.8839\n",
      "Epoch 08: train_acc=0.9169, val_acc=0.8982\n",
      "Epoch 09: train_acc=0.9287, val_acc=0.8921\n",
      "Epoch 10: train_acc=0.9361, val_acc=0.9104\n",
      "Epoch 11: train_acc=0.9504, val_acc=0.9063\n",
      "Epoch 12: train_acc=0.9567, val_acc=0.9022\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=128, LR=0.001\n",
      "Epoch 01: train_acc=0.5188, val_acc=0.7393\n",
      "Epoch 02: train_acc=0.7584, val_acc=0.8289\n",
      "Epoch 03: train_acc=0.8438, val_acc=0.8737\n",
      "Epoch 04: train_acc=0.8832, val_acc=0.8758\n",
      "Epoch 05: train_acc=0.8995, val_acc=0.8697\n",
      "Epoch 06: train_acc=0.9282, val_acc=0.8921\n",
      "Epoch 07: train_acc=0.9450, val_acc=0.8921\n",
      "Epoch 08: train_acc=0.9547, val_acc=0.8819\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=128, LR=0.002\n",
      "Epoch 01: train_acc=0.5736, val_acc=0.7658\n",
      "Epoch 02: train_acc=0.8184, val_acc=0.8452\n",
      "Epoch 03: train_acc=0.8854, val_acc=0.8697\n",
      "Epoch 04: train_acc=0.9103, val_acc=0.8697\n",
      "Epoch 05: train_acc=0.9226, val_acc=0.8737\n",
      "Epoch 06: train_acc=0.9488, val_acc=0.8758\n",
      "Epoch 07: train_acc=0.9669, val_acc=0.9002\n",
      "Epoch 08: train_acc=0.9803, val_acc=0.8961\n",
      "Epoch 09: train_acc=0.9885, val_acc=0.9022\n",
      "Epoch 10: train_acc=0.9939, val_acc=0.8880\n",
      "Epoch 11: train_acc=0.9941, val_acc=0.8859\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=128, LR=0.003\n",
      "Epoch 01: train_acc=0.5956, val_acc=0.7678\n",
      "Epoch 02: train_acc=0.8111, val_acc=0.8635\n",
      "Epoch 03: train_acc=0.8721, val_acc=0.8391\n",
      "Epoch 04: train_acc=0.9241, val_acc=0.8819\n",
      "Epoch 05: train_acc=0.9543, val_acc=0.8778\n",
      "Epoch 06: train_acc=0.9712, val_acc=0.8819\n",
      "Early stopping\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     67\u001b[39m         model.load_state_dict(best_model_state)\n\u001b[32m     69\u001b[39m     results.append({\u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m: bs, \u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: lr, \u001b[33m\"\u001b[39m\u001b[33mval_acc\u001b[39m\u001b[33m\"\u001b[39m: best_val_acc})\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m df_results = \u001b[43mpd\u001b[49m.DataFrame(results).sort_values(\u001b[33m\"\u001b[39m\u001b[33mval_acc\u001b[39m\u001b[33m\"\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     72\u001b[39m best = df_results.iloc[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Grid search with early stopping that SAVES BEST WEIGHTS\n",
    "# -----------------------------\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.3\n",
    "NUM_CLASSES = len(LABEL.vocab)\n",
    "MAX_EPOCHS = 20\n",
    "PATIENCE = 3\n",
    "\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "batch_sizes = [32, 64, 128]\n",
    "learning_rates = [5e-4, 1e-3, 2e-3, 3e-3]\n",
    "\n",
    "results = []\n",
    "\n",
    "for bs, lr in itertools.product(batch_sizes, learning_rates):\n",
    "    print(f\"\\n🔍 Testing BATCH={bs}, LR={lr}\")\n",
    "    train_iter, valid_iter = data.BucketIterator.splits(\n",
    "        (train_data, valid_data),\n",
    "        batch_size=bs,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # fresh embedding weights each trial\n",
    "    emb_layer = nn.Embedding.from_pretrained(\n",
    "        embedding_layer.weight.data.clone(), freeze=False, padding_idx=PAD_IDX\n",
    "    )\n",
    "    model = RNNClassifier(\n",
    "        embedding_layer=emb_layer,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        bidirectional=BIDIRECTIONAL,\n",
    "        dropout=DROPOUT,\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_acc = -1.0\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        train_loss, train_acc = epoch_run(train_iter, model, criterion, optimizer)\n",
    "        val_loss, val_acc = epoch_run(valid_iter, model, criterion, optimizer=None)\n",
    "        print(f\"Epoch {epoch:02d}: train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "        # ---- your early stopping block (with saving best) ----\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())  # keep BEST weights\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    # restore best weights before logging result (so this combo truly reflects its best)\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    results.append({\"batch_size\": bs, \"lr\": lr, \"val_acc\": best_val_acc})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏆 Best configuration:\n",
      "batch_size    32.000000\n",
      "lr             0.001000\n",
      "val_acc        0.912424\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "All results (sorted):\n",
      "     batch_size      lr   val_acc\n",
      "0           32  0.0010  0.912424\n",
      "1          128  0.0005  0.910387\n",
      "2           64  0.0005  0.908350\n",
      "3           32  0.0005  0.906314\n",
      "4          128  0.0020  0.902240\n",
      "5           64  0.0010  0.898167\n",
      "6          128  0.0010  0.892057\n",
      "7           64  0.0020  0.892057\n",
      "8           64  0.0030  0.885947\n",
      "9           32  0.0030  0.883910\n",
      "10         128  0.0030  0.881874\n",
      "11          32  0.0020  0.867617\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results).sort_values(\"val_acc\", ascending=False)\n",
    "best = df_results.iloc[0]\n",
    "print(\"\\n🏆 Best configuration:\")\n",
    "print(best)\n",
    "print(\"\\nAll results (sorted):\\n\", df_results.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Fix best batch size & lr from your previous grid search result `best` ---\n",
    "best_batch_size = int(best[\"batch_size\"])\n",
    "best_lr = float(best[\"lr\"])\n",
    "print(f\"\\n✅ Using best hyperparams from LR/Batch search -> batch_size={best_batch_size}, lr={best_lr}\")\n",
    "\n",
    "# --- 2) Hidden-dim search using fixed (batch_size, lr) ---\n",
    "\n",
    "# Search space (tweak as you like)\n",
    "hidden_dims = [64, 96, 128, 192, 256, 384, 512]\n",
    "\n",
    "results_hd = []\n",
    "\n",
    "# Rebuild iterators ONCE with best batch size\n",
    "train_iter, valid_iter = data.BucketIterator.splits(\n",
    "    (train_data, valid_data),\n",
    "    batch_size=best_batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "for hd in hidden_dims:\n",
    "    print(f\"\\n🧪 Testing hidden_dim={hd}\")\n",
    "\n",
    "    # fresh embedding weights for each trial\n",
    "    emb_layer = nn.Embedding.from_pretrained(\n",
    "        embedding_layer.weight.data.clone(), freeze=False, padding_idx=PAD_IDX\n",
    "    )\n",
    "\n",
    "    model = RNNClassifier(\n",
    "        embedding_layer=emb_layer,\n",
    "        hidden_dim=hd,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        bidirectional=BIDIRECTIONAL,\n",
    "        dropout=DROPOUT,\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)\n",
    "\n",
    "    best_val_acc_hd = -1.0\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state_hd = None\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        train_loss, train_acc = epoch_run(train_iter, model, criterion, optimizer)\n",
    "        val_loss, val_acc = epoch_run(valid_iter, model, criterion, optimizer=None)\n",
    "        print(f\"Epoch {epoch:02d}: train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "        # Early stopping + save-best (your pattern)\n",
    "        if val_acc > best_val_acc_hd:\n",
    "            best_val_acc_hd = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state_hd = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    # Restore best for this hidden_dim before recording\n",
    "    if best_model_state_hd is not None:\n",
    "        model.load_state_dict(best_model_state_hd)\n",
    "\n",
    "    results_hd.append({\"hidden_dim\": hd, \"val_acc\": best_val_acc_hd})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) Pick best hidden dim, print & save variable ---\n",
    "df_hd = pd.DataFrame(results_hd).sort_values(\"val_acc\", ascending=False).reset_index(drop=True)\n",
    "best_hidden_dim = int(df_hd.loc[0, \"hidden_dim\"])\n",
    "\n",
    "print(\"\\n🏆 Best hidden_dim configuration:\")\n",
    "print(df_hd.loc[0])\n",
    "print(\"\\nAll hidden_dim results (sorted):\")\n",
    "print(df_hd)\n",
    "\n",
    "# Variables now set for subsequent training:\n",
    "print(f\"\\n👉 Final choice: best_batch_size={best_batch_size}, best_lr={best_lr}, best_hidden_dim={best_hidden_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torchtext import data\n",
    "\n",
    "# -----------------------------\n",
    "# Generic helpers\n",
    "# -----------------------------\n",
    "def build_iters(batch_size, train_data, valid_data, device):\n",
    "    return data.BucketIterator.splits(\n",
    "        (train_data, valid_data),\n",
    "        batch_size=batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "def build_model(embedding_layer, pad_idx, hidden_dim, num_layers, bidirectional, dropout, num_classes, device):\n",
    "    emb_layer = nn.Embedding.from_pretrained(\n",
    "        embedding_layer.weight.data.clone(), freeze=False, padding_idx=pad_idx\n",
    "    )\n",
    "    model = RNNClassifier(\n",
    "        embedding_layer=emb_layer,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        bidirectional=bidirectional,\n",
    "        dropout=dropout,\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    return model\n",
    "\n",
    "def epoch_run(iterator, model, criterion, optimizer=None):\n",
    "    model.train(optimizer is not None)\n",
    "    tot_loss, tot_correct, tot_count = 0.0, 0, 0\n",
    "    for batch in iterator:\n",
    "        text, labels = batch.text, batch.label\n",
    "        logits = model(text)\n",
    "        loss = criterion(logits, labels)\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "        tot_loss   += loss.item() * labels.size(0)\n",
    "        tot_correct += (logits.argmax(1) == labels).sum().item()\n",
    "        tot_count  += labels.size(0)\n",
    "    return tot_loss / max(tot_count,1), tot_correct / max(tot_count,1)\n",
    "\n",
    "def train_one_config(batch_size, lr, hidden_dim, *,\n",
    "                     num_layers, bidirectional, dropout, num_classes,\n",
    "                     train_data, valid_data, pad_idx, device,\n",
    "                     max_epochs=20, patience=3):\n",
    "    \"\"\"Train one (batch_size, lr, hidden_dim) with early stopping; return best_val_acc and best_state.\"\"\"\n",
    "    train_iter, valid_iter = build_iters(batch_size, train_data, valid_data, device)\n",
    "    model = build_model(embedding_layer, pad_idx, hidden_dim, num_layers, bidirectional, dropout, num_classes, device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_acc = -1.0\n",
    "    epochs_no_improve = 0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        train_loss, train_acc = epoch_run(train_iter, model, criterion, optimizer)\n",
    "        val_loss,   val_acc   = epoch_run(valid_iter, model, criterion, optimizer=None)\n",
    "        print(f\"Epoch {epoch:02d}: train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return best_val_acc, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Testing BATCH=32, LR=0.0005\n",
      "Epoch 01: train_acc=0.6187, val_acc=0.8147\n",
      "Epoch 02: train_acc=0.8102, val_acc=0.8391\n",
      "Epoch 03: train_acc=0.8718, val_acc=0.8778\n",
      "Epoch 04: train_acc=0.8981, val_acc=0.9043\n",
      "Epoch 05: train_acc=0.9167, val_acc=0.8880\n",
      "Epoch 06: train_acc=0.9366, val_acc=0.9043\n",
      "Epoch 07: train_acc=0.9545, val_acc=0.8941\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=32, LR=0.001\n",
      "Epoch 01: train_acc=0.6769, val_acc=0.8737\n",
      "Epoch 02: train_acc=0.8406, val_acc=0.8635\n",
      "Epoch 03: train_acc=0.9051, val_acc=0.8859\n",
      "Epoch 04: train_acc=0.9282, val_acc=0.9022\n",
      "Epoch 05: train_acc=0.9452, val_acc=0.8758\n",
      "Epoch 06: train_acc=0.9654, val_acc=0.8859\n",
      "Epoch 07: train_acc=0.9758, val_acc=0.8880\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=32, LR=0.002\n",
      "Epoch 01: train_acc=0.6791, val_acc=0.8391\n",
      "Epoch 02: train_acc=0.8576, val_acc=0.8819\n",
      "Epoch 03: train_acc=0.9092, val_acc=0.8819\n",
      "Epoch 04: train_acc=0.9466, val_acc=0.8819\n",
      "Epoch 05: train_acc=0.9558, val_acc=0.9043\n",
      "Epoch 06: train_acc=0.9817, val_acc=0.8900\n",
      "Epoch 07: train_acc=0.9706, val_acc=0.8534\n",
      "Epoch 08: train_acc=0.9694, val_acc=0.8758\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=32, LR=0.003\n",
      "Epoch 01: train_acc=0.7339, val_acc=0.8248\n",
      "Epoch 02: train_acc=0.8462, val_acc=0.8493\n",
      "Epoch 03: train_acc=0.9083, val_acc=0.8656\n",
      "Epoch 04: train_acc=0.9062, val_acc=0.8819\n",
      "Epoch 05: train_acc=0.9599, val_acc=0.8819\n",
      "Epoch 06: train_acc=0.9699, val_acc=0.8697\n",
      "Epoch 07: train_acc=0.9832, val_acc=0.8676\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=64, LR=0.0005\n",
      "Epoch 01: train_acc=0.5410, val_acc=0.7291\n",
      "Epoch 02: train_acc=0.7634, val_acc=0.8167\n",
      "Epoch 03: train_acc=0.8367, val_acc=0.8737\n",
      "Epoch 04: train_acc=0.8734, val_acc=0.8778\n",
      "Epoch 05: train_acc=0.8986, val_acc=0.9043\n",
      "Epoch 06: train_acc=0.9194, val_acc=0.9002\n",
      "Epoch 07: train_acc=0.9334, val_acc=0.8839\n",
      "Epoch 08: train_acc=0.9438, val_acc=0.8982\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=64, LR=0.001\n",
      "Epoch 01: train_acc=0.6341, val_acc=0.8106\n",
      "Epoch 02: train_acc=0.8245, val_acc=0.8432\n",
      "Epoch 03: train_acc=0.8793, val_acc=0.8859\n",
      "Epoch 04: train_acc=0.9153, val_acc=0.8880\n",
      "Epoch 05: train_acc=0.9359, val_acc=0.8961\n",
      "Epoch 06: train_acc=0.9531, val_acc=0.8900\n",
      "Epoch 07: train_acc=0.9642, val_acc=0.8961\n",
      "Epoch 08: train_acc=0.9796, val_acc=0.8798\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=64, LR=0.002\n",
      "Epoch 01: train_acc=0.6843, val_acc=0.8248\n",
      "Epoch 02: train_acc=0.8548, val_acc=0.8574\n",
      "Epoch 03: train_acc=0.8952, val_acc=0.8880\n",
      "Epoch 04: train_acc=0.9355, val_acc=0.9043\n",
      "Epoch 05: train_acc=0.9581, val_acc=0.8656\n",
      "Epoch 06: train_acc=0.9423, val_acc=0.8737\n",
      "Epoch 07: train_acc=0.9611, val_acc=0.8778\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=64, LR=0.003\n",
      "Epoch 01: train_acc=0.6803, val_acc=0.8187\n",
      "Epoch 02: train_acc=0.8487, val_acc=0.8513\n",
      "Epoch 03: train_acc=0.9090, val_acc=0.8900\n",
      "Epoch 04: train_acc=0.9466, val_acc=0.8635\n",
      "Epoch 05: train_acc=0.9504, val_acc=0.8411\n",
      "Epoch 06: train_acc=0.9697, val_acc=0.8513\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=128, LR=0.0005\n",
      "Epoch 01: train_acc=0.4432, val_acc=0.5825\n",
      "Epoch 02: train_acc=0.6630, val_acc=0.7739\n",
      "Epoch 03: train_acc=0.7618, val_acc=0.8147\n",
      "Epoch 04: train_acc=0.8311, val_acc=0.8534\n",
      "Epoch 05: train_acc=0.8628, val_acc=0.8778\n",
      "Epoch 06: train_acc=0.8843, val_acc=0.8880\n",
      "Epoch 07: train_acc=0.8965, val_acc=0.8900\n",
      "Epoch 08: train_acc=0.9146, val_acc=0.8941\n",
      "Epoch 09: train_acc=0.9289, val_acc=0.8921\n",
      "Epoch 10: train_acc=0.9418, val_acc=0.8982\n",
      "Epoch 11: train_acc=0.9504, val_acc=0.8900\n",
      "Epoch 12: train_acc=0.9531, val_acc=0.9022\n",
      "Epoch 13: train_acc=0.9663, val_acc=0.9063\n",
      "Epoch 14: train_acc=0.9710, val_acc=0.9043\n",
      "Epoch 15: train_acc=0.9751, val_acc=0.9104\n",
      "Epoch 16: train_acc=0.9794, val_acc=0.8961\n",
      "Epoch 17: train_acc=0.9812, val_acc=0.8697\n",
      "Epoch 18: train_acc=0.9810, val_acc=0.8839\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=128, LR=0.001\n",
      "Epoch 01: train_acc=0.5306, val_acc=0.7393\n",
      "Epoch 02: train_acc=0.7720, val_acc=0.8371\n",
      "Epoch 03: train_acc=0.8415, val_acc=0.8758\n",
      "Epoch 04: train_acc=0.8870, val_acc=0.8961\n",
      "Epoch 05: train_acc=0.9058, val_acc=0.8900\n",
      "Epoch 06: train_acc=0.9293, val_acc=0.8982\n",
      "Epoch 07: train_acc=0.9380, val_acc=0.9043\n",
      "Epoch 08: train_acc=0.9595, val_acc=0.8961\n",
      "Epoch 09: train_acc=0.9706, val_acc=0.8982\n",
      "Epoch 10: train_acc=0.9792, val_acc=0.9043\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=128, LR=0.002\n",
      "Epoch 01: train_acc=0.5829, val_acc=0.7617\n",
      "Epoch 02: train_acc=0.8003, val_acc=0.8330\n",
      "Epoch 03: train_acc=0.8453, val_acc=0.8717\n",
      "Epoch 04: train_acc=0.8990, val_acc=0.8859\n",
      "Epoch 05: train_acc=0.9307, val_acc=0.8880\n",
      "Epoch 06: train_acc=0.9488, val_acc=0.8941\n",
      "Epoch 07: train_acc=0.9703, val_acc=0.8778\n",
      "Epoch 08: train_acc=0.9826, val_acc=0.8921\n",
      "Epoch 09: train_acc=0.9878, val_acc=0.8941\n",
      "Early stopping\n",
      "\n",
      "🔍 Testing BATCH=128, LR=0.003\n",
      "Epoch 01: train_acc=0.6162, val_acc=0.7862\n",
      "Epoch 02: train_acc=0.8218, val_acc=0.8737\n",
      "Epoch 03: train_acc=0.8906, val_acc=0.8635\n",
      "Epoch 04: train_acc=0.9368, val_acc=0.8961\n",
      "Epoch 05: train_acc=0.9509, val_acc=0.8819\n",
      "Epoch 06: train_acc=0.9622, val_acc=0.8921\n",
      "Epoch 07: train_acc=0.9758, val_acc=0.8921\n",
      "Early stopping\n",
      "\n",
      "🏆 Best (batch, lr):\n",
      "batch_size    128.000000\n",
      "lr              0.000500\n",
      "val_acc         0.910387\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Saved vars -> best_batch_size=128, best_lr=0.0005\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES   = len(LABEL.vocab)\n",
    "NUM_LAYERS    = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT       = 0.3\n",
    "MAX_EPOCHS    = 20\n",
    "PATIENCE      = 3\n",
    "\n",
    "batch_sizes     = [32, 64, 128]\n",
    "learning_rates  = [5e-4, 1e-3, 2e-3, 3e-3]\n",
    "HIDDEN_DIM_FIXED = 128   # fixed while tuning (batch, lr)\n",
    "\n",
    "results = []\n",
    "for bs, lr in itertools.product(batch_sizes, learning_rates):\n",
    "    print(f\"\\n🔍 Testing BATCH={bs}, LR={lr}\")\n",
    "    val_acc, _ = train_one_config(\n",
    "        bs, lr, HIDDEN_DIM_FIXED,\n",
    "        num_layers=NUM_LAYERS, bidirectional=BIDIRECTIONAL, dropout=DROPOUT, num_classes=NUM_CLASSES,\n",
    "        train_data=train_data, valid_data=valid_data, pad_idx=PAD_IDX, device=device,\n",
    "        max_epochs=MAX_EPOCHS, patience=PATIENCE\n",
    "    )\n",
    "    results.append({\"batch_size\": bs, \"lr\": lr, \"val_acc\": val_acc})\n",
    "\n",
    "df_results = pd.DataFrame(results).sort_values(\"val_acc\", ascending=False).reset_index(drop=True)\n",
    "best = df_results.loc[0]\n",
    "best_batch_size = int(best[\"batch_size\"])\n",
    "best_lr        = float(best[\"lr\"])\n",
    "print(\"\\n🏆 Best (batch, lr):\")\n",
    "print(best)\n",
    "print(f\"\\nSaved vars -> best_batch_size={best_batch_size}, best_lr={best_lr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Testing hidden_dim=64 (batch=128, lr=0.0005)\n",
      "Epoch 01: train_acc=0.3718, val_acc=0.4847\n",
      "Epoch 02: train_acc=0.5661, val_acc=0.7006\n",
      "Epoch 03: train_acc=0.6891, val_acc=0.7780\n",
      "Epoch 04: train_acc=0.7538, val_acc=0.8086\n",
      "Epoch 05: train_acc=0.8000, val_acc=0.8411\n",
      "Epoch 06: train_acc=0.8379, val_acc=0.8554\n",
      "Epoch 07: train_acc=0.8659, val_acc=0.8778\n",
      "Epoch 08: train_acc=0.8915, val_acc=0.8819\n",
      "Epoch 09: train_acc=0.9062, val_acc=0.8900\n",
      "Epoch 10: train_acc=0.9214, val_acc=0.9002\n",
      "Epoch 11: train_acc=0.9275, val_acc=0.8941\n",
      "Epoch 12: train_acc=0.9418, val_acc=0.8900\n",
      "Epoch 13: train_acc=0.9472, val_acc=0.8941\n",
      "Early stopping\n",
      "\n",
      "🧪 Testing hidden_dim=96 (batch=128, lr=0.0005)\n",
      "Epoch 01: train_acc=0.3872, val_acc=0.5295\n",
      "Epoch 02: train_acc=0.5942, val_acc=0.6802\n",
      "Epoch 03: train_acc=0.7235, val_acc=0.7637\n",
      "Epoch 04: train_acc=0.8030, val_acc=0.8411\n",
      "Epoch 05: train_acc=0.8458, val_acc=0.8310\n",
      "Epoch 06: train_acc=0.8755, val_acc=0.8880\n",
      "Epoch 07: train_acc=0.8956, val_acc=0.8941\n",
      "Epoch 08: train_acc=0.9101, val_acc=0.8921\n",
      "Epoch 09: train_acc=0.9235, val_acc=0.9002\n",
      "Epoch 10: train_acc=0.9352, val_acc=0.8941\n",
      "Epoch 11: train_acc=0.9466, val_acc=0.8982\n",
      "Epoch 12: train_acc=0.9481, val_acc=0.8900\n",
      "Early stopping\n",
      "\n",
      "🧪 Testing hidden_dim=128 (batch=128, lr=0.0005)\n",
      "Epoch 01: train_acc=0.4300, val_acc=0.5886\n",
      "Epoch 02: train_acc=0.6784, val_acc=0.7597\n",
      "Epoch 03: train_acc=0.7724, val_acc=0.8248\n",
      "Epoch 04: train_acc=0.8265, val_acc=0.8574\n",
      "Epoch 05: train_acc=0.8544, val_acc=0.8880\n",
      "Epoch 06: train_acc=0.8813, val_acc=0.8574\n",
      "Epoch 07: train_acc=0.8836, val_acc=0.8941\n",
      "Epoch 08: train_acc=0.9112, val_acc=0.8941\n",
      "Epoch 09: train_acc=0.9303, val_acc=0.9104\n",
      "Epoch 10: train_acc=0.9373, val_acc=0.8941\n",
      "Epoch 11: train_acc=0.9441, val_acc=0.9124\n",
      "Epoch 12: train_acc=0.9563, val_acc=0.9002\n",
      "Epoch 13: train_acc=0.9615, val_acc=0.9063\n",
      "Epoch 14: train_acc=0.9703, val_acc=0.9104\n",
      "Early stopping\n",
      "\n",
      "🧪 Testing hidden_dim=192 (batch=128, lr=0.0005)\n",
      "Epoch 01: train_acc=0.4740, val_acc=0.6110\n",
      "Epoch 02: train_acc=0.6966, val_acc=0.7821\n",
      "Epoch 03: train_acc=0.7989, val_acc=0.8574\n",
      "Epoch 04: train_acc=0.8481, val_acc=0.8513\n",
      "Epoch 05: train_acc=0.8770, val_acc=0.8961\n",
      "Epoch 06: train_acc=0.8922, val_acc=0.8350\n",
      "Epoch 07: train_acc=0.9087, val_acc=0.8798\n",
      "Epoch 08: train_acc=0.9207, val_acc=0.8961\n",
      "Early stopping\n",
      "\n",
      "🧪 Testing hidden_dim=256 (batch=128, lr=0.0005)\n",
      "Epoch 01: train_acc=0.4993, val_acc=0.7210\n",
      "Epoch 02: train_acc=0.7396, val_acc=0.8147\n",
      "Epoch 03: train_acc=0.8193, val_acc=0.8554\n",
      "Epoch 04: train_acc=0.8623, val_acc=0.8859\n",
      "Epoch 05: train_acc=0.8739, val_acc=0.8839\n",
      "Epoch 06: train_acc=0.8890, val_acc=0.8941\n",
      "Epoch 07: train_acc=0.9189, val_acc=0.8921\n",
      "Epoch 08: train_acc=0.9248, val_acc=0.8839\n",
      "Epoch 09: train_acc=0.9300, val_acc=0.8921\n",
      "Early stopping\n",
      "\n",
      "🧪 Testing hidden_dim=384 (batch=128, lr=0.0005)\n",
      "Epoch 01: train_acc=0.5288, val_acc=0.7536\n",
      "Epoch 02: train_acc=0.7600, val_acc=0.8106\n",
      "Epoch 03: train_acc=0.8340, val_acc=0.8656\n",
      "Epoch 04: train_acc=0.8653, val_acc=0.8961\n",
      "Epoch 05: train_acc=0.8875, val_acc=0.8880\n",
      "Epoch 06: train_acc=0.8972, val_acc=0.8676\n",
      "Epoch 07: train_acc=0.9099, val_acc=0.8982\n",
      "Epoch 08: train_acc=0.9192, val_acc=0.8859\n",
      "Epoch 09: train_acc=0.9398, val_acc=0.8982\n",
      "Epoch 10: train_acc=0.9479, val_acc=0.8778\n",
      "Early stopping\n",
      "\n",
      "🧪 Testing hidden_dim=512 (batch=128, lr=0.0005)\n",
      "Epoch 01: train_acc=0.5365, val_acc=0.7189\n",
      "Epoch 02: train_acc=0.7577, val_acc=0.8554\n",
      "Epoch 03: train_acc=0.8383, val_acc=0.8574\n",
      "Epoch 04: train_acc=0.8671, val_acc=0.8595\n",
      "Epoch 05: train_acc=0.8877, val_acc=0.8859\n",
      "Epoch 06: train_acc=0.9024, val_acc=0.8635\n",
      "Epoch 07: train_acc=0.9076, val_acc=0.8493\n",
      "Epoch 08: train_acc=0.9106, val_acc=0.8717\n",
      "Early stopping\n",
      "\n",
      "🏆 Best hidden_dim configuration:\n",
      "hidden_dim    128.000000\n",
      "val_acc         0.912424\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Saved var -> best_hidden_dim=128\n",
      "\n",
      "All hidden_dim results:\n",
      "    hidden_dim   val_acc\n",
      "0         128  0.912424\n",
      "1          64  0.900204\n",
      "2          96  0.900204\n",
      "3         384  0.898167\n",
      "4         192  0.896130\n",
      "5         256  0.894094\n",
      "6         512  0.885947\n"
     ]
    }
   ],
   "source": [
    "hidden_dims = [64, 96, 128, 192, 256, 384, 512]\n",
    "results_hd = []\n",
    "\n",
    "for hd in hidden_dims:\n",
    "    print(f\"\\n🧪 Testing hidden_dim={hd} (batch={best_batch_size}, lr={best_lr})\")\n",
    "    val_acc, _ = train_one_config(\n",
    "        best_batch_size, best_lr, hd,\n",
    "        num_layers=NUM_LAYERS, bidirectional=BIDIRECTIONAL, dropout=DROPOUT, num_classes=NUM_CLASSES,\n",
    "        train_data=train_data, valid_data=valid_data, pad_idx=PAD_IDX, device=device,\n",
    "        max_epochs=MAX_EPOCHS, patience=PATIENCE\n",
    "    )\n",
    "    results_hd.append({\"hidden_dim\": hd, \"val_acc\": val_acc})\n",
    "\n",
    "df_hd = pd.DataFrame(results_hd).sort_values(\"val_acc\", ascending=False).reset_index(drop=True)\n",
    "best_hidden_dim = int(df_hd.loc[0, \"hidden_dim\"])\n",
    "print(\"\\n🏆 Best hidden_dim configuration:\")\n",
    "print(df_hd.loc[0])\n",
    "print(f\"\\nSaved var -> best_hidden_dim={best_hidden_dim}\")\n",
    "print(\"\\nAll hidden_dim results:\\n\", df_hd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue here for kfold cross validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best batch_sizes and lr (with gradient clipping alrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 K-Fold Testing BATCH=32, LR=0.005\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.5785, val_acc=0.7204\n",
      "Epoch 02: train_acc=0.7624, val_acc=0.7809\n",
      "Epoch 03: train_acc=0.8592, val_acc=0.8048\n",
      "Epoch 04: train_acc=0.9037, val_acc=0.8268\n",
      "Epoch 05: train_acc=0.9418, val_acc=0.8378\n",
      "Epoch 06: train_acc=0.9663, val_acc=0.8332\n",
      "Epoch 07: train_acc=0.9672, val_acc=0.8231\n",
      "Epoch 08: train_acc=0.9704, val_acc=0.8249\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8378\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.6320, val_acc=0.7030\n",
      "Epoch 02: train_acc=0.7608, val_acc=0.7663\n",
      "Epoch 03: train_acc=0.8565, val_acc=0.8084\n",
      "Epoch 04: train_acc=0.9204, val_acc=0.8057\n",
      "Epoch 05: train_acc=0.9376, val_acc=0.7929\n",
      "Epoch 06: train_acc=0.9587, val_acc=0.7956\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8084\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.5954, val_acc=0.7248\n",
      "Epoch 02: train_acc=0.7423, val_acc=0.7798\n",
      "Epoch 03: train_acc=0.8310, val_acc=0.7807\n",
      "Epoch 04: train_acc=0.8783, val_acc=0.7633\n",
      "Epoch 05: train_acc=0.9097, val_acc=0.8211\n",
      "Epoch 06: train_acc=0.9558, val_acc=0.7963\n",
      "Epoch 07: train_acc=0.9619, val_acc=0.7963\n",
      "Epoch 08: train_acc=0.9665, val_acc=0.8073\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8211\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.6077, val_acc=0.7440\n",
      "Epoch 02: train_acc=0.7808, val_acc=0.7394\n",
      "Epoch 03: train_acc=0.8661, val_acc=0.7780\n",
      "Epoch 04: train_acc=0.8932, val_acc=0.7982\n",
      "Epoch 05: train_acc=0.9285, val_acc=0.8174\n",
      "Epoch 06: train_acc=0.9629, val_acc=0.8110\n",
      "Epoch 07: train_acc=0.9752, val_acc=0.8284\n",
      "Epoch 08: train_acc=0.9830, val_acc=0.8422\n",
      "Epoch 09: train_acc=0.9762, val_acc=0.8257\n",
      "Epoch 10: train_acc=0.9764, val_acc=0.8064\n",
      "Epoch 11: train_acc=0.9697, val_acc=0.8248\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8422\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.6162, val_acc=0.7624\n",
      "Epoch 02: train_acc=0.7879, val_acc=0.7725\n",
      "Epoch 03: train_acc=0.8602, val_acc=0.7789\n",
      "Epoch 04: train_acc=0.9202, val_acc=0.8174\n",
      "Epoch 05: train_acc=0.9395, val_acc=0.8312\n",
      "Epoch 06: train_acc=0.9599, val_acc=0.8119\n",
      "Epoch 07: train_acc=0.9720, val_acc=0.8385\n",
      "Epoch 08: train_acc=0.9856, val_acc=0.8229\n",
      "Epoch 09: train_acc=0.9785, val_acc=0.8202\n",
      "Epoch 10: train_acc=0.9844, val_acc=0.8349\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8385\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8378\n",
      "Fold 2: 0.8084\n",
      "Fold 3: 0.8211\n",
      "Fold 4: 0.8422\n",
      "Fold 5: 0.8385\n",
      "Average Val Accuracy: 0.8296\n",
      "\n",
      "🔍 K-Fold Testing BATCH=32, LR=0.001\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.5845, val_acc=0.7553\n",
      "Epoch 02: train_acc=0.7913, val_acc=0.8084\n",
      "Epoch 03: train_acc=0.8500, val_acc=0.8231\n",
      "Epoch 04: train_acc=0.9003, val_acc=0.8478\n",
      "Epoch 05: train_acc=0.9257, val_acc=0.8543\n",
      "Epoch 06: train_acc=0.9573, val_acc=0.8368\n",
      "Epoch 07: train_acc=0.9713, val_acc=0.8442\n",
      "Epoch 08: train_acc=0.9844, val_acc=0.8561\n",
      "Epoch 09: train_acc=0.9915, val_acc=0.8497\n",
      "Epoch 10: train_acc=0.9952, val_acc=0.8478\n",
      "Epoch 11: train_acc=0.9936, val_acc=0.8451\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8561\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.5893, val_acc=0.7608\n",
      "Epoch 02: train_acc=0.7939, val_acc=0.7855\n",
      "Epoch 03: train_acc=0.8519, val_acc=0.8103\n",
      "Epoch 04: train_acc=0.8989, val_acc=0.8368\n",
      "Epoch 05: train_acc=0.9319, val_acc=0.8258\n",
      "Epoch 06: train_acc=0.9580, val_acc=0.8332\n",
      "Epoch 07: train_acc=0.9796, val_acc=0.8194\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8368\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.6061, val_acc=0.7651\n",
      "Epoch 02: train_acc=0.7797, val_acc=0.7853\n",
      "Epoch 03: train_acc=0.8514, val_acc=0.8294\n",
      "Epoch 04: train_acc=0.9005, val_acc=0.8321\n",
      "Epoch 05: train_acc=0.9321, val_acc=0.8459\n",
      "Epoch 06: train_acc=0.9560, val_acc=0.8358\n",
      "Epoch 07: train_acc=0.9746, val_acc=0.8477\n",
      "Epoch 08: train_acc=0.9874, val_acc=0.8385\n",
      "Epoch 09: train_acc=0.9869, val_acc=0.8294\n",
      "Epoch 10: train_acc=0.9920, val_acc=0.8239\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8477\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.6045, val_acc=0.7211\n",
      "Epoch 02: train_acc=0.8003, val_acc=0.7835\n",
      "Epoch 03: train_acc=0.8574, val_acc=0.8073\n",
      "Epoch 04: train_acc=0.9021, val_acc=0.8046\n",
      "Epoch 05: train_acc=0.9376, val_acc=0.8037\n",
      "Epoch 06: train_acc=0.9555, val_acc=0.8147\n",
      "Epoch 07: train_acc=0.9725, val_acc=0.8248\n",
      "Epoch 08: train_acc=0.9895, val_acc=0.8385\n",
      "Epoch 09: train_acc=0.9842, val_acc=0.8239\n",
      "Epoch 10: train_acc=0.9899, val_acc=0.8220\n",
      "Epoch 11: train_acc=0.9931, val_acc=0.8312\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8385\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.6100, val_acc=0.7413\n",
      "Epoch 02: train_acc=0.7884, val_acc=0.8083\n",
      "Epoch 03: train_acc=0.8542, val_acc=0.8321\n",
      "Epoch 04: train_acc=0.8945, val_acc=0.8183\n",
      "Epoch 05: train_acc=0.9296, val_acc=0.8018\n",
      "Epoch 06: train_acc=0.9585, val_acc=0.8413\n",
      "Epoch 07: train_acc=0.9736, val_acc=0.8459\n",
      "Epoch 08: train_acc=0.9876, val_acc=0.8330\n",
      "Epoch 09: train_acc=0.9904, val_acc=0.8376\n",
      "Epoch 10: train_acc=0.9950, val_acc=0.8349\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8459\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8561\n",
      "Fold 2: 0.8368\n",
      "Fold 3: 0.8477\n",
      "Fold 4: 0.8385\n",
      "Fold 5: 0.8459\n",
      "Average Val Accuracy: 0.8450\n",
      "\n",
      "🔍 K-Fold Testing BATCH=32, LR=0.0005\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.5221, val_acc=0.6957\n",
      "Epoch 02: train_acc=0.7418, val_acc=0.7855\n",
      "Epoch 03: train_acc=0.8051, val_acc=0.8268\n",
      "Epoch 04: train_acc=0.8528, val_acc=0.8277\n",
      "Epoch 05: train_acc=0.8808, val_acc=0.8387\n",
      "Epoch 06: train_acc=0.9053, val_acc=0.8359\n",
      "Epoch 07: train_acc=0.9326, val_acc=0.8387\n",
      "Epoch 08: train_acc=0.9516, val_acc=0.8469\n",
      "Epoch 09: train_acc=0.9642, val_acc=0.8598\n",
      "Epoch 10: train_acc=0.9750, val_acc=0.8561\n",
      "Epoch 11: train_acc=0.9828, val_acc=0.8561\n",
      "Epoch 12: train_acc=0.9883, val_acc=0.8524\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8598\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.5247, val_acc=0.6856\n",
      "Epoch 02: train_acc=0.7352, val_acc=0.7489\n",
      "Epoch 03: train_acc=0.8140, val_acc=0.7984\n",
      "Epoch 04: train_acc=0.8443, val_acc=0.8203\n",
      "Epoch 05: train_acc=0.8844, val_acc=0.8240\n",
      "Epoch 06: train_acc=0.9138, val_acc=0.8213\n",
      "Epoch 07: train_acc=0.9349, val_acc=0.8075\n",
      "Epoch 08: train_acc=0.9539, val_acc=0.8277\n",
      "Epoch 09: train_acc=0.9668, val_acc=0.8350\n",
      "Epoch 10: train_acc=0.9784, val_acc=0.8258\n",
      "Epoch 11: train_acc=0.9883, val_acc=0.8341\n",
      "Epoch 12: train_acc=0.9881, val_acc=0.8323\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8350\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.5321, val_acc=0.7303\n",
      "Epoch 02: train_acc=0.7336, val_acc=0.7789\n",
      "Epoch 03: train_acc=0.8024, val_acc=0.7991\n",
      "Epoch 04: train_acc=0.8494, val_acc=0.8303\n",
      "Epoch 05: train_acc=0.8833, val_acc=0.8257\n",
      "Epoch 06: train_acc=0.9115, val_acc=0.8358\n",
      "Epoch 07: train_acc=0.9351, val_acc=0.8413\n",
      "Epoch 08: train_acc=0.9525, val_acc=0.8358\n",
      "Epoch 09: train_acc=0.9656, val_acc=0.8294\n",
      "Epoch 10: train_acc=0.9757, val_acc=0.8211\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8413\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.5305, val_acc=0.6679\n",
      "Epoch 02: train_acc=0.7469, val_acc=0.7431\n",
      "Epoch 03: train_acc=0.8129, val_acc=0.7890\n",
      "Epoch 04: train_acc=0.8597, val_acc=0.8064\n",
      "Epoch 05: train_acc=0.8851, val_acc=0.7991\n",
      "Epoch 06: train_acc=0.9147, val_acc=0.8128\n",
      "Epoch 07: train_acc=0.9356, val_acc=0.8147\n",
      "Epoch 08: train_acc=0.9525, val_acc=0.8147\n",
      "Epoch 09: train_acc=0.9668, val_acc=0.8312\n",
      "Epoch 10: train_acc=0.9746, val_acc=0.8321\n",
      "Epoch 11: train_acc=0.9819, val_acc=0.8394\n",
      "Epoch 12: train_acc=0.9883, val_acc=0.8330\n",
      "Epoch 13: train_acc=0.9936, val_acc=0.8294\n",
      "Epoch 14: train_acc=0.9956, val_acc=0.8229\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8394\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.5298, val_acc=0.6945\n",
      "Epoch 02: train_acc=0.7481, val_acc=0.7752\n",
      "Epoch 03: train_acc=0.8077, val_acc=0.8156\n",
      "Epoch 04: train_acc=0.8448, val_acc=0.8009\n",
      "Epoch 05: train_acc=0.8867, val_acc=0.8330\n",
      "Epoch 06: train_acc=0.9182, val_acc=0.8367\n",
      "Epoch 07: train_acc=0.9358, val_acc=0.8404\n",
      "Epoch 08: train_acc=0.9537, val_acc=0.8431\n",
      "Epoch 09: train_acc=0.9658, val_acc=0.8431\n",
      "Epoch 10: train_acc=0.9752, val_acc=0.8468\n",
      "Epoch 11: train_acc=0.9851, val_acc=0.8394\n",
      "Epoch 12: train_acc=0.9899, val_acc=0.8358\n",
      "Epoch 13: train_acc=0.9943, val_acc=0.8321\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8468\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8598\n",
      "Fold 2: 0.8350\n",
      "Fold 3: 0.8413\n",
      "Fold 4: 0.8394\n",
      "Fold 5: 0.8468\n",
      "Average Val Accuracy: 0.8445\n",
      "\n",
      "🔍 K-Fold Testing BATCH=32, LR=0.0001\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.3662, val_acc=0.4143\n",
      "Epoch 02: train_acc=0.5042, val_acc=0.5316\n",
      "Epoch 03: train_acc=0.5728, val_acc=0.5985\n",
      "Epoch 04: train_acc=0.6448, val_acc=0.6893\n",
      "Epoch 05: train_acc=0.7037, val_acc=0.7342\n",
      "Epoch 06: train_acc=0.7393, val_acc=0.7663\n",
      "Epoch 07: train_acc=0.7652, val_acc=0.7837\n",
      "Epoch 08: train_acc=0.7865, val_acc=0.8002\n",
      "Epoch 09: train_acc=0.8049, val_acc=0.8112\n",
      "Epoch 10: train_acc=0.8221, val_acc=0.8103\n",
      "Epoch 11: train_acc=0.8356, val_acc=0.8194\n",
      "Epoch 12: train_acc=0.8461, val_acc=0.8249\n",
      "Epoch 13: train_acc=0.8571, val_acc=0.8222\n",
      "Epoch 14: train_acc=0.8640, val_acc=0.8203\n",
      "Epoch 15: train_acc=0.8734, val_acc=0.8368\n",
      "Epoch 16: train_acc=0.8860, val_acc=0.8359\n",
      "Epoch 17: train_acc=0.8920, val_acc=0.8295\n",
      "Epoch 18: train_acc=0.8993, val_acc=0.8304\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8368\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.3472, val_acc=0.4885\n",
      "Epoch 02: train_acc=0.4974, val_acc=0.5500\n",
      "Epoch 03: train_acc=0.5950, val_acc=0.6361\n",
      "Epoch 04: train_acc=0.6634, val_acc=0.6774\n",
      "Epoch 05: train_acc=0.7134, val_acc=0.7039\n",
      "Epoch 06: train_acc=0.7400, val_acc=0.7223\n",
      "Epoch 07: train_acc=0.7585, val_acc=0.7489\n",
      "Epoch 08: train_acc=0.7833, val_acc=0.7525\n",
      "Epoch 09: train_acc=0.8003, val_acc=0.7644\n",
      "Epoch 10: train_acc=0.8191, val_acc=0.7809\n",
      "Epoch 11: train_acc=0.8292, val_acc=0.7984\n",
      "Epoch 12: train_acc=0.8390, val_acc=0.8002\n",
      "Epoch 13: train_acc=0.8496, val_acc=0.8066\n",
      "Epoch 14: train_acc=0.8604, val_acc=0.7956\n",
      "Epoch 15: train_acc=0.8698, val_acc=0.8103\n",
      "Epoch 16: train_acc=0.8782, val_acc=0.8139\n",
      "Epoch 17: train_acc=0.8863, val_acc=0.8158\n",
      "Epoch 18: train_acc=0.8915, val_acc=0.8112\n",
      "Epoch 19: train_acc=0.8996, val_acc=0.8158\n",
      "Epoch 20: train_acc=0.9071, val_acc=0.8194\n",
      "Epoch 21: train_acc=0.9152, val_acc=0.8249\n",
      "Epoch 22: train_acc=0.9172, val_acc=0.8194\n",
      "Epoch 23: train_acc=0.9243, val_acc=0.8148\n",
      "Epoch 24: train_acc=0.9305, val_acc=0.8121\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8249\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.3556, val_acc=0.5147\n",
      "Epoch 02: train_acc=0.5197, val_acc=0.5523\n",
      "Epoch 03: train_acc=0.6018, val_acc=0.6532\n",
      "Epoch 04: train_acc=0.6660, val_acc=0.7028\n",
      "Epoch 05: train_acc=0.7031, val_acc=0.7330\n",
      "Epoch 06: train_acc=0.7416, val_acc=0.7578\n",
      "Epoch 07: train_acc=0.7646, val_acc=0.7550\n",
      "Epoch 08: train_acc=0.7815, val_acc=0.7587\n",
      "Epoch 09: train_acc=0.8049, val_acc=0.7661\n",
      "Epoch 10: train_acc=0.8164, val_acc=0.7771\n",
      "Epoch 11: train_acc=0.8320, val_acc=0.7798\n",
      "Epoch 12: train_acc=0.8473, val_acc=0.7807\n",
      "Epoch 13: train_acc=0.8505, val_acc=0.7908\n",
      "Epoch 14: train_acc=0.8641, val_acc=0.8055\n",
      "Epoch 15: train_acc=0.8739, val_acc=0.8000\n",
      "Epoch 16: train_acc=0.8831, val_acc=0.8138\n",
      "Epoch 17: train_acc=0.8918, val_acc=0.8211\n",
      "Epoch 18: train_acc=0.8996, val_acc=0.8092\n",
      "Epoch 19: train_acc=0.9062, val_acc=0.8092\n",
      "Epoch 20: train_acc=0.9149, val_acc=0.8101\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8211\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.3576, val_acc=0.5422\n",
      "Epoch 02: train_acc=0.5250, val_acc=0.5716\n",
      "Epoch 03: train_acc=0.5903, val_acc=0.6294\n",
      "Epoch 04: train_acc=0.6619, val_acc=0.6826\n",
      "Epoch 05: train_acc=0.7127, val_acc=0.7147\n",
      "Epoch 06: train_acc=0.7499, val_acc=0.7330\n",
      "Epoch 07: train_acc=0.7795, val_acc=0.7468\n",
      "Epoch 08: train_acc=0.7932, val_acc=0.7532\n",
      "Epoch 09: train_acc=0.8150, val_acc=0.7596\n",
      "Epoch 10: train_acc=0.8283, val_acc=0.7670\n",
      "Epoch 11: train_acc=0.8393, val_acc=0.7817\n",
      "Epoch 12: train_acc=0.8508, val_acc=0.7817\n",
      "Epoch 13: train_acc=0.8599, val_acc=0.7798\n",
      "Epoch 14: train_acc=0.8689, val_acc=0.7963\n",
      "Epoch 15: train_acc=0.8810, val_acc=0.7890\n",
      "Epoch 16: train_acc=0.8913, val_acc=0.8073\n",
      "Epoch 17: train_acc=0.8950, val_acc=0.8028\n",
      "Epoch 18: train_acc=0.9017, val_acc=0.7936\n",
      "Epoch 19: train_acc=0.9078, val_acc=0.8018\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8073\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.3398, val_acc=0.5037\n",
      "Epoch 02: train_acc=0.5083, val_acc=0.5358\n",
      "Epoch 03: train_acc=0.6022, val_acc=0.6275\n",
      "Epoch 04: train_acc=0.6751, val_acc=0.6872\n",
      "Epoch 05: train_acc=0.7171, val_acc=0.7147\n",
      "Epoch 06: train_acc=0.7469, val_acc=0.7339\n",
      "Epoch 07: train_acc=0.7705, val_acc=0.7532\n",
      "Epoch 08: train_acc=0.7884, val_acc=0.7670\n",
      "Epoch 09: train_acc=0.8008, val_acc=0.7853\n",
      "Epoch 10: train_acc=0.8207, val_acc=0.7945\n",
      "Epoch 11: train_acc=0.8230, val_acc=0.8037\n",
      "Epoch 12: train_acc=0.8409, val_acc=0.8064\n",
      "Epoch 13: train_acc=0.8530, val_acc=0.8165\n",
      "Epoch 14: train_acc=0.8611, val_acc=0.8138\n",
      "Epoch 15: train_acc=0.8712, val_acc=0.8229\n",
      "Epoch 16: train_acc=0.8776, val_acc=0.8312\n",
      "Epoch 17: train_acc=0.8881, val_acc=0.8220\n",
      "Epoch 18: train_acc=0.8945, val_acc=0.8174\n",
      "Epoch 19: train_acc=0.9037, val_acc=0.8174\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8312\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8368\n",
      "Fold 2: 0.8249\n",
      "Fold 3: 0.8211\n",
      "Fold 4: 0.8073\n",
      "Fold 5: 0.8312\n",
      "Average Val Accuracy: 0.8243\n",
      "\n",
      "🔍 K-Fold Testing BATCH=64, LR=0.005\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.5795, val_acc=0.6939\n",
      "Epoch 02: train_acc=0.7544, val_acc=0.7736\n",
      "Epoch 03: train_acc=0.8356, val_acc=0.8093\n",
      "Epoch 04: train_acc=0.8991, val_acc=0.8368\n",
      "Epoch 05: train_acc=0.9278, val_acc=0.8442\n",
      "Epoch 06: train_acc=0.9629, val_acc=0.8286\n",
      "Epoch 07: train_acc=0.9727, val_acc=0.8323\n",
      "Epoch 08: train_acc=0.9819, val_acc=0.8524\n",
      "Epoch 09: train_acc=0.9883, val_acc=0.8488\n",
      "Epoch 10: train_acc=0.9947, val_acc=0.8561\n",
      "Epoch 11: train_acc=0.9952, val_acc=0.8405\n",
      "Epoch 12: train_acc=0.9938, val_acc=0.8323\n",
      "Epoch 13: train_acc=0.9954, val_acc=0.8323\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8561\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.5886, val_acc=0.7168\n",
      "Epoch 02: train_acc=0.7608, val_acc=0.7104\n",
      "Epoch 03: train_acc=0.8351, val_acc=0.8148\n",
      "Epoch 04: train_acc=0.9124, val_acc=0.8130\n",
      "Epoch 05: train_acc=0.9181, val_acc=0.8093\n",
      "Epoch 06: train_acc=0.9674, val_acc=0.7919\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8148\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.5908, val_acc=0.6688\n",
      "Epoch 02: train_acc=0.7295, val_acc=0.6486\n",
      "Epoch 03: train_acc=0.7873, val_acc=0.7982\n",
      "Epoch 04: train_acc=0.8939, val_acc=0.7991\n",
      "Epoch 05: train_acc=0.9415, val_acc=0.8294\n",
      "Epoch 06: train_acc=0.9567, val_acc=0.7606\n",
      "Epoch 07: train_acc=0.9415, val_acc=0.7835\n",
      "Epoch 08: train_acc=0.9645, val_acc=0.8156\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8294\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.5935, val_acc=0.6725\n",
      "Epoch 02: train_acc=0.7554, val_acc=0.7220\n",
      "Epoch 03: train_acc=0.8294, val_acc=0.7807\n",
      "Epoch 04: train_acc=0.9030, val_acc=0.8119\n",
      "Epoch 05: train_acc=0.9092, val_acc=0.8028\n",
      "Epoch 06: train_acc=0.9466, val_acc=0.8138\n",
      "Epoch 07: train_acc=0.9823, val_acc=0.8339\n",
      "Epoch 08: train_acc=0.9885, val_acc=0.8183\n",
      "Epoch 09: train_acc=0.9961, val_acc=0.8174\n",
      "Epoch 10: train_acc=0.9920, val_acc=0.8248\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8339\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.5869, val_acc=0.6908\n",
      "Epoch 02: train_acc=0.7513, val_acc=0.7633\n",
      "Epoch 03: train_acc=0.8287, val_acc=0.8028\n",
      "Epoch 04: train_acc=0.9110, val_acc=0.8046\n",
      "Epoch 05: train_acc=0.9445, val_acc=0.8046\n",
      "Epoch 06: train_acc=0.9486, val_acc=0.8202\n",
      "Epoch 07: train_acc=0.9796, val_acc=0.8083\n",
      "Epoch 08: train_acc=0.9856, val_acc=0.7752\n",
      "Epoch 09: train_acc=0.9420, val_acc=0.8202\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8202\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8561\n",
      "Fold 2: 0.8148\n",
      "Fold 3: 0.8294\n",
      "Fold 4: 0.8339\n",
      "Fold 5: 0.8202\n",
      "Average Val Accuracy: 0.8309\n",
      "\n",
      "🔍 K-Fold Testing BATCH=64, LR=0.001\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.5341, val_acc=0.6911\n",
      "Epoch 02: train_acc=0.7441, val_acc=0.8011\n",
      "Epoch 03: train_acc=0.8150, val_acc=0.8203\n",
      "Epoch 04: train_acc=0.8746, val_acc=0.8332\n",
      "Epoch 05: train_acc=0.8984, val_acc=0.8405\n",
      "Epoch 06: train_acc=0.9356, val_acc=0.8359\n",
      "Epoch 07: train_acc=0.9486, val_acc=0.8332\n",
      "Epoch 08: train_acc=0.9739, val_acc=0.8405\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8405\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.5297, val_acc=0.6691\n",
      "Epoch 02: train_acc=0.7413, val_acc=0.7745\n",
      "Epoch 03: train_acc=0.8232, val_acc=0.8093\n",
      "Epoch 04: train_acc=0.8620, val_acc=0.8057\n",
      "Epoch 05: train_acc=0.9110, val_acc=0.8396\n",
      "Epoch 06: train_acc=0.9429, val_acc=0.8066\n",
      "Epoch 07: train_acc=0.9523, val_acc=0.8231\n",
      "Epoch 08: train_acc=0.9720, val_acc=0.8213\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8396\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.5509, val_acc=0.6982\n",
      "Epoch 02: train_acc=0.7501, val_acc=0.7817\n",
      "Epoch 03: train_acc=0.8196, val_acc=0.8000\n",
      "Epoch 04: train_acc=0.8565, val_acc=0.8156\n",
      "Epoch 05: train_acc=0.9037, val_acc=0.8275\n",
      "Epoch 06: train_acc=0.9308, val_acc=0.8266\n",
      "Epoch 07: train_acc=0.9528, val_acc=0.8367\n",
      "Epoch 08: train_acc=0.9665, val_acc=0.8376\n",
      "Epoch 09: train_acc=0.9823, val_acc=0.8358\n",
      "Epoch 10: train_acc=0.9853, val_acc=0.8138\n",
      "Epoch 11: train_acc=0.9874, val_acc=0.8367\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8376\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.5268, val_acc=0.6706\n",
      "Epoch 02: train_acc=0.7393, val_acc=0.7431\n",
      "Epoch 03: train_acc=0.8235, val_acc=0.7761\n",
      "Epoch 04: train_acc=0.8696, val_acc=0.7862\n",
      "Epoch 05: train_acc=0.8955, val_acc=0.7991\n",
      "Epoch 06: train_acc=0.9381, val_acc=0.8349\n",
      "Epoch 07: train_acc=0.9516, val_acc=0.8239\n",
      "Epoch 08: train_acc=0.9688, val_acc=0.8376\n",
      "Epoch 09: train_acc=0.9805, val_acc=0.8266\n",
      "Epoch 10: train_acc=0.9892, val_acc=0.8284\n",
      "Epoch 11: train_acc=0.9899, val_acc=0.8404\n",
      "Epoch 12: train_acc=0.9940, val_acc=0.8312\n",
      "Epoch 13: train_acc=0.9961, val_acc=0.8394\n",
      "Epoch 14: train_acc=0.9961, val_acc=0.8303\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8404\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.5225, val_acc=0.7110\n",
      "Epoch 02: train_acc=0.7510, val_acc=0.7569\n",
      "Epoch 03: train_acc=0.8159, val_acc=0.8092\n",
      "Epoch 04: train_acc=0.8691, val_acc=0.8202\n",
      "Epoch 05: train_acc=0.9035, val_acc=0.8349\n",
      "Epoch 06: train_acc=0.9317, val_acc=0.8367\n",
      "Epoch 07: train_acc=0.9448, val_acc=0.8394\n",
      "Epoch 08: train_acc=0.9672, val_acc=0.8440\n",
      "Epoch 09: train_acc=0.9814, val_acc=0.8394\n",
      "Epoch 10: train_acc=0.9883, val_acc=0.8431\n",
      "Epoch 11: train_acc=0.9927, val_acc=0.8284\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8440\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8405\n",
      "Fold 2: 0.8396\n",
      "Fold 3: 0.8376\n",
      "Fold 4: 0.8404\n",
      "Fold 5: 0.8440\n",
      "Average Val Accuracy: 0.8404\n",
      "\n",
      "🔍 K-Fold Testing BATCH=64, LR=0.0005\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.4540, val_acc=0.5628\n",
      "Epoch 02: train_acc=0.6503, val_acc=0.7204\n",
      "Epoch 03: train_acc=0.7535, val_acc=0.7855\n",
      "Epoch 04: train_acc=0.8147, val_acc=0.8176\n",
      "Epoch 05: train_acc=0.8386, val_acc=0.8203\n",
      "Epoch 06: train_acc=0.8691, val_acc=0.8249\n",
      "Epoch 07: train_acc=0.8920, val_acc=0.8359\n",
      "Epoch 08: train_acc=0.9200, val_acc=0.8405\n",
      "Epoch 09: train_acc=0.9310, val_acc=0.8396\n",
      "Epoch 10: train_acc=0.9475, val_acc=0.8524\n",
      "Epoch 11: train_acc=0.9624, val_acc=0.8478\n",
      "Epoch 12: train_acc=0.9697, val_acc=0.8515\n",
      "Epoch 13: train_acc=0.9782, val_acc=0.8478\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8524\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.4522, val_acc=0.6086\n",
      "Epoch 02: train_acc=0.6712, val_acc=0.7021\n",
      "Epoch 03: train_acc=0.7606, val_acc=0.7718\n",
      "Epoch 04: train_acc=0.8062, val_acc=0.7919\n",
      "Epoch 05: train_acc=0.8480, val_acc=0.8103\n",
      "Epoch 06: train_acc=0.8709, val_acc=0.8038\n",
      "Epoch 07: train_acc=0.8961, val_acc=0.8240\n",
      "Epoch 08: train_acc=0.9239, val_acc=0.8240\n",
      "Epoch 09: train_acc=0.9404, val_acc=0.8286\n",
      "Epoch 10: train_acc=0.9502, val_acc=0.8341\n",
      "Epoch 11: train_acc=0.9633, val_acc=0.8304\n",
      "Epoch 12: train_acc=0.9702, val_acc=0.8295\n",
      "Epoch 13: train_acc=0.9789, val_acc=0.8332\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8341\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.4631, val_acc=0.6128\n",
      "Epoch 02: train_acc=0.6651, val_acc=0.7385\n",
      "Epoch 03: train_acc=0.7570, val_acc=0.7679\n",
      "Epoch 04: train_acc=0.8042, val_acc=0.7881\n",
      "Epoch 05: train_acc=0.8372, val_acc=0.8073\n",
      "Epoch 06: train_acc=0.8728, val_acc=0.8092\n",
      "Epoch 07: train_acc=0.8975, val_acc=0.8248\n",
      "Epoch 08: train_acc=0.9246, val_acc=0.8294\n",
      "Epoch 09: train_acc=0.9390, val_acc=0.8422\n",
      "Epoch 10: train_acc=0.9532, val_acc=0.8349\n",
      "Epoch 11: train_acc=0.9617, val_acc=0.8404\n",
      "Epoch 12: train_acc=0.9778, val_acc=0.8339\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8422\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.4443, val_acc=0.6339\n",
      "Epoch 02: train_acc=0.6674, val_acc=0.7110\n",
      "Epoch 03: train_acc=0.7698, val_acc=0.7505\n",
      "Epoch 04: train_acc=0.8134, val_acc=0.7908\n",
      "Epoch 05: train_acc=0.8473, val_acc=0.8000\n",
      "Epoch 06: train_acc=0.8776, val_acc=0.8073\n",
      "Epoch 07: train_acc=0.9000, val_acc=0.7991\n",
      "Epoch 08: train_acc=0.9172, val_acc=0.8110\n",
      "Epoch 09: train_acc=0.9356, val_acc=0.8128\n",
      "Epoch 10: train_acc=0.9498, val_acc=0.8239\n",
      "Epoch 11: train_acc=0.9619, val_acc=0.8202\n",
      "Epoch 12: train_acc=0.9684, val_acc=0.8193\n",
      "Epoch 13: train_acc=0.9778, val_acc=0.8339\n",
      "Epoch 14: train_acc=0.9851, val_acc=0.8257\n",
      "Epoch 15: train_acc=0.9881, val_acc=0.8248\n",
      "Epoch 16: train_acc=0.9927, val_acc=0.8358\n",
      "Epoch 17: train_acc=0.9940, val_acc=0.8303\n",
      "Epoch 18: train_acc=0.9968, val_acc=0.8312\n",
      "Epoch 19: train_acc=0.9970, val_acc=0.8321\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8358\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.4397, val_acc=0.5936\n",
      "Epoch 02: train_acc=0.6680, val_acc=0.7239\n",
      "Epoch 03: train_acc=0.7597, val_acc=0.7835\n",
      "Epoch 04: train_acc=0.8086, val_acc=0.8046\n",
      "Epoch 05: train_acc=0.8503, val_acc=0.8092\n",
      "Epoch 06: train_acc=0.8698, val_acc=0.8339\n",
      "Epoch 07: train_acc=0.9062, val_acc=0.8339\n",
      "Epoch 08: train_acc=0.9232, val_acc=0.8266\n",
      "Epoch 09: train_acc=0.9328, val_acc=0.8211\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8339\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8524\n",
      "Fold 2: 0.8341\n",
      "Fold 3: 0.8422\n",
      "Fold 4: 0.8358\n",
      "Fold 5: 0.8339\n",
      "Average Val Accuracy: 0.8397\n",
      "\n",
      "🔍 K-Fold Testing BATCH=64, LR=0.0001\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.3410, val_acc=0.4308\n",
      "Epoch 02: train_acc=0.4345, val_acc=0.4519\n",
      "Epoch 03: train_acc=0.4987, val_acc=0.5215\n",
      "Epoch 04: train_acc=0.5370, val_acc=0.5665\n",
      "Epoch 05: train_acc=0.5948, val_acc=0.5995\n",
      "Epoch 06: train_acc=0.6343, val_acc=0.6471\n",
      "Epoch 07: train_acc=0.6742, val_acc=0.6948\n",
      "Epoch 08: train_acc=0.7081, val_acc=0.7324\n",
      "Epoch 09: train_acc=0.7294, val_acc=0.7562\n",
      "Epoch 10: train_acc=0.7524, val_acc=0.7663\n",
      "Epoch 11: train_acc=0.7734, val_acc=0.7699\n",
      "Epoch 12: train_acc=0.7863, val_acc=0.7883\n",
      "Epoch 13: train_acc=0.8053, val_acc=0.8020\n",
      "Epoch 14: train_acc=0.8163, val_acc=0.8103\n",
      "Epoch 15: train_acc=0.8253, val_acc=0.8121\n",
      "Epoch 16: train_acc=0.8331, val_acc=0.8203\n",
      "Epoch 17: train_acc=0.8418, val_acc=0.8103\n",
      "Epoch 18: train_acc=0.8514, val_acc=0.8148\n",
      "Epoch 19: train_acc=0.8615, val_acc=0.8194\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8203\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.2892, val_acc=0.4464\n",
      "Epoch 02: train_acc=0.4235, val_acc=0.4895\n",
      "Epoch 03: train_acc=0.5104, val_acc=0.5490\n",
      "Epoch 04: train_acc=0.5597, val_acc=0.5738\n",
      "Epoch 05: train_acc=0.6079, val_acc=0.6123\n",
      "Epoch 06: train_acc=0.6531, val_acc=0.6709\n",
      "Epoch 07: train_acc=0.6893, val_acc=0.6957\n",
      "Epoch 08: train_acc=0.7134, val_acc=0.7039\n",
      "Epoch 09: train_acc=0.7292, val_acc=0.7177\n",
      "Epoch 10: train_acc=0.7503, val_acc=0.7250\n",
      "Epoch 11: train_acc=0.7705, val_acc=0.7342\n",
      "Epoch 12: train_acc=0.7792, val_acc=0.7672\n",
      "Epoch 13: train_acc=0.7948, val_acc=0.7654\n",
      "Epoch 14: train_acc=0.8115, val_acc=0.7626\n",
      "Epoch 15: train_acc=0.8147, val_acc=0.7800\n",
      "Epoch 16: train_acc=0.8292, val_acc=0.7910\n",
      "Epoch 17: train_acc=0.8358, val_acc=0.7938\n",
      "Epoch 18: train_acc=0.8461, val_acc=0.7919\n",
      "Epoch 19: train_acc=0.8535, val_acc=0.8057\n",
      "Epoch 20: train_acc=0.8620, val_acc=0.8103\n",
      "Epoch 21: train_acc=0.8691, val_acc=0.8020\n",
      "Epoch 22: train_acc=0.8746, val_acc=0.8130\n",
      "Epoch 23: train_acc=0.8769, val_acc=0.8167\n",
      "Epoch 24: train_acc=0.8853, val_acc=0.8075\n",
      "Epoch 25: train_acc=0.8890, val_acc=0.8176\n",
      "Epoch 26: train_acc=0.8975, val_acc=0.8121\n",
      "Epoch 27: train_acc=0.8989, val_acc=0.8176\n",
      "Epoch 28: train_acc=0.9092, val_acc=0.8084\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8176\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.3086, val_acc=0.4394\n",
      "Epoch 02: train_acc=0.4232, val_acc=0.4596\n",
      "Epoch 03: train_acc=0.5083, val_acc=0.5688\n",
      "Epoch 04: train_acc=0.5575, val_acc=0.6110\n",
      "Epoch 05: train_acc=0.6288, val_acc=0.6514\n",
      "Epoch 06: train_acc=0.6609, val_acc=0.6706\n",
      "Epoch 07: train_acc=0.6834, val_acc=0.6954\n",
      "Epoch 08: train_acc=0.7132, val_acc=0.7303\n",
      "Epoch 09: train_acc=0.7405, val_acc=0.7385\n",
      "Epoch 10: train_acc=0.7570, val_acc=0.7468\n",
      "Epoch 11: train_acc=0.7721, val_acc=0.7376\n",
      "Epoch 12: train_acc=0.7836, val_acc=0.7642\n",
      "Epoch 13: train_acc=0.7967, val_acc=0.7670\n",
      "Epoch 14: train_acc=0.8116, val_acc=0.7752\n",
      "Epoch 15: train_acc=0.8191, val_acc=0.7761\n",
      "Epoch 16: train_acc=0.8290, val_acc=0.7780\n",
      "Epoch 17: train_acc=0.8411, val_acc=0.7798\n",
      "Epoch 18: train_acc=0.8478, val_acc=0.7789\n",
      "Epoch 19: train_acc=0.8553, val_acc=0.7908\n",
      "Epoch 20: train_acc=0.8627, val_acc=0.7881\n",
      "Epoch 21: train_acc=0.8730, val_acc=0.7881\n",
      "Epoch 22: train_acc=0.8792, val_acc=0.7963\n",
      "Epoch 23: train_acc=0.8870, val_acc=0.7917\n",
      "Epoch 24: train_acc=0.8932, val_acc=0.8009\n",
      "Epoch 25: train_acc=0.8975, val_acc=0.8064\n",
      "Epoch 26: train_acc=0.9046, val_acc=0.7972\n",
      "Epoch 27: train_acc=0.9090, val_acc=0.8083\n",
      "Epoch 28: train_acc=0.9101, val_acc=0.8064\n",
      "Epoch 29: train_acc=0.9172, val_acc=0.8138\n",
      "Epoch 30: train_acc=0.9207, val_acc=0.8156\n",
      "Epoch 31: train_acc=0.9298, val_acc=0.8193\n",
      "Epoch 32: train_acc=0.9310, val_acc=0.8193\n",
      "Epoch 33: train_acc=0.9347, val_acc=0.8202\n",
      "Epoch 34: train_acc=0.9425, val_acc=0.8165\n",
      "Epoch 35: train_acc=0.9459, val_acc=0.8174\n",
      "Epoch 36: train_acc=0.9498, val_acc=0.8220\n",
      "Epoch 37: train_acc=0.9539, val_acc=0.8266\n",
      "Epoch 38: train_acc=0.9553, val_acc=0.8174\n",
      "Epoch 39: train_acc=0.9590, val_acc=0.8312\n",
      "Epoch 40: train_acc=0.9624, val_acc=0.8165\n",
      "Epoch 41: train_acc=0.9629, val_acc=0.8156\n",
      "Epoch 42: train_acc=0.9681, val_acc=0.8257\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8312\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.3141, val_acc=0.4514\n",
      "Epoch 02: train_acc=0.4159, val_acc=0.5193\n",
      "Epoch 03: train_acc=0.4991, val_acc=0.5624\n",
      "Epoch 04: train_acc=0.5699, val_acc=0.6110\n",
      "Epoch 05: train_acc=0.6032, val_acc=0.6294\n",
      "Epoch 06: train_acc=0.6527, val_acc=0.6798\n",
      "Epoch 07: train_acc=0.6921, val_acc=0.6789\n",
      "Epoch 08: train_acc=0.7185, val_acc=0.7064\n",
      "Epoch 09: train_acc=0.7464, val_acc=0.7248\n",
      "Epoch 10: train_acc=0.7652, val_acc=0.7303\n",
      "Epoch 11: train_acc=0.7847, val_acc=0.7330\n",
      "Epoch 12: train_acc=0.7983, val_acc=0.7468\n",
      "Epoch 13: train_acc=0.8127, val_acc=0.7523\n",
      "Epoch 14: train_acc=0.8237, val_acc=0.7615\n",
      "Epoch 15: train_acc=0.8322, val_acc=0.7651\n",
      "Epoch 16: train_acc=0.8414, val_acc=0.7771\n",
      "Epoch 17: train_acc=0.8450, val_acc=0.7697\n",
      "Epoch 18: train_acc=0.8595, val_acc=0.7844\n",
      "Epoch 19: train_acc=0.8638, val_acc=0.7835\n",
      "Epoch 20: train_acc=0.8716, val_acc=0.7890\n",
      "Epoch 21: train_acc=0.8751, val_acc=0.7972\n",
      "Epoch 22: train_acc=0.8849, val_acc=0.7917\n",
      "Epoch 23: train_acc=0.8923, val_acc=0.7972\n",
      "Epoch 24: train_acc=0.8980, val_acc=0.7890\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.7972\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.2971, val_acc=0.4128\n",
      "Epoch 02: train_acc=0.4078, val_acc=0.4917\n",
      "Epoch 03: train_acc=0.5183, val_acc=0.5514\n",
      "Epoch 04: train_acc=0.5621, val_acc=0.5954\n",
      "Epoch 05: train_acc=0.6231, val_acc=0.6394\n",
      "Epoch 06: train_acc=0.6655, val_acc=0.6743\n",
      "Epoch 07: train_acc=0.6983, val_acc=0.7009\n",
      "Epoch 08: train_acc=0.7162, val_acc=0.7229\n",
      "Epoch 09: train_acc=0.7380, val_acc=0.7339\n",
      "Epoch 10: train_acc=0.7623, val_acc=0.7550\n",
      "Epoch 11: train_acc=0.7792, val_acc=0.7615\n",
      "Epoch 12: train_acc=0.7893, val_acc=0.7651\n",
      "Epoch 13: train_acc=0.8028, val_acc=0.7817\n",
      "Epoch 14: train_acc=0.8138, val_acc=0.7771\n",
      "Epoch 15: train_acc=0.8193, val_acc=0.7761\n",
      "Epoch 16: train_acc=0.8276, val_acc=0.7945\n",
      "Epoch 17: train_acc=0.8381, val_acc=0.7991\n",
      "Epoch 18: train_acc=0.8457, val_acc=0.8037\n",
      "Epoch 19: train_acc=0.8563, val_acc=0.8110\n",
      "Epoch 20: train_acc=0.8631, val_acc=0.8138\n",
      "Epoch 21: train_acc=0.8693, val_acc=0.8202\n",
      "Epoch 22: train_acc=0.8803, val_acc=0.8193\n",
      "Epoch 23: train_acc=0.8858, val_acc=0.8193\n",
      "Epoch 24: train_acc=0.8927, val_acc=0.8101\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8202\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8203\n",
      "Fold 2: 0.8176\n",
      "Fold 3: 0.8312\n",
      "Fold 4: 0.7972\n",
      "Fold 5: 0.8202\n",
      "Average Val Accuracy: 0.8173\n",
      "\n",
      "🔍 K-Fold Testing BATCH=128, LR=0.005\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.5313, val_acc=0.6728\n",
      "Epoch 02: train_acc=0.6716, val_acc=0.7333\n",
      "Epoch 03: train_acc=0.7792, val_acc=0.7489\n",
      "Epoch 04: train_acc=0.8624, val_acc=0.8203\n",
      "Epoch 05: train_acc=0.8950, val_acc=0.7974\n",
      "Epoch 06: train_acc=0.9319, val_acc=0.8378\n",
      "Epoch 07: train_acc=0.9523, val_acc=0.8616\n",
      "Epoch 08: train_acc=0.9768, val_acc=0.8442\n",
      "Epoch 09: train_acc=0.9874, val_acc=0.8433\n",
      "Epoch 10: train_acc=0.9947, val_acc=0.8588\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8616\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.5572, val_acc=0.6141\n",
      "Epoch 02: train_acc=0.7464, val_acc=0.7287\n",
      "Epoch 03: train_acc=0.8106, val_acc=0.7791\n",
      "Epoch 04: train_acc=0.8803, val_acc=0.8093\n",
      "Epoch 05: train_acc=0.9353, val_acc=0.8121\n",
      "Epoch 06: train_acc=0.9690, val_acc=0.8203\n",
      "Epoch 07: train_acc=0.9656, val_acc=0.7965\n",
      "Epoch 08: train_acc=0.9138, val_acc=0.8066\n",
      "Epoch 09: train_acc=0.9697, val_acc=0.8231\n",
      "Epoch 10: train_acc=0.9927, val_acc=0.8176\n",
      "Epoch 11: train_acc=0.9977, val_acc=0.8185\n",
      "Epoch 12: train_acc=0.9993, val_acc=0.8167\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8231\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.5484, val_acc=0.7183\n",
      "Epoch 02: train_acc=0.7428, val_acc=0.7982\n",
      "Epoch 03: train_acc=0.8359, val_acc=0.7670\n",
      "Epoch 04: train_acc=0.8746, val_acc=0.8165\n",
      "Epoch 05: train_acc=0.9301, val_acc=0.8055\n",
      "Epoch 06: train_acc=0.9434, val_acc=0.8257\n",
      "Epoch 07: train_acc=0.9713, val_acc=0.8330\n",
      "Epoch 08: train_acc=0.9771, val_acc=0.8202\n",
      "Epoch 09: train_acc=0.9677, val_acc=0.8248\n",
      "Epoch 10: train_acc=0.9876, val_acc=0.8321\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8330\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.5280, val_acc=0.6661\n",
      "Epoch 02: train_acc=0.7380, val_acc=0.7202\n",
      "Epoch 03: train_acc=0.7886, val_acc=0.7743\n",
      "Epoch 04: train_acc=0.8776, val_acc=0.8083\n",
      "Epoch 05: train_acc=0.9179, val_acc=0.8193\n",
      "Epoch 06: train_acc=0.9461, val_acc=0.8083\n",
      "Epoch 07: train_acc=0.9475, val_acc=0.8092\n",
      "Epoch 08: train_acc=0.9736, val_acc=0.8266\n",
      "Epoch 09: train_acc=0.9734, val_acc=0.8119\n",
      "Epoch 10: train_acc=0.9913, val_acc=0.8349\n",
      "Epoch 11: train_acc=0.9968, val_acc=0.8385\n",
      "Epoch 12: train_acc=0.9993, val_acc=0.8394\n",
      "Epoch 13: train_acc=0.9995, val_acc=0.8394\n",
      "Epoch 14: train_acc=0.9998, val_acc=0.8385\n",
      "Epoch 15: train_acc=0.9998, val_acc=0.8394\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8394\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.5585, val_acc=0.6679\n",
      "Epoch 02: train_acc=0.7137, val_acc=0.7651\n",
      "Epoch 03: train_acc=0.8081, val_acc=0.7835\n",
      "Epoch 04: train_acc=0.8879, val_acc=0.8009\n",
      "Epoch 05: train_acc=0.9248, val_acc=0.8119\n",
      "Epoch 06: train_acc=0.9615, val_acc=0.8193\n",
      "Epoch 07: train_acc=0.9801, val_acc=0.8202\n",
      "Epoch 08: train_acc=0.9759, val_acc=0.8303\n",
      "Epoch 09: train_acc=0.9860, val_acc=0.8193\n",
      "Epoch 10: train_acc=0.9830, val_acc=0.8211\n",
      "Epoch 11: train_acc=0.9892, val_acc=0.8229\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8303\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8616\n",
      "Fold 2: 0.8231\n",
      "Fold 3: 0.8330\n",
      "Fold 4: 0.8394\n",
      "Fold 5: 0.8303\n",
      "Average Val Accuracy: 0.8375\n",
      "\n",
      "🔍 K-Fold Testing BATCH=128, LR=0.001\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.4384, val_acc=0.5949\n",
      "Epoch 02: train_acc=0.6579, val_acc=0.7278\n",
      "Epoch 03: train_acc=0.7526, val_acc=0.7993\n",
      "Epoch 04: train_acc=0.8170, val_acc=0.8066\n",
      "Epoch 05: train_acc=0.8576, val_acc=0.8176\n",
      "Epoch 06: train_acc=0.8895, val_acc=0.8258\n",
      "Epoch 07: train_acc=0.9005, val_acc=0.8313\n",
      "Epoch 08: train_acc=0.9360, val_acc=0.8313\n",
      "Epoch 09: train_acc=0.9541, val_acc=0.8368\n",
      "Epoch 10: train_acc=0.9608, val_acc=0.8543\n",
      "Epoch 11: train_acc=0.9796, val_acc=0.8460\n",
      "Epoch 12: train_acc=0.9842, val_acc=0.8460\n",
      "Epoch 13: train_acc=0.9915, val_acc=0.8423\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8543\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.4102, val_acc=0.6196\n",
      "Epoch 02: train_acc=0.6664, val_acc=0.7186\n",
      "Epoch 03: train_acc=0.7487, val_acc=0.7369\n",
      "Epoch 04: train_acc=0.8138, val_acc=0.8038\n",
      "Epoch 05: train_acc=0.8537, val_acc=0.8103\n",
      "Epoch 06: train_acc=0.8750, val_acc=0.8103\n",
      "Epoch 07: train_acc=0.9126, val_acc=0.8268\n",
      "Epoch 08: train_acc=0.9443, val_acc=0.8295\n",
      "Epoch 09: train_acc=0.9578, val_acc=0.8240\n",
      "Epoch 10: train_acc=0.9706, val_acc=0.8268\n",
      "Epoch 11: train_acc=0.9805, val_acc=0.8268\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8295\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.4425, val_acc=0.6312\n",
      "Epoch 02: train_acc=0.6657, val_acc=0.7413\n",
      "Epoch 03: train_acc=0.7597, val_acc=0.7780\n",
      "Epoch 04: train_acc=0.8164, val_acc=0.7954\n",
      "Epoch 05: train_acc=0.8430, val_acc=0.8156\n",
      "Epoch 06: train_acc=0.8801, val_acc=0.8073\n",
      "Epoch 07: train_acc=0.9062, val_acc=0.8248\n",
      "Epoch 08: train_acc=0.9312, val_acc=0.8083\n",
      "Epoch 09: train_acc=0.9477, val_acc=0.8385\n",
      "Epoch 10: train_acc=0.9617, val_acc=0.8229\n",
      "Epoch 11: train_acc=0.9624, val_acc=0.8321\n",
      "Epoch 12: train_acc=0.9789, val_acc=0.8321\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8385\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.4232, val_acc=0.6275\n",
      "Epoch 02: train_acc=0.6605, val_acc=0.6899\n",
      "Epoch 03: train_acc=0.7515, val_acc=0.7633\n",
      "Epoch 04: train_acc=0.8141, val_acc=0.7844\n",
      "Epoch 05: train_acc=0.8586, val_acc=0.7890\n",
      "Epoch 06: train_acc=0.8879, val_acc=0.7954\n",
      "Epoch 07: train_acc=0.9161, val_acc=0.8202\n",
      "Epoch 08: train_acc=0.9358, val_acc=0.8211\n",
      "Epoch 09: train_acc=0.9558, val_acc=0.8367\n",
      "Epoch 10: train_acc=0.9686, val_acc=0.8275\n",
      "Epoch 11: train_acc=0.9746, val_acc=0.8202\n",
      "Epoch 12: train_acc=0.9791, val_acc=0.8138\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8367\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.4195, val_acc=0.6046\n",
      "Epoch 02: train_acc=0.6786, val_acc=0.7028\n",
      "Epoch 03: train_acc=0.7620, val_acc=0.7844\n",
      "Epoch 04: train_acc=0.8230, val_acc=0.7899\n",
      "Epoch 05: train_acc=0.8402, val_acc=0.8229\n",
      "Epoch 06: train_acc=0.8952, val_acc=0.8064\n",
      "Epoch 07: train_acc=0.9092, val_acc=0.8330\n",
      "Epoch 08: train_acc=0.9372, val_acc=0.8266\n",
      "Epoch 09: train_acc=0.9505, val_acc=0.8413\n",
      "Epoch 10: train_acc=0.9697, val_acc=0.8376\n",
      "Epoch 11: train_acc=0.9826, val_acc=0.8165\n",
      "Epoch 12: train_acc=0.9883, val_acc=0.8275\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8413\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8543\n",
      "Fold 2: 0.8295\n",
      "Fold 3: 0.8385\n",
      "Fold 4: 0.8367\n",
      "Fold 5: 0.8413\n",
      "Average Val Accuracy: 0.8401\n",
      "\n",
      "🔍 K-Fold Testing BATCH=128, LR=0.0005\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.3655, val_acc=0.5160\n",
      "Epoch 02: train_acc=0.5643, val_acc=0.5619\n",
      "Epoch 03: train_acc=0.6407, val_acc=0.7067\n",
      "Epoch 04: train_acc=0.7274, val_acc=0.7681\n",
      "Epoch 05: train_acc=0.7826, val_acc=0.7874\n",
      "Epoch 06: train_acc=0.8184, val_acc=0.8231\n",
      "Epoch 07: train_acc=0.8317, val_acc=0.8066\n",
      "Epoch 08: train_acc=0.8633, val_acc=0.8203\n",
      "Epoch 09: train_acc=0.8796, val_acc=0.8222\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8231\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.3472, val_acc=0.5390\n",
      "Epoch 02: train_acc=0.5698, val_acc=0.6214\n",
      "Epoch 03: train_acc=0.6604, val_acc=0.6801\n",
      "Epoch 04: train_acc=0.7248, val_acc=0.7269\n",
      "Epoch 05: train_acc=0.7760, val_acc=0.7690\n",
      "Epoch 06: train_acc=0.8133, val_acc=0.7837\n",
      "Epoch 07: train_acc=0.8441, val_acc=0.8057\n",
      "Epoch 08: train_acc=0.8652, val_acc=0.7892\n",
      "Epoch 09: train_acc=0.8769, val_acc=0.8112\n",
      "Epoch 10: train_acc=0.9003, val_acc=0.8222\n",
      "Epoch 11: train_acc=0.9175, val_acc=0.8277\n",
      "Epoch 12: train_acc=0.9333, val_acc=0.8203\n",
      "Epoch 13: train_acc=0.9381, val_acc=0.8304\n",
      "Epoch 14: train_acc=0.9541, val_acc=0.8258\n",
      "Epoch 15: train_acc=0.9612, val_acc=0.8222\n",
      "Epoch 16: train_acc=0.9697, val_acc=0.8313\n",
      "Epoch 17: train_acc=0.9762, val_acc=0.8277\n",
      "Epoch 18: train_acc=0.9798, val_acc=0.8313\n",
      "Epoch 19: train_acc=0.9862, val_acc=0.8277\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8313\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.3702, val_acc=0.5413\n",
      "Epoch 02: train_acc=0.5754, val_acc=0.6468\n",
      "Epoch 03: train_acc=0.6648, val_acc=0.6872\n",
      "Epoch 04: train_acc=0.7283, val_acc=0.7541\n",
      "Epoch 05: train_acc=0.7801, val_acc=0.7817\n",
      "Epoch 06: train_acc=0.8063, val_acc=0.7872\n",
      "Epoch 07: train_acc=0.8301, val_acc=0.7963\n",
      "Epoch 08: train_acc=0.8624, val_acc=0.8101\n",
      "Epoch 09: train_acc=0.8870, val_acc=0.8119\n",
      "Epoch 10: train_acc=0.9023, val_acc=0.8211\n",
      "Epoch 11: train_acc=0.9143, val_acc=0.8229\n",
      "Epoch 12: train_acc=0.9315, val_acc=0.8119\n",
      "Epoch 13: train_acc=0.9434, val_acc=0.8275\n",
      "Epoch 14: train_acc=0.9580, val_acc=0.8339\n",
      "Epoch 15: train_acc=0.9601, val_acc=0.8275\n",
      "Epoch 16: train_acc=0.9661, val_acc=0.8266\n",
      "Epoch 17: train_acc=0.9752, val_acc=0.8193\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8339\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.3702, val_acc=0.5761\n",
      "Epoch 02: train_acc=0.5681, val_acc=0.6349\n",
      "Epoch 03: train_acc=0.6470, val_acc=0.6789\n",
      "Epoch 04: train_acc=0.7295, val_acc=0.7174\n",
      "Epoch 05: train_acc=0.7852, val_acc=0.7514\n",
      "Epoch 06: train_acc=0.8173, val_acc=0.7771\n",
      "Epoch 07: train_acc=0.8434, val_acc=0.7798\n",
      "Epoch 08: train_acc=0.8641, val_acc=0.7936\n",
      "Epoch 09: train_acc=0.8890, val_acc=0.7982\n",
      "Epoch 10: train_acc=0.9065, val_acc=0.8018\n",
      "Epoch 11: train_acc=0.9120, val_acc=0.8018\n",
      "Epoch 12: train_acc=0.9262, val_acc=0.8055\n",
      "Epoch 13: train_acc=0.9392, val_acc=0.8156\n",
      "Epoch 14: train_acc=0.9551, val_acc=0.8202\n",
      "Epoch 15: train_acc=0.9603, val_acc=0.8202\n",
      "Epoch 16: train_acc=0.9711, val_acc=0.8128\n",
      "Epoch 17: train_acc=0.9775, val_acc=0.8211\n",
      "Epoch 18: train_acc=0.9840, val_acc=0.8239\n",
      "Epoch 19: train_acc=0.9846, val_acc=0.8174\n",
      "Epoch 20: train_acc=0.9869, val_acc=0.8193\n",
      "Epoch 21: train_acc=0.9901, val_acc=0.8239\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8239\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.3508, val_acc=0.5651\n",
      "Epoch 02: train_acc=0.5718, val_acc=0.6101\n",
      "Epoch 03: train_acc=0.6701, val_acc=0.6890\n",
      "Epoch 04: train_acc=0.7425, val_acc=0.7495\n",
      "Epoch 05: train_acc=0.7889, val_acc=0.7706\n",
      "Epoch 06: train_acc=0.8157, val_acc=0.7890\n",
      "Epoch 07: train_acc=0.8331, val_acc=0.8147\n",
      "Epoch 08: train_acc=0.8654, val_acc=0.8266\n",
      "Epoch 09: train_acc=0.8865, val_acc=0.8257\n",
      "Epoch 10: train_acc=0.9078, val_acc=0.8312\n",
      "Epoch 11: train_acc=0.9211, val_acc=0.8376\n",
      "Epoch 12: train_acc=0.9381, val_acc=0.8339\n",
      "Epoch 13: train_acc=0.9473, val_acc=0.8358\n",
      "Epoch 14: train_acc=0.9558, val_acc=0.8339\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8376\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8231\n",
      "Fold 2: 0.8313\n",
      "Fold 3: 0.8339\n",
      "Fold 4: 0.8239\n",
      "Fold 5: 0.8376\n",
      "Average Val Accuracy: 0.8300\n",
      "\n",
      "🔍 K-Fold Testing BATCH=128, LR=0.0001\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.2921, val_acc=0.4005\n",
      "Epoch 02: train_acc=0.3978, val_acc=0.4189\n",
      "Epoch 03: train_acc=0.4079, val_acc=0.4684\n",
      "Epoch 04: train_acc=0.4703, val_acc=0.4830\n",
      "Epoch 05: train_acc=0.5001, val_acc=0.5124\n",
      "Epoch 06: train_acc=0.5302, val_acc=0.5380\n",
      "Epoch 07: train_acc=0.5487, val_acc=0.5591\n",
      "Epoch 08: train_acc=0.5813, val_acc=0.5793\n",
      "Epoch 09: train_acc=0.6111, val_acc=0.6214\n",
      "Epoch 10: train_acc=0.6377, val_acc=0.6334\n",
      "Epoch 11: train_acc=0.6581, val_acc=0.6599\n",
      "Epoch 12: train_acc=0.6797, val_acc=0.6975\n",
      "Epoch 13: train_acc=0.7019, val_acc=0.7113\n",
      "Epoch 14: train_acc=0.7111, val_acc=0.7397\n",
      "Epoch 15: train_acc=0.7287, val_acc=0.7415\n",
      "Epoch 16: train_acc=0.7517, val_acc=0.7544\n",
      "Epoch 17: train_acc=0.7640, val_acc=0.7644\n",
      "Epoch 18: train_acc=0.7737, val_acc=0.7809\n",
      "Epoch 19: train_acc=0.7854, val_acc=0.7901\n",
      "Epoch 20: train_acc=0.7987, val_acc=0.7929\n",
      "Epoch 21: train_acc=0.8017, val_acc=0.7947\n",
      "Epoch 22: train_acc=0.8159, val_acc=0.8038\n",
      "Epoch 23: train_acc=0.8184, val_acc=0.8020\n",
      "Epoch 24: train_acc=0.8333, val_acc=0.8002\n",
      "Epoch 25: train_acc=0.8372, val_acc=0.8011\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8038\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.2357, val_acc=0.2392\n",
      "Epoch 02: train_acc=0.3621, val_acc=0.4491\n",
      "Epoch 03: train_acc=0.4215, val_acc=0.4959\n",
      "Epoch 04: train_acc=0.4802, val_acc=0.4968\n",
      "Epoch 05: train_acc=0.5141, val_acc=0.5399\n",
      "Epoch 06: train_acc=0.5421, val_acc=0.5564\n",
      "Epoch 07: train_acc=0.5735, val_acc=0.5793\n",
      "Epoch 08: train_acc=0.5822, val_acc=0.6104\n",
      "Epoch 09: train_acc=0.6258, val_acc=0.6334\n",
      "Epoch 10: train_acc=0.6501, val_acc=0.6673\n",
      "Epoch 11: train_acc=0.6767, val_acc=0.6774\n",
      "Epoch 12: train_acc=0.6998, val_acc=0.6911\n",
      "Epoch 13: train_acc=0.7115, val_acc=0.7021\n",
      "Epoch 14: train_acc=0.7255, val_acc=0.6902\n",
      "Epoch 15: train_acc=0.7354, val_acc=0.7122\n",
      "Epoch 16: train_acc=0.7462, val_acc=0.7149\n",
      "Epoch 17: train_acc=0.7553, val_acc=0.7269\n",
      "Epoch 18: train_acc=0.7613, val_acc=0.7379\n",
      "Epoch 19: train_acc=0.7785, val_acc=0.7470\n",
      "Epoch 20: train_acc=0.7835, val_acc=0.7525\n",
      "Epoch 21: train_acc=0.7943, val_acc=0.7507\n",
      "Epoch 22: train_acc=0.8078, val_acc=0.7562\n",
      "Epoch 23: train_acc=0.8150, val_acc=0.7681\n",
      "Epoch 24: train_acc=0.8195, val_acc=0.7736\n",
      "Epoch 25: train_acc=0.8227, val_acc=0.7736\n",
      "Epoch 26: train_acc=0.8344, val_acc=0.7855\n",
      "Epoch 27: train_acc=0.8416, val_acc=0.7874\n",
      "Epoch 28: train_acc=0.8526, val_acc=0.7901\n",
      "Epoch 29: train_acc=0.8528, val_acc=0.7901\n",
      "Epoch 30: train_acc=0.8562, val_acc=0.7828\n",
      "Epoch 31: train_acc=0.8647, val_acc=0.8084\n",
      "Epoch 32: train_acc=0.8714, val_acc=0.8020\n",
      "Epoch 33: train_acc=0.8723, val_acc=0.8048\n",
      "Epoch 34: train_acc=0.8759, val_acc=0.8048\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8084\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.2602, val_acc=0.3541\n",
      "Epoch 02: train_acc=0.3744, val_acc=0.4394\n",
      "Epoch 03: train_acc=0.4147, val_acc=0.5156\n",
      "Epoch 04: train_acc=0.4917, val_acc=0.5266\n",
      "Epoch 05: train_acc=0.5085, val_acc=0.5615\n",
      "Epoch 06: train_acc=0.5424, val_acc=0.5651\n",
      "Epoch 07: train_acc=0.5745, val_acc=0.5844\n",
      "Epoch 08: train_acc=0.5972, val_acc=0.6450\n",
      "Epoch 09: train_acc=0.6387, val_acc=0.6532\n",
      "Epoch 10: train_acc=0.6655, val_acc=0.6771\n",
      "Epoch 11: train_acc=0.6857, val_acc=0.6890\n",
      "Epoch 12: train_acc=0.6997, val_acc=0.7064\n",
      "Epoch 13: train_acc=0.7075, val_acc=0.7092\n",
      "Epoch 14: train_acc=0.7235, val_acc=0.7183\n",
      "Epoch 15: train_acc=0.7419, val_acc=0.7284\n",
      "Epoch 16: train_acc=0.7549, val_acc=0.7358\n",
      "Epoch 17: train_acc=0.7680, val_acc=0.7440\n",
      "Epoch 18: train_acc=0.7774, val_acc=0.7440\n",
      "Epoch 19: train_acc=0.7836, val_acc=0.7587\n",
      "Epoch 20: train_acc=0.7967, val_acc=0.7569\n",
      "Epoch 21: train_acc=0.8003, val_acc=0.7587\n",
      "Epoch 22: train_acc=0.8109, val_acc=0.7587\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.7587\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.2850, val_acc=0.4037\n",
      "Epoch 02: train_acc=0.3698, val_acc=0.4119\n",
      "Epoch 03: train_acc=0.4033, val_acc=0.5257\n",
      "Epoch 04: train_acc=0.4700, val_acc=0.5468\n",
      "Epoch 05: train_acc=0.5163, val_acc=0.5550\n",
      "Epoch 06: train_acc=0.5401, val_acc=0.5679\n",
      "Epoch 07: train_acc=0.5812, val_acc=0.6119\n",
      "Epoch 08: train_acc=0.5878, val_acc=0.6239\n",
      "Epoch 09: train_acc=0.6100, val_acc=0.6440\n",
      "Epoch 10: train_acc=0.6453, val_acc=0.6587\n",
      "Epoch 11: train_acc=0.6756, val_acc=0.6642\n",
      "Epoch 12: train_acc=0.6951, val_acc=0.6807\n",
      "Epoch 13: train_acc=0.7107, val_acc=0.7018\n",
      "Epoch 14: train_acc=0.7311, val_acc=0.7119\n",
      "Epoch 15: train_acc=0.7469, val_acc=0.7110\n",
      "Epoch 16: train_acc=0.7627, val_acc=0.7193\n",
      "Epoch 17: train_acc=0.7751, val_acc=0.7275\n",
      "Epoch 18: train_acc=0.7854, val_acc=0.7294\n",
      "Epoch 19: train_acc=0.7928, val_acc=0.7376\n",
      "Epoch 20: train_acc=0.8038, val_acc=0.7376\n",
      "Epoch 21: train_acc=0.8159, val_acc=0.7468\n",
      "Epoch 22: train_acc=0.8237, val_acc=0.7560\n",
      "Epoch 23: train_acc=0.8287, val_acc=0.7596\n",
      "Epoch 24: train_acc=0.8333, val_acc=0.7661\n",
      "Epoch 25: train_acc=0.8393, val_acc=0.7734\n",
      "Epoch 26: train_acc=0.8496, val_acc=0.7734\n",
      "Epoch 27: train_acc=0.8503, val_acc=0.7817\n",
      "Epoch 28: train_acc=0.8569, val_acc=0.7844\n",
      "Epoch 29: train_acc=0.8654, val_acc=0.7789\n",
      "Epoch 30: train_acc=0.8686, val_acc=0.7853\n",
      "Epoch 31: train_acc=0.8757, val_acc=0.7872\n",
      "Epoch 32: train_acc=0.8801, val_acc=0.7844\n",
      "Epoch 33: train_acc=0.8812, val_acc=0.7872\n",
      "Epoch 34: train_acc=0.8867, val_acc=0.7908\n",
      "Epoch 35: train_acc=0.8929, val_acc=0.7982\n",
      "Epoch 36: train_acc=0.8978, val_acc=0.8009\n",
      "Epoch 37: train_acc=0.8989, val_acc=0.7908\n",
      "Epoch 38: train_acc=0.9055, val_acc=0.7991\n",
      "Epoch 39: train_acc=0.9094, val_acc=0.7945\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8009\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.2689, val_acc=0.3239\n",
      "Epoch 02: train_acc=0.3604, val_acc=0.3798\n",
      "Epoch 03: train_acc=0.3904, val_acc=0.4560\n",
      "Epoch 04: train_acc=0.4580, val_acc=0.5092\n",
      "Epoch 05: train_acc=0.5115, val_acc=0.5440\n",
      "Epoch 06: train_acc=0.5397, val_acc=0.5248\n",
      "Epoch 07: train_acc=0.5640, val_acc=0.5550\n",
      "Epoch 08: train_acc=0.5931, val_acc=0.6119\n",
      "Epoch 09: train_acc=0.6314, val_acc=0.6358\n",
      "Epoch 10: train_acc=0.6541, val_acc=0.6670\n",
      "Epoch 11: train_acc=0.6843, val_acc=0.6780\n",
      "Epoch 12: train_acc=0.6953, val_acc=0.6927\n",
      "Epoch 13: train_acc=0.7146, val_acc=0.7064\n",
      "Epoch 14: train_acc=0.7272, val_acc=0.7220\n",
      "Epoch 15: train_acc=0.7451, val_acc=0.7174\n",
      "Epoch 16: train_acc=0.7549, val_acc=0.7358\n",
      "Epoch 17: train_acc=0.7650, val_acc=0.7422\n",
      "Epoch 18: train_acc=0.7742, val_acc=0.7523\n",
      "Epoch 19: train_acc=0.7907, val_acc=0.7560\n",
      "Epoch 20: train_acc=0.7944, val_acc=0.7615\n",
      "Epoch 21: train_acc=0.8019, val_acc=0.7633\n",
      "Epoch 22: train_acc=0.8061, val_acc=0.7780\n",
      "Epoch 23: train_acc=0.8148, val_acc=0.7789\n",
      "Epoch 24: train_acc=0.8193, val_acc=0.7908\n",
      "Epoch 25: train_acc=0.8267, val_acc=0.7817\n",
      "Epoch 26: train_acc=0.8370, val_acc=0.7853\n",
      "Epoch 27: train_acc=0.8411, val_acc=0.7771\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.7908\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8038\n",
      "Fold 2: 0.8084\n",
      "Fold 3: 0.7587\n",
      "Fold 4: 0.8009\n",
      "Fold 5: 0.7908\n",
      "Average Val Accuracy: 0.7925\n",
      "\n",
      "🔍 K-Fold Testing BATCH=256, LR=0.005\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.4407, val_acc=0.5747\n",
      "Epoch 02: train_acc=0.6161, val_acc=0.6636\n",
      "Epoch 03: train_acc=0.7335, val_acc=0.7498\n",
      "Epoch 04: train_acc=0.8145, val_acc=0.7864\n",
      "Epoch 05: train_acc=0.8844, val_acc=0.7910\n",
      "Epoch 06: train_acc=0.8606, val_acc=0.7791\n",
      "Epoch 07: train_acc=0.8892, val_acc=0.7883\n",
      "Epoch 08: train_acc=0.9145, val_acc=0.8433\n",
      "Epoch 09: train_acc=0.9578, val_acc=0.8451\n",
      "Epoch 10: train_acc=0.9814, val_acc=0.8286\n",
      "Epoch 11: train_acc=0.9865, val_acc=0.8387\n",
      "Epoch 12: train_acc=0.9924, val_acc=0.8396\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8451\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.4545, val_acc=0.6251\n",
      "Epoch 02: train_acc=0.6856, val_acc=0.7012\n",
      "Epoch 03: train_acc=0.7411, val_acc=0.7864\n",
      "Epoch 04: train_acc=0.8161, val_acc=0.7516\n",
      "Epoch 05: train_acc=0.8312, val_acc=0.7754\n",
      "Epoch 06: train_acc=0.9108, val_acc=0.7919\n",
      "Epoch 07: train_acc=0.9509, val_acc=0.8020\n",
      "Epoch 08: train_acc=0.9679, val_acc=0.8167\n",
      "Epoch 09: train_acc=0.9700, val_acc=0.8139\n",
      "Epoch 10: train_acc=0.9869, val_acc=0.8148\n",
      "Epoch 11: train_acc=0.9934, val_acc=0.8130\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8167\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.4413, val_acc=0.6220\n",
      "Epoch 02: train_acc=0.6646, val_acc=0.7147\n",
      "Epoch 03: train_acc=0.7336, val_acc=0.7367\n",
      "Epoch 04: train_acc=0.8065, val_acc=0.7651\n",
      "Epoch 05: train_acc=0.8652, val_acc=0.7789\n",
      "Epoch 06: train_acc=0.8680, val_acc=0.7688\n",
      "Epoch 07: train_acc=0.8755, val_acc=0.8037\n",
      "Epoch 08: train_acc=0.9392, val_acc=0.8110\n",
      "Epoch 09: train_acc=0.9759, val_acc=0.8101\n",
      "Epoch 10: train_acc=0.9856, val_acc=0.8229\n",
      "Epoch 11: train_acc=0.9934, val_acc=0.8266\n",
      "Epoch 12: train_acc=0.9972, val_acc=0.8294\n",
      "Epoch 13: train_acc=0.9993, val_acc=0.8229\n",
      "Epoch 14: train_acc=0.9993, val_acc=0.8266\n",
      "Epoch 15: train_acc=0.9995, val_acc=0.8303\n",
      "Epoch 16: train_acc=0.9998, val_acc=0.8257\n",
      "Epoch 17: train_acc=0.9998, val_acc=0.8239\n",
      "Epoch 18: train_acc=0.9998, val_acc=0.8275\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8303\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.4519, val_acc=0.6092\n",
      "Epoch 02: train_acc=0.6204, val_acc=0.6945\n",
      "Epoch 03: train_acc=0.6921, val_acc=0.7385\n",
      "Epoch 04: train_acc=0.7765, val_acc=0.7633\n",
      "Epoch 05: train_acc=0.8517, val_acc=0.7624\n",
      "Epoch 06: train_acc=0.8377, val_acc=0.8064\n",
      "Epoch 07: train_acc=0.8780, val_acc=0.7991\n",
      "Epoch 08: train_acc=0.9452, val_acc=0.8257\n",
      "Epoch 09: train_acc=0.9640, val_acc=0.8220\n",
      "Epoch 10: train_acc=0.9853, val_acc=0.8303\n",
      "Epoch 11: train_acc=0.9881, val_acc=0.8202\n",
      "Epoch 12: train_acc=0.9917, val_acc=0.8174\n",
      "Epoch 13: train_acc=0.9945, val_acc=0.8128\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8303\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.4484, val_acc=0.6055\n",
      "Epoch 02: train_acc=0.6839, val_acc=0.6817\n",
      "Epoch 03: train_acc=0.7162, val_acc=0.7376\n",
      "Epoch 04: train_acc=0.8184, val_acc=0.7798\n",
      "Epoch 05: train_acc=0.8831, val_acc=0.7734\n",
      "Epoch 06: train_acc=0.8721, val_acc=0.7716\n",
      "Epoch 07: train_acc=0.9177, val_acc=0.8092\n",
      "Epoch 08: train_acc=0.9434, val_acc=0.7807\n",
      "Epoch 09: train_acc=0.9631, val_acc=0.8138\n",
      "Epoch 10: train_acc=0.9865, val_acc=0.8101\n",
      "Epoch 11: train_acc=0.9956, val_acc=0.8055\n",
      "Epoch 12: train_acc=0.9975, val_acc=0.8064\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8138\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8451\n",
      "Fold 2: 0.8167\n",
      "Fold 3: 0.8303\n",
      "Fold 4: 0.8303\n",
      "Fold 5: 0.8138\n",
      "Average Val Accuracy: 0.8272\n",
      "\n",
      "🔍 K-Fold Testing BATCH=256, LR=0.001\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.3504, val_acc=0.4812\n",
      "Epoch 02: train_acc=0.5324, val_acc=0.6114\n",
      "Epoch 03: train_acc=0.6501, val_acc=0.6783\n",
      "Epoch 04: train_acc=0.7113, val_acc=0.7314\n",
      "Epoch 05: train_acc=0.7689, val_acc=0.7846\n",
      "Epoch 06: train_acc=0.8184, val_acc=0.8029\n",
      "Epoch 07: train_acc=0.8471, val_acc=0.8249\n",
      "Epoch 08: train_acc=0.8764, val_acc=0.8258\n",
      "Epoch 09: train_acc=0.9044, val_acc=0.8176\n",
      "Epoch 10: train_acc=0.9145, val_acc=0.8350\n",
      "Epoch 11: train_acc=0.9333, val_acc=0.8332\n",
      "Epoch 12: train_acc=0.9443, val_acc=0.8396\n",
      "Epoch 13: train_acc=0.9555, val_acc=0.8359\n",
      "Epoch 14: train_acc=0.9649, val_acc=0.8350\n",
      "Epoch 15: train_acc=0.9791, val_acc=0.8359\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8396\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.3497, val_acc=0.4885\n",
      "Epoch 02: train_acc=0.5425, val_acc=0.6022\n",
      "Epoch 03: train_acc=0.6441, val_acc=0.6783\n",
      "Epoch 04: train_acc=0.7129, val_acc=0.7186\n",
      "Epoch 05: train_acc=0.7663, val_acc=0.7608\n",
      "Epoch 06: train_acc=0.8069, val_acc=0.7800\n",
      "Epoch 07: train_acc=0.8484, val_acc=0.8121\n",
      "Epoch 08: train_acc=0.8707, val_acc=0.8222\n",
      "Epoch 09: train_acc=0.8913, val_acc=0.8075\n",
      "Epoch 10: train_acc=0.9101, val_acc=0.8167\n",
      "Epoch 11: train_acc=0.9246, val_acc=0.8194\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8222\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.3487, val_acc=0.5807\n",
      "Epoch 02: train_acc=0.5566, val_acc=0.6229\n",
      "Epoch 03: train_acc=0.6573, val_acc=0.6798\n",
      "Epoch 04: train_acc=0.6946, val_acc=0.6945\n",
      "Epoch 05: train_acc=0.7448, val_acc=0.7624\n",
      "Epoch 06: train_acc=0.7994, val_acc=0.7908\n",
      "Epoch 07: train_acc=0.8386, val_acc=0.8037\n",
      "Epoch 08: train_acc=0.8716, val_acc=0.8064\n",
      "Epoch 09: train_acc=0.8991, val_acc=0.8248\n",
      "Epoch 10: train_acc=0.9122, val_acc=0.8275\n",
      "Epoch 11: train_acc=0.9312, val_acc=0.8193\n",
      "Epoch 12: train_acc=0.9512, val_acc=0.8211\n",
      "Epoch 13: train_acc=0.9608, val_acc=0.8303\n",
      "Epoch 14: train_acc=0.9684, val_acc=0.8220\n",
      "Epoch 15: train_acc=0.9773, val_acc=0.8248\n",
      "Epoch 16: train_acc=0.9817, val_acc=0.8239\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8303\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.3436, val_acc=0.5119\n",
      "Epoch 02: train_acc=0.5119, val_acc=0.6156\n",
      "Epoch 03: train_acc=0.6272, val_acc=0.6642\n",
      "Epoch 04: train_acc=0.7109, val_acc=0.7229\n",
      "Epoch 05: train_acc=0.7710, val_acc=0.7083\n",
      "Epoch 06: train_acc=0.8216, val_acc=0.7670\n",
      "Epoch 07: train_acc=0.8505, val_acc=0.7716\n",
      "Epoch 08: train_acc=0.8751, val_acc=0.7890\n",
      "Epoch 09: train_acc=0.8936, val_acc=0.7872\n",
      "Epoch 10: train_acc=0.9101, val_acc=0.7963\n",
      "Epoch 11: train_acc=0.9278, val_acc=0.8055\n",
      "Epoch 12: train_acc=0.9461, val_acc=0.8101\n",
      "Epoch 13: train_acc=0.9564, val_acc=0.8000\n",
      "Epoch 14: train_acc=0.9674, val_acc=0.7972\n",
      "Epoch 15: train_acc=0.9764, val_acc=0.8165\n",
      "Epoch 16: train_acc=0.9823, val_acc=0.8119\n",
      "Epoch 17: train_acc=0.9869, val_acc=0.8220\n",
      "Epoch 18: train_acc=0.9901, val_acc=0.8101\n",
      "Epoch 19: train_acc=0.9917, val_acc=0.8202\n",
      "Epoch 20: train_acc=0.9950, val_acc=0.8138\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8220\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.3489, val_acc=0.4881\n",
      "Epoch 02: train_acc=0.5105, val_acc=0.5991\n",
      "Epoch 03: train_acc=0.6447, val_acc=0.6789\n",
      "Epoch 04: train_acc=0.7102, val_acc=0.7294\n",
      "Epoch 05: train_acc=0.7696, val_acc=0.7651\n",
      "Epoch 06: train_acc=0.8182, val_acc=0.7862\n",
      "Epoch 07: train_acc=0.8398, val_acc=0.8183\n",
      "Epoch 08: train_acc=0.8689, val_acc=0.8128\n",
      "Epoch 09: train_acc=0.8934, val_acc=0.8294\n",
      "Epoch 10: train_acc=0.9136, val_acc=0.8257\n",
      "Epoch 11: train_acc=0.9358, val_acc=0.8303\n",
      "Epoch 12: train_acc=0.9489, val_acc=0.8303\n",
      "Epoch 13: train_acc=0.9633, val_acc=0.8349\n",
      "Epoch 14: train_acc=0.9704, val_acc=0.8183\n",
      "Epoch 15: train_acc=0.9757, val_acc=0.8275\n",
      "Epoch 16: train_acc=0.9729, val_acc=0.8239\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8349\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8396\n",
      "Fold 2: 0.8222\n",
      "Fold 3: 0.8303\n",
      "Fold 4: 0.8220\n",
      "Fold 5: 0.8349\n",
      "Average Val Accuracy: 0.8298\n",
      "\n",
      "🔍 K-Fold Testing BATCH=256, LR=0.0005\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.3213, val_acc=0.4051\n",
      "Epoch 02: train_acc=0.4116, val_acc=0.4830\n",
      "Epoch 03: train_acc=0.5593, val_acc=0.5454\n",
      "Epoch 04: train_acc=0.5861, val_acc=0.6169\n",
      "Epoch 05: train_acc=0.6473, val_acc=0.6810\n",
      "Epoch 06: train_acc=0.7086, val_acc=0.7388\n",
      "Epoch 07: train_acc=0.7420, val_acc=0.7525\n",
      "Epoch 08: train_acc=0.7721, val_acc=0.7947\n",
      "Epoch 09: train_acc=0.8104, val_acc=0.8020\n",
      "Epoch 10: train_acc=0.8321, val_acc=0.8130\n",
      "Epoch 11: train_acc=0.8565, val_acc=0.8148\n",
      "Epoch 12: train_acc=0.8720, val_acc=0.8231\n",
      "Epoch 13: train_acc=0.8840, val_acc=0.8304\n",
      "Epoch 14: train_acc=0.8966, val_acc=0.8185\n",
      "Epoch 15: train_acc=0.9074, val_acc=0.8203\n",
      "Epoch 16: train_acc=0.9186, val_acc=0.8231\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8304\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.3093, val_acc=0.4345\n",
      "Epoch 02: train_acc=0.4364, val_acc=0.5060\n",
      "Epoch 03: train_acc=0.5455, val_acc=0.5362\n",
      "Epoch 04: train_acc=0.5925, val_acc=0.6554\n",
      "Epoch 05: train_acc=0.6560, val_acc=0.6920\n",
      "Epoch 06: train_acc=0.6982, val_acc=0.7159\n",
      "Epoch 07: train_acc=0.7368, val_acc=0.7259\n",
      "Epoch 08: train_acc=0.7640, val_acc=0.7333\n",
      "Epoch 09: train_acc=0.7982, val_acc=0.7819\n",
      "Epoch 10: train_acc=0.8161, val_acc=0.7919\n",
      "Epoch 11: train_acc=0.8399, val_acc=0.8011\n",
      "Epoch 12: train_acc=0.8594, val_acc=0.7956\n",
      "Epoch 13: train_acc=0.8762, val_acc=0.7956\n",
      "Epoch 14: train_acc=0.8835, val_acc=0.8194\n",
      "Epoch 15: train_acc=0.8925, val_acc=0.8066\n",
      "Epoch 16: train_acc=0.9106, val_acc=0.8139\n",
      "Epoch 17: train_acc=0.9211, val_acc=0.8203\n",
      "Epoch 18: train_acc=0.9333, val_acc=0.8268\n",
      "Epoch 19: train_acc=0.9454, val_acc=0.8093\n",
      "Epoch 20: train_acc=0.9505, val_acc=0.8222\n",
      "Epoch 21: train_acc=0.9610, val_acc=0.8185\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8268\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.3106, val_acc=0.4284\n",
      "Epoch 02: train_acc=0.4230, val_acc=0.4936\n",
      "Epoch 03: train_acc=0.5376, val_acc=0.5560\n",
      "Epoch 04: train_acc=0.5993, val_acc=0.6633\n",
      "Epoch 05: train_acc=0.6710, val_acc=0.7037\n",
      "Epoch 06: train_acc=0.7130, val_acc=0.7312\n",
      "Epoch 07: train_acc=0.7398, val_acc=0.7532\n",
      "Epoch 08: train_acc=0.7799, val_acc=0.7688\n",
      "Epoch 09: train_acc=0.8054, val_acc=0.7743\n",
      "Epoch 10: train_acc=0.8251, val_acc=0.7780\n",
      "Epoch 11: train_acc=0.8485, val_acc=0.7862\n",
      "Epoch 12: train_acc=0.8549, val_acc=0.7734\n",
      "Epoch 13: train_acc=0.8730, val_acc=0.7908\n",
      "Epoch 14: train_acc=0.8867, val_acc=0.7945\n",
      "Epoch 15: train_acc=0.9012, val_acc=0.7982\n",
      "Epoch 16: train_acc=0.9127, val_acc=0.8147\n",
      "Epoch 17: train_acc=0.9301, val_acc=0.8193\n",
      "Epoch 18: train_acc=0.9399, val_acc=0.8211\n",
      "Epoch 19: train_acc=0.9489, val_acc=0.8165\n",
      "Epoch 20: train_acc=0.9569, val_acc=0.8138\n",
      "Epoch 21: train_acc=0.9594, val_acc=0.8128\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8211\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.3127, val_acc=0.4440\n",
      "Epoch 02: train_acc=0.4312, val_acc=0.5239\n",
      "Epoch 03: train_acc=0.5415, val_acc=0.5917\n",
      "Epoch 04: train_acc=0.6084, val_acc=0.6486\n",
      "Epoch 05: train_acc=0.6625, val_acc=0.6716\n",
      "Epoch 06: train_acc=0.7056, val_acc=0.7110\n",
      "Epoch 07: train_acc=0.7476, val_acc=0.7202\n",
      "Epoch 08: train_acc=0.7811, val_acc=0.7422\n",
      "Epoch 09: train_acc=0.8079, val_acc=0.7440\n",
      "Epoch 10: train_acc=0.8310, val_acc=0.7587\n",
      "Epoch 11: train_acc=0.8514, val_acc=0.7670\n",
      "Epoch 12: train_acc=0.8624, val_acc=0.7826\n",
      "Epoch 13: train_acc=0.8767, val_acc=0.7826\n",
      "Epoch 14: train_acc=0.8886, val_acc=0.7890\n",
      "Epoch 15: train_acc=0.9072, val_acc=0.7872\n",
      "Epoch 16: train_acc=0.9133, val_acc=0.7927\n",
      "Epoch 17: train_acc=0.9324, val_acc=0.7936\n",
      "Epoch 18: train_acc=0.9372, val_acc=0.8009\n",
      "Epoch 19: train_acc=0.9443, val_acc=0.7991\n",
      "Epoch 20: train_acc=0.9484, val_acc=0.8000\n",
      "Epoch 21: train_acc=0.9603, val_acc=0.7963\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8009\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.3086, val_acc=0.3716\n",
      "Epoch 02: train_acc=0.4175, val_acc=0.5422\n",
      "Epoch 03: train_acc=0.5254, val_acc=0.5073\n",
      "Epoch 04: train_acc=0.6000, val_acc=0.6422\n",
      "Epoch 05: train_acc=0.6667, val_acc=0.6725\n",
      "Epoch 06: train_acc=0.7006, val_acc=0.6963\n",
      "Epoch 07: train_acc=0.7460, val_acc=0.7330\n",
      "Epoch 08: train_acc=0.7788, val_acc=0.7596\n",
      "Epoch 09: train_acc=0.8056, val_acc=0.7780\n",
      "Epoch 10: train_acc=0.8244, val_acc=0.7734\n",
      "Epoch 11: train_acc=0.8441, val_acc=0.7945\n",
      "Epoch 12: train_acc=0.8572, val_acc=0.8028\n",
      "Epoch 13: train_acc=0.8741, val_acc=0.8174\n",
      "Epoch 14: train_acc=0.8906, val_acc=0.8174\n",
      "Epoch 15: train_acc=0.9033, val_acc=0.8248\n",
      "Epoch 16: train_acc=0.9156, val_acc=0.8156\n",
      "Epoch 17: train_acc=0.9255, val_acc=0.8275\n",
      "Epoch 18: train_acc=0.9381, val_acc=0.8183\n",
      "Epoch 19: train_acc=0.9470, val_acc=0.8220\n",
      "Epoch 20: train_acc=0.9558, val_acc=0.8220\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8275\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8304\n",
      "Fold 2: 0.8268\n",
      "Fold 3: 0.8211\n",
      "Fold 4: 0.8009\n",
      "Fold 5: 0.8275\n",
      "Average Val Accuracy: 0.8213\n",
      "\n",
      "🔍 K-Fold Testing BATCH=256, LR=0.0001\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.2346, val_acc=0.3639\n",
      "Epoch 02: train_acc=0.3405, val_acc=0.3850\n",
      "Epoch 03: train_acc=0.3921, val_acc=0.4134\n",
      "Epoch 04: train_acc=0.3905, val_acc=0.4180\n",
      "Epoch 05: train_acc=0.4217, val_acc=0.4491\n",
      "Epoch 06: train_acc=0.4460, val_acc=0.4675\n",
      "Epoch 07: train_acc=0.4577, val_acc=0.4775\n",
      "Epoch 08: train_acc=0.4893, val_acc=0.4904\n",
      "Epoch 09: train_acc=0.5079, val_acc=0.4959\n",
      "Epoch 10: train_acc=0.5178, val_acc=0.5105\n",
      "Epoch 11: train_acc=0.5322, val_acc=0.5316\n",
      "Epoch 12: train_acc=0.5501, val_acc=0.5435\n",
      "Epoch 13: train_acc=0.5639, val_acc=0.5573\n",
      "Epoch 14: train_acc=0.5790, val_acc=0.5655\n",
      "Epoch 15: train_acc=0.6012, val_acc=0.5830\n",
      "Epoch 16: train_acc=0.6010, val_acc=0.5958\n",
      "Epoch 17: train_acc=0.6274, val_acc=0.6260\n",
      "Epoch 18: train_acc=0.6320, val_acc=0.6416\n",
      "Epoch 19: train_acc=0.6599, val_acc=0.6535\n",
      "Epoch 20: train_acc=0.6776, val_acc=0.6755\n",
      "Epoch 21: train_acc=0.6820, val_acc=0.6911\n",
      "Epoch 22: train_acc=0.6971, val_acc=0.6994\n",
      "Epoch 23: train_acc=0.7067, val_acc=0.7039\n",
      "Epoch 24: train_acc=0.7193, val_acc=0.7259\n",
      "Epoch 25: train_acc=0.7246, val_acc=0.7269\n",
      "Epoch 26: train_acc=0.7411, val_acc=0.7369\n",
      "Epoch 27: train_acc=0.7443, val_acc=0.7452\n",
      "Epoch 28: train_acc=0.7574, val_acc=0.7571\n",
      "Epoch 29: train_acc=0.7663, val_acc=0.7571\n",
      "Epoch 30: train_acc=0.7753, val_acc=0.7635\n",
      "Epoch 31: train_acc=0.7810, val_acc=0.7764\n",
      "Epoch 32: train_acc=0.7884, val_acc=0.7819\n",
      "Epoch 33: train_acc=0.7987, val_acc=0.7819\n",
      "Epoch 34: train_acc=0.7998, val_acc=0.7855\n",
      "Epoch 35: train_acc=0.8115, val_acc=0.7874\n",
      "Epoch 36: train_acc=0.8108, val_acc=0.7947\n",
      "Epoch 37: train_acc=0.8244, val_acc=0.7883\n",
      "Epoch 38: train_acc=0.8317, val_acc=0.7956\n",
      "Epoch 39: train_acc=0.8344, val_acc=0.7956\n",
      "Epoch 40: train_acc=0.8342, val_acc=0.7993\n",
      "Epoch 41: train_acc=0.8459, val_acc=0.7947\n",
      "Epoch 42: train_acc=0.8487, val_acc=0.7956\n",
      "Epoch 43: train_acc=0.8553, val_acc=0.7956\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.7993\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.2172, val_acc=0.2108\n",
      "Epoch 02: train_acc=0.2642, val_acc=0.2466\n",
      "Epoch 03: train_acc=0.3362, val_acc=0.3850\n",
      "Epoch 04: train_acc=0.3582, val_acc=0.4207\n",
      "Epoch 05: train_acc=0.4166, val_acc=0.4629\n",
      "Epoch 06: train_acc=0.4490, val_acc=0.4794\n",
      "Epoch 07: train_acc=0.4708, val_acc=0.4931\n",
      "Epoch 08: train_acc=0.5031, val_acc=0.5041\n",
      "Epoch 09: train_acc=0.5237, val_acc=0.5160\n",
      "Epoch 10: train_acc=0.5318, val_acc=0.5344\n",
      "Epoch 11: train_acc=0.5455, val_acc=0.5435\n",
      "Epoch 12: train_acc=0.5517, val_acc=0.5646\n",
      "Epoch 13: train_acc=0.5758, val_acc=0.5839\n",
      "Epoch 14: train_acc=0.5778, val_acc=0.5912\n",
      "Epoch 15: train_acc=0.6040, val_acc=0.5949\n",
      "Epoch 16: train_acc=0.6205, val_acc=0.6214\n",
      "Epoch 17: train_acc=0.6411, val_acc=0.6389\n",
      "Epoch 18: train_acc=0.6512, val_acc=0.6563\n",
      "Epoch 19: train_acc=0.6748, val_acc=0.6664\n",
      "Epoch 20: train_acc=0.6845, val_acc=0.6728\n",
      "Epoch 21: train_acc=0.6902, val_acc=0.6838\n",
      "Epoch 22: train_acc=0.7010, val_acc=0.6792\n",
      "Epoch 23: train_acc=0.7083, val_acc=0.6929\n",
      "Epoch 24: train_acc=0.7131, val_acc=0.6929\n",
      "Epoch 25: train_acc=0.7251, val_acc=0.6984\n",
      "Epoch 26: train_acc=0.7319, val_acc=0.7049\n",
      "Epoch 27: train_acc=0.7363, val_acc=0.7058\n",
      "Epoch 28: train_acc=0.7443, val_acc=0.7159\n",
      "Epoch 29: train_acc=0.7487, val_acc=0.7177\n",
      "Epoch 30: train_acc=0.7567, val_acc=0.7204\n",
      "Epoch 31: train_acc=0.7698, val_acc=0.7214\n",
      "Epoch 32: train_acc=0.7718, val_acc=0.7296\n",
      "Epoch 33: train_acc=0.7744, val_acc=0.7351\n",
      "Epoch 34: train_acc=0.7888, val_acc=0.7333\n",
      "Epoch 35: train_acc=0.7856, val_acc=0.7507\n",
      "Epoch 36: train_acc=0.7996, val_acc=0.7434\n",
      "Epoch 37: train_acc=0.8026, val_acc=0.7553\n",
      "Epoch 38: train_acc=0.8131, val_acc=0.7571\n",
      "Epoch 39: train_acc=0.8108, val_acc=0.7608\n",
      "Epoch 40: train_acc=0.8205, val_acc=0.7699\n",
      "Epoch 41: train_acc=0.8241, val_acc=0.7709\n",
      "Epoch 42: train_acc=0.8296, val_acc=0.7654\n",
      "Epoch 43: train_acc=0.8354, val_acc=0.7718\n",
      "Epoch 44: train_acc=0.8406, val_acc=0.7745\n",
      "Epoch 45: train_acc=0.8468, val_acc=0.7773\n",
      "Epoch 46: train_acc=0.8480, val_acc=0.7782\n",
      "Epoch 47: train_acc=0.8539, val_acc=0.7855\n",
      "Epoch 48: train_acc=0.8617, val_acc=0.7846\n",
      "Epoch 49: train_acc=0.8622, val_acc=0.7864\n",
      "Epoch 50: train_acc=0.8610, val_acc=0.7901\n",
      "Fold 2 best val_acc=0.7901\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.2276, val_acc=0.2385\n",
      "Epoch 02: train_acc=0.2971, val_acc=0.3661\n",
      "Epoch 03: train_acc=0.3549, val_acc=0.4064\n",
      "Epoch 04: train_acc=0.3913, val_acc=0.4477\n",
      "Epoch 05: train_acc=0.4161, val_acc=0.4853\n",
      "Epoch 06: train_acc=0.4441, val_acc=0.4798\n",
      "Epoch 07: train_acc=0.4585, val_acc=0.5046\n",
      "Epoch 08: train_acc=0.4938, val_acc=0.5321\n",
      "Epoch 09: train_acc=0.5181, val_acc=0.5440\n",
      "Epoch 10: train_acc=0.5312, val_acc=0.5394\n",
      "Epoch 11: train_acc=0.5367, val_acc=0.5569\n",
      "Epoch 12: train_acc=0.5630, val_acc=0.5826\n",
      "Epoch 13: train_acc=0.5809, val_acc=0.5927\n",
      "Epoch 14: train_acc=0.5903, val_acc=0.6028\n",
      "Epoch 15: train_acc=0.6194, val_acc=0.6294\n",
      "Epoch 16: train_acc=0.6424, val_acc=0.6394\n",
      "Epoch 17: train_acc=0.6534, val_acc=0.6697\n",
      "Epoch 18: train_acc=0.6683, val_acc=0.6734\n",
      "Epoch 19: train_acc=0.6800, val_acc=0.6917\n",
      "Epoch 20: train_acc=0.6923, val_acc=0.6817\n",
      "Epoch 21: train_acc=0.7027, val_acc=0.6982\n",
      "Epoch 22: train_acc=0.7121, val_acc=0.6963\n",
      "Epoch 23: train_acc=0.7201, val_acc=0.7028\n",
      "Epoch 24: train_acc=0.7260, val_acc=0.7028\n",
      "Epoch 25: train_acc=0.7288, val_acc=0.7128\n",
      "Epoch 26: train_acc=0.7448, val_acc=0.7202\n",
      "Epoch 27: train_acc=0.7458, val_acc=0.7312\n",
      "Epoch 28: train_acc=0.7655, val_acc=0.7275\n",
      "Epoch 29: train_acc=0.7662, val_acc=0.7312\n",
      "Epoch 30: train_acc=0.7737, val_acc=0.7339\n",
      "Epoch 31: train_acc=0.7834, val_acc=0.7339\n",
      "Epoch 32: train_acc=0.7829, val_acc=0.7413\n",
      "Epoch 33: train_acc=0.7902, val_acc=0.7440\n",
      "Epoch 34: train_acc=0.8006, val_acc=0.7468\n",
      "Epoch 35: train_acc=0.8040, val_acc=0.7505\n",
      "Epoch 36: train_acc=0.8093, val_acc=0.7523\n",
      "Epoch 37: train_acc=0.8161, val_acc=0.7477\n",
      "Epoch 38: train_acc=0.8177, val_acc=0.7578\n",
      "Epoch 39: train_acc=0.8235, val_acc=0.7550\n",
      "Epoch 40: train_acc=0.8317, val_acc=0.7541\n",
      "Epoch 41: train_acc=0.8336, val_acc=0.7606\n",
      "Epoch 42: train_acc=0.8388, val_acc=0.7477\n",
      "Epoch 43: train_acc=0.8361, val_acc=0.7670\n",
      "Epoch 44: train_acc=0.8482, val_acc=0.7670\n",
      "Epoch 45: train_acc=0.8524, val_acc=0.7670\n",
      "Epoch 46: train_acc=0.8597, val_acc=0.7651\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.7670\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.2318, val_acc=0.3505\n",
      "Epoch 02: train_acc=0.3138, val_acc=0.4119\n",
      "Epoch 03: train_acc=0.3510, val_acc=0.4128\n",
      "Epoch 04: train_acc=0.3757, val_acc=0.4110\n",
      "Epoch 05: train_acc=0.4122, val_acc=0.4798\n",
      "Epoch 06: train_acc=0.4436, val_acc=0.5101\n",
      "Epoch 07: train_acc=0.4626, val_acc=0.5413\n",
      "Epoch 08: train_acc=0.5002, val_acc=0.5633\n",
      "Epoch 09: train_acc=0.5204, val_acc=0.5651\n",
      "Epoch 10: train_acc=0.5413, val_acc=0.5734\n",
      "Epoch 11: train_acc=0.5552, val_acc=0.5908\n",
      "Epoch 12: train_acc=0.5672, val_acc=0.6009\n",
      "Epoch 13: train_acc=0.5892, val_acc=0.6165\n",
      "Epoch 14: train_acc=0.6041, val_acc=0.6248\n",
      "Epoch 15: train_acc=0.6169, val_acc=0.6358\n",
      "Epoch 16: train_acc=0.6300, val_acc=0.6358\n",
      "Epoch 17: train_acc=0.6463, val_acc=0.6450\n",
      "Epoch 18: train_acc=0.6614, val_acc=0.6541\n",
      "Epoch 19: train_acc=0.6786, val_acc=0.6596\n",
      "Epoch 20: train_acc=0.6894, val_acc=0.6725\n",
      "Epoch 21: train_acc=0.7024, val_acc=0.6807\n",
      "Epoch 22: train_acc=0.7038, val_acc=0.6771\n",
      "Epoch 23: train_acc=0.7169, val_acc=0.6899\n",
      "Epoch 24: train_acc=0.7343, val_acc=0.6899\n",
      "Epoch 25: train_acc=0.7432, val_acc=0.7055\n",
      "Epoch 26: train_acc=0.7513, val_acc=0.7165\n",
      "Epoch 27: train_acc=0.7632, val_acc=0.7119\n",
      "Epoch 28: train_acc=0.7751, val_acc=0.7211\n",
      "Epoch 29: train_acc=0.7801, val_acc=0.7202\n",
      "Epoch 30: train_acc=0.7912, val_acc=0.7229\n",
      "Epoch 31: train_acc=0.7893, val_acc=0.7266\n",
      "Epoch 32: train_acc=0.8001, val_acc=0.7376\n",
      "Epoch 33: train_acc=0.8058, val_acc=0.7312\n",
      "Epoch 34: train_acc=0.8145, val_acc=0.7358\n",
      "Epoch 35: train_acc=0.8148, val_acc=0.7404\n",
      "Epoch 36: train_acc=0.8187, val_acc=0.7459\n",
      "Epoch 37: train_acc=0.8258, val_acc=0.7486\n",
      "Epoch 38: train_acc=0.8283, val_acc=0.7523\n",
      "Epoch 39: train_acc=0.8315, val_acc=0.7606\n",
      "Epoch 40: train_acc=0.8363, val_acc=0.7670\n",
      "Epoch 41: train_acc=0.8411, val_acc=0.7633\n",
      "Epoch 42: train_acc=0.8503, val_acc=0.7651\n",
      "Epoch 43: train_acc=0.8521, val_acc=0.7679\n",
      "Epoch 44: train_acc=0.8576, val_acc=0.7706\n",
      "Epoch 45: train_acc=0.8615, val_acc=0.7734\n",
      "Epoch 46: train_acc=0.8618, val_acc=0.7706\n",
      "Epoch 47: train_acc=0.8673, val_acc=0.7761\n",
      "Epoch 48: train_acc=0.8721, val_acc=0.7798\n",
      "Epoch 49: train_acc=0.8746, val_acc=0.7752\n",
      "Epoch 50: train_acc=0.8858, val_acc=0.7752\n",
      "Fold 4 best val_acc=0.7798\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.2368, val_acc=0.3239\n",
      "Epoch 02: train_acc=0.3033, val_acc=0.3303\n",
      "Epoch 03: train_acc=0.3359, val_acc=0.3431\n",
      "Epoch 04: train_acc=0.3613, val_acc=0.3688\n",
      "Epoch 05: train_acc=0.3867, val_acc=0.3844\n",
      "Epoch 06: train_acc=0.4218, val_acc=0.4670\n",
      "Epoch 07: train_acc=0.4486, val_acc=0.4991\n",
      "Epoch 08: train_acc=0.4821, val_acc=0.5183\n",
      "Epoch 09: train_acc=0.5105, val_acc=0.5330\n",
      "Epoch 10: train_acc=0.5268, val_acc=0.5321\n",
      "Epoch 11: train_acc=0.5403, val_acc=0.5202\n",
      "Epoch 12: train_acc=0.5433, val_acc=0.5440\n",
      "Epoch 13: train_acc=0.5750, val_acc=0.5642\n",
      "Epoch 14: train_acc=0.5839, val_acc=0.5789\n",
      "Epoch 15: train_acc=0.6201, val_acc=0.6028\n",
      "Epoch 16: train_acc=0.6288, val_acc=0.6165\n",
      "Epoch 17: train_acc=0.6568, val_acc=0.6422\n",
      "Epoch 18: train_acc=0.6621, val_acc=0.6422\n",
      "Epoch 19: train_acc=0.6774, val_acc=0.6688\n",
      "Epoch 20: train_acc=0.6910, val_acc=0.6743\n",
      "Epoch 21: train_acc=0.7013, val_acc=0.6697\n",
      "Epoch 22: train_acc=0.7036, val_acc=0.6927\n",
      "Epoch 23: train_acc=0.7160, val_acc=0.6982\n",
      "Epoch 24: train_acc=0.7249, val_acc=0.7110\n",
      "Epoch 25: train_acc=0.7348, val_acc=0.7202\n",
      "Epoch 26: train_acc=0.7432, val_acc=0.7229\n",
      "Epoch 27: train_acc=0.7506, val_acc=0.7266\n",
      "Epoch 28: train_acc=0.7639, val_acc=0.7303\n",
      "Epoch 29: train_acc=0.7703, val_acc=0.7284\n",
      "Epoch 30: train_acc=0.7795, val_acc=0.7404\n",
      "Epoch 31: train_acc=0.7797, val_acc=0.7486\n",
      "Epoch 32: train_acc=0.7916, val_acc=0.7459\n",
      "Epoch 33: train_acc=0.7948, val_acc=0.7541\n",
      "Epoch 34: train_acc=0.7999, val_acc=0.7569\n",
      "Epoch 35: train_acc=0.8072, val_acc=0.7569\n",
      "Epoch 36: train_acc=0.8120, val_acc=0.7679\n",
      "Epoch 37: train_acc=0.8152, val_acc=0.7679\n",
      "Epoch 38: train_acc=0.8177, val_acc=0.7661\n",
      "Epoch 39: train_acc=0.8304, val_acc=0.7679\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.7679\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.7993\n",
      "Fold 2: 0.7901\n",
      "Fold 3: 0.7670\n",
      "Fold 4: 0.7798\n",
      "Fold 5: 0.7679\n",
      "Average Val Accuracy: 0.7808\n",
      "\n",
      "🏆 Best (batch, lr) by K-Fold:\n",
      "batch_size           32\n",
      "lr                0.001\n",
      "cv_mean_acc     0.84501\n",
      "cv_std         0.006921\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Saved vars -> best_batch_size=32, best_lr=0.001\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# K-Fold grid search for (batch_size, lr)\n",
    "# ===========================\n",
    "import itertools, numpy as np, pandas as pd\n",
    "\n",
    "# ---- Your fixed model/config bits ----\n",
    "NUM_CLASSES   = len(LABEL.vocab)\n",
    "NUM_LAYERS    = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT       = 0.3\n",
    "MAX_EPOCHS    = 50\n",
    "PATIENCE      = 3\n",
    "SEED          = 42\n",
    "K_FOLDS       = 5\n",
    "\n",
    "batch_sizes       = [32, 64, 128, 256]\n",
    "learning_rates    = [5e-3, 1e-3, 5e-4, 1e-4 ]\n",
    "HIDDEN_DIM_FIXED  = 128   # fixed while tuning (batch, lr)\n",
    "\n",
    "# IMPORTANT: the full dataset for K-Fold (use your combined training set)\n",
    "FULL_DATASET = train_data   # <- if your full corpus variable has a different name, replace here\n",
    "\n",
    "results = []\n",
    "\n",
    "for bs, lr in itertools.product(batch_sizes, learning_rates):\n",
    "    print(f\"\\n🔍 K-Fold Testing BATCH={bs}, LR={lr}\")\n",
    "    mean_acc, fold_accs, fold_models = train_kfold_config(\n",
    "        k_folds=K_FOLDS,\n",
    "        batch_size=bs,\n",
    "        lr=lr,\n",
    "        hidden_dim=HIDDEN_DIM_FIXED,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        bidirectional=BIDIRECTIONAL,\n",
    "        dropout=DROPOUT,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        full_dataset=FULL_DATASET,\n",
    "        pad_idx=PAD_IDX,\n",
    "        device=device,\n",
    "        embedding_layer=embedding_layer,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        patience=PATIENCE,\n",
    "        seed=SEED\n",
    "    )\n",
    "    results.append({\n",
    "        \"batch_size\": bs,\n",
    "        \"lr\": lr,\n",
    "        \"cv_mean_acc\": float(mean_acc),\n",
    "        \"cv_std\": float(np.std(fold_accs)),\n",
    "        \"per_fold\": [float(x) for x in fold_accs],  # optional: inspect later\n",
    "    })\n",
    "\n",
    "# Rank by mean CV acc (desc), then by lower std (tie-breaker)\n",
    "df_results = (\n",
    "    pd.DataFrame(results)\n",
    "      .sort_values([\"cv_mean_acc\", \"cv_std\"], ascending=[False, True])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "best = df_results.loc[0]\n",
    "best_batch_size = int(best[\"batch_size\"])\n",
    "best_lr         = float(best[\"lr\"])\n",
    "\n",
    "print(\"\\n🏆 Best (batch, lr) by K-Fold:\")\n",
    "print(best[[\"batch_size\",\"lr\",\"cv_mean_acc\",\"cv_std\"]])\n",
    "print(f\"\\nSaved vars -> best_batch_size={best_batch_size}, best_lr={best_lr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best hidden dimension (with gradient clipping alrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 K-Fold Testing hidden_dim=64 (batch=32, lr=0.001)\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.5604, val_acc=0.7186\n",
      "Epoch 02: train_acc=0.7562, val_acc=0.8011\n",
      "Epoch 03: train_acc=0.8344, val_acc=0.8167\n",
      "Epoch 04: train_acc=0.8863, val_acc=0.8423\n",
      "Epoch 05: train_acc=0.9168, val_acc=0.8359\n",
      "Epoch 06: train_acc=0.9482, val_acc=0.8405\n",
      "Epoch 07: train_acc=0.9709, val_acc=0.8469\n",
      "Epoch 08: train_acc=0.9837, val_acc=0.8442\n",
      "Epoch 09: train_acc=0.9885, val_acc=0.8414\n",
      "Epoch 10: train_acc=0.9927, val_acc=0.8561\n",
      "Epoch 11: train_acc=0.9938, val_acc=0.8488\n",
      "Epoch 12: train_acc=0.9984, val_acc=0.8478\n",
      "Epoch 13: train_acc=0.9986, val_acc=0.8387\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8561\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.5641, val_acc=0.6929\n",
      "Epoch 02: train_acc=0.7643, val_acc=0.7929\n",
      "Epoch 03: train_acc=0.8377, val_acc=0.8112\n",
      "Epoch 04: train_acc=0.8897, val_acc=0.8222\n",
      "Epoch 05: train_acc=0.9266, val_acc=0.8249\n",
      "Epoch 06: train_acc=0.9539, val_acc=0.8213\n",
      "Epoch 07: train_acc=0.9700, val_acc=0.8277\n",
      "Epoch 08: train_acc=0.9810, val_acc=0.8213\n",
      "Epoch 09: train_acc=0.9872, val_acc=0.8341\n",
      "Epoch 10: train_acc=0.9922, val_acc=0.8286\n",
      "Epoch 11: train_acc=0.9950, val_acc=0.8222\n",
      "Epoch 12: train_acc=0.9959, val_acc=0.8268\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8341\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.5557, val_acc=0.7495\n",
      "Epoch 02: train_acc=0.7627, val_acc=0.8037\n",
      "Epoch 03: train_acc=0.8414, val_acc=0.8358\n",
      "Epoch 04: train_acc=0.9028, val_acc=0.8560\n",
      "Epoch 05: train_acc=0.9342, val_acc=0.8422\n",
      "Epoch 06: train_acc=0.9585, val_acc=0.8459\n",
      "Epoch 07: train_acc=0.9746, val_acc=0.8523\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8560\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.5757, val_acc=0.7009\n",
      "Epoch 02: train_acc=0.7806, val_acc=0.7670\n",
      "Epoch 03: train_acc=0.8510, val_acc=0.7963\n",
      "Epoch 04: train_acc=0.8975, val_acc=0.8110\n",
      "Epoch 05: train_acc=0.9310, val_acc=0.8101\n",
      "Epoch 06: train_acc=0.9521, val_acc=0.8128\n",
      "Epoch 07: train_acc=0.9695, val_acc=0.8055\n",
      "Epoch 08: train_acc=0.9789, val_acc=0.8284\n",
      "Epoch 09: train_acc=0.9895, val_acc=0.8257\n",
      "Epoch 10: train_acc=0.9922, val_acc=0.8211\n",
      "Epoch 11: train_acc=0.9936, val_acc=0.8119\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8284\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.5546, val_acc=0.7046\n",
      "Epoch 02: train_acc=0.7643, val_acc=0.8046\n",
      "Epoch 03: train_acc=0.8464, val_acc=0.8358\n",
      "Epoch 04: train_acc=0.8916, val_acc=0.8349\n",
      "Epoch 05: train_acc=0.9296, val_acc=0.8303\n",
      "Epoch 06: train_acc=0.9580, val_acc=0.8303\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8358\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8561\n",
      "Fold 2: 0.8341\n",
      "Fold 3: 0.8560\n",
      "Fold 4: 0.8284\n",
      "Fold 5: 0.8358\n",
      "Average Val Accuracy: 0.8421\n",
      "\n",
      "🧪 K-Fold Testing hidden_dim=96 (batch=32, lr=0.001)\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.5698, val_acc=0.7498\n",
      "Epoch 02: train_acc=0.7806, val_acc=0.7938\n",
      "Epoch 03: train_acc=0.8390, val_acc=0.8203\n",
      "Epoch 04: train_acc=0.8961, val_acc=0.8616\n",
      "Epoch 05: train_acc=0.9321, val_acc=0.8570\n",
      "Epoch 06: train_acc=0.9491, val_acc=0.8359\n",
      "Epoch 07: train_acc=0.9743, val_acc=0.8607\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8616\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.5815, val_acc=0.6838\n",
      "Epoch 02: train_acc=0.7778, val_acc=0.7874\n",
      "Epoch 03: train_acc=0.8374, val_acc=0.8084\n",
      "Epoch 04: train_acc=0.8902, val_acc=0.8423\n",
      "Epoch 05: train_acc=0.9317, val_acc=0.8396\n",
      "Epoch 06: train_acc=0.9590, val_acc=0.8323\n",
      "Epoch 07: train_acc=0.9702, val_acc=0.8323\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8423\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.5715, val_acc=0.7697\n",
      "Epoch 02: train_acc=0.7714, val_acc=0.7835\n",
      "Epoch 03: train_acc=0.8453, val_acc=0.8358\n",
      "Epoch 04: train_acc=0.9010, val_acc=0.8505\n",
      "Epoch 05: train_acc=0.9379, val_acc=0.8468\n",
      "Epoch 06: train_acc=0.9622, val_acc=0.8495\n",
      "Epoch 07: train_acc=0.9707, val_acc=0.8459\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8505\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.5715, val_acc=0.7211\n",
      "Epoch 02: train_acc=0.7882, val_acc=0.7312\n",
      "Epoch 03: train_acc=0.8436, val_acc=0.7991\n",
      "Epoch 04: train_acc=0.8978, val_acc=0.8165\n",
      "Epoch 05: train_acc=0.9326, val_acc=0.8220\n",
      "Epoch 06: train_acc=0.9569, val_acc=0.8229\n",
      "Epoch 07: train_acc=0.9720, val_acc=0.8229\n",
      "Epoch 08: train_acc=0.9842, val_acc=0.8358\n",
      "Epoch 09: train_acc=0.9862, val_acc=0.8229\n",
      "Epoch 10: train_acc=0.9913, val_acc=0.8202\n",
      "Epoch 11: train_acc=0.9968, val_acc=0.8183\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8358\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.5860, val_acc=0.7339\n",
      "Epoch 02: train_acc=0.7719, val_acc=0.8183\n",
      "Epoch 03: train_acc=0.8457, val_acc=0.8229\n",
      "Epoch 04: train_acc=0.8939, val_acc=0.8174\n",
      "Epoch 05: train_acc=0.9326, val_acc=0.8275\n",
      "Epoch 06: train_acc=0.9558, val_acc=0.8367\n",
      "Epoch 07: train_acc=0.9748, val_acc=0.8321\n",
      "Epoch 08: train_acc=0.9858, val_acc=0.8165\n",
      "Epoch 09: train_acc=0.9908, val_acc=0.8275\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8367\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8616\n",
      "Fold 2: 0.8423\n",
      "Fold 3: 0.8505\n",
      "Fold 4: 0.8358\n",
      "Fold 5: 0.8367\n",
      "Average Val Accuracy: 0.8454\n",
      "\n",
      "🧪 K-Fold Testing hidden_dim=128 (batch=32, lr=0.001)\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.5845, val_acc=0.7553\n",
      "Epoch 02: train_acc=0.7913, val_acc=0.8084\n",
      "Epoch 03: train_acc=0.8500, val_acc=0.8231\n",
      "Epoch 04: train_acc=0.9003, val_acc=0.8478\n",
      "Epoch 05: train_acc=0.9257, val_acc=0.8543\n",
      "Epoch 06: train_acc=0.9573, val_acc=0.8368\n",
      "Epoch 07: train_acc=0.9713, val_acc=0.8442\n",
      "Epoch 08: train_acc=0.9844, val_acc=0.8561\n",
      "Epoch 09: train_acc=0.9915, val_acc=0.8497\n",
      "Epoch 10: train_acc=0.9952, val_acc=0.8478\n",
      "Epoch 11: train_acc=0.9936, val_acc=0.8451\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8561\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.5893, val_acc=0.7608\n",
      "Epoch 02: train_acc=0.7939, val_acc=0.7855\n",
      "Epoch 03: train_acc=0.8519, val_acc=0.8103\n",
      "Epoch 04: train_acc=0.8989, val_acc=0.8368\n",
      "Epoch 05: train_acc=0.9319, val_acc=0.8258\n",
      "Epoch 06: train_acc=0.9580, val_acc=0.8332\n",
      "Epoch 07: train_acc=0.9796, val_acc=0.8194\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8368\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.6061, val_acc=0.7651\n",
      "Epoch 02: train_acc=0.7797, val_acc=0.7853\n",
      "Epoch 03: train_acc=0.8514, val_acc=0.8294\n",
      "Epoch 04: train_acc=0.9005, val_acc=0.8321\n",
      "Epoch 05: train_acc=0.9321, val_acc=0.8459\n",
      "Epoch 06: train_acc=0.9560, val_acc=0.8358\n",
      "Epoch 07: train_acc=0.9746, val_acc=0.8477\n",
      "Epoch 08: train_acc=0.9874, val_acc=0.8385\n",
      "Epoch 09: train_acc=0.9869, val_acc=0.8294\n",
      "Epoch 10: train_acc=0.9920, val_acc=0.8239\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8477\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.6045, val_acc=0.7211\n",
      "Epoch 02: train_acc=0.8003, val_acc=0.7835\n",
      "Epoch 03: train_acc=0.8574, val_acc=0.8073\n",
      "Epoch 04: train_acc=0.9021, val_acc=0.8046\n",
      "Epoch 05: train_acc=0.9376, val_acc=0.8037\n",
      "Epoch 06: train_acc=0.9555, val_acc=0.8147\n",
      "Epoch 07: train_acc=0.9725, val_acc=0.8248\n",
      "Epoch 08: train_acc=0.9895, val_acc=0.8385\n",
      "Epoch 09: train_acc=0.9842, val_acc=0.8239\n",
      "Epoch 10: train_acc=0.9899, val_acc=0.8220\n",
      "Epoch 11: train_acc=0.9931, val_acc=0.8312\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8385\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.6100, val_acc=0.7413\n",
      "Epoch 02: train_acc=0.7884, val_acc=0.8083\n",
      "Epoch 03: train_acc=0.8542, val_acc=0.8321\n",
      "Epoch 04: train_acc=0.8945, val_acc=0.8183\n",
      "Epoch 05: train_acc=0.9296, val_acc=0.8018\n",
      "Epoch 06: train_acc=0.9585, val_acc=0.8413\n",
      "Epoch 07: train_acc=0.9736, val_acc=0.8459\n",
      "Epoch 08: train_acc=0.9876, val_acc=0.8330\n",
      "Epoch 09: train_acc=0.9904, val_acc=0.8376\n",
      "Epoch 10: train_acc=0.9950, val_acc=0.8349\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8459\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8561\n",
      "Fold 2: 0.8368\n",
      "Fold 3: 0.8477\n",
      "Fold 4: 0.8385\n",
      "Fold 5: 0.8459\n",
      "Average Val Accuracy: 0.8450\n",
      "\n",
      "🧪 K-Fold Testing hidden_dim=192 (batch=32, lr=0.001)\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.6083, val_acc=0.7672\n",
      "Epoch 02: train_acc=0.7874, val_acc=0.7745\n",
      "Epoch 03: train_acc=0.8450, val_acc=0.8313\n",
      "Epoch 04: train_acc=0.8915, val_acc=0.8359\n",
      "Epoch 05: train_acc=0.9248, val_acc=0.8396\n",
      "Epoch 06: train_acc=0.9507, val_acc=0.8304\n",
      "Epoch 07: train_acc=0.9651, val_acc=0.8249\n",
      "Epoch 08: train_acc=0.9736, val_acc=0.8506\n",
      "Epoch 09: train_acc=0.9837, val_acc=0.8378\n",
      "Epoch 10: train_acc=0.9869, val_acc=0.8313\n",
      "Epoch 11: train_acc=0.9911, val_acc=0.8249\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8506\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.6017, val_acc=0.7434\n",
      "Epoch 02: train_acc=0.7884, val_acc=0.7819\n",
      "Epoch 03: train_acc=0.8454, val_acc=0.8130\n",
      "Epoch 04: train_acc=0.8936, val_acc=0.8121\n",
      "Epoch 05: train_acc=0.9195, val_acc=0.8213\n",
      "Epoch 06: train_acc=0.9518, val_acc=0.8258\n",
      "Epoch 07: train_acc=0.9677, val_acc=0.8258\n",
      "Epoch 08: train_acc=0.9734, val_acc=0.8268\n",
      "Epoch 09: train_acc=0.9858, val_acc=0.8203\n",
      "Epoch 10: train_acc=0.9929, val_acc=0.8350\n",
      "Epoch 11: train_acc=0.9959, val_acc=0.8286\n",
      "Epoch 12: train_acc=0.9954, val_acc=0.8258\n",
      "Epoch 13: train_acc=0.9672, val_acc=0.8002\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8350\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.6139, val_acc=0.7670\n",
      "Epoch 02: train_acc=0.7801, val_acc=0.7743\n",
      "Epoch 03: train_acc=0.8402, val_acc=0.8018\n",
      "Epoch 04: train_acc=0.8904, val_acc=0.8303\n",
      "Epoch 05: train_acc=0.9312, val_acc=0.8275\n",
      "Epoch 06: train_acc=0.9500, val_acc=0.8220\n",
      "Epoch 07: train_acc=0.9619, val_acc=0.8376\n",
      "Epoch 08: train_acc=0.9901, val_acc=0.8404\n",
      "Epoch 09: train_acc=0.9929, val_acc=0.8239\n",
      "Epoch 10: train_acc=0.9727, val_acc=0.8275\n",
      "Epoch 11: train_acc=0.9954, val_acc=0.8284\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8404\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.6162, val_acc=0.7468\n",
      "Epoch 02: train_acc=0.7912, val_acc=0.7578\n",
      "Epoch 03: train_acc=0.8556, val_acc=0.7954\n",
      "Epoch 04: train_acc=0.8945, val_acc=0.7982\n",
      "Epoch 05: train_acc=0.9161, val_acc=0.8229\n",
      "Epoch 06: train_acc=0.9457, val_acc=0.8220\n",
      "Epoch 07: train_acc=0.9695, val_acc=0.8284\n",
      "Epoch 08: train_acc=0.9688, val_acc=0.7890\n",
      "Epoch 09: train_acc=0.9775, val_acc=0.8303\n",
      "Epoch 10: train_acc=0.9927, val_acc=0.8202\n",
      "Epoch 11: train_acc=0.9950, val_acc=0.8037\n",
      "Epoch 12: train_acc=0.9908, val_acc=0.8303\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8303\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.6178, val_acc=0.7505\n",
      "Epoch 02: train_acc=0.7811, val_acc=0.7972\n",
      "Epoch 03: train_acc=0.8535, val_acc=0.8165\n",
      "Epoch 04: train_acc=0.8856, val_acc=0.8101\n",
      "Epoch 05: train_acc=0.9108, val_acc=0.7908\n",
      "Epoch 06: train_acc=0.9383, val_acc=0.8239\n",
      "Epoch 07: train_acc=0.9624, val_acc=0.8450\n",
      "Epoch 08: train_acc=0.9826, val_acc=0.8128\n",
      "Epoch 09: train_acc=0.9883, val_acc=0.8367\n",
      "Epoch 10: train_acc=0.9821, val_acc=0.8358\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8450\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8506\n",
      "Fold 2: 0.8350\n",
      "Fold 3: 0.8404\n",
      "Fold 4: 0.8303\n",
      "Fold 5: 0.8450\n",
      "Average Val Accuracy: 0.8402\n",
      "\n",
      "🧪 K-Fold Testing hidden_dim=256 (batch=32, lr=0.001)\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.6157, val_acc=0.7516\n",
      "Epoch 02: train_acc=0.7716, val_acc=0.7736\n",
      "Epoch 03: train_acc=0.8340, val_acc=0.8203\n",
      "Epoch 04: train_acc=0.8656, val_acc=0.8103\n",
      "Epoch 05: train_acc=0.9119, val_acc=0.8368\n",
      "Epoch 06: train_acc=0.9303, val_acc=0.8442\n",
      "Epoch 07: train_acc=0.9436, val_acc=0.8359\n",
      "Epoch 08: train_acc=0.9663, val_acc=0.8213\n",
      "Epoch 09: train_acc=0.9704, val_acc=0.8194\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8442\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.6304, val_acc=0.7461\n",
      "Epoch 02: train_acc=0.7867, val_acc=0.7534\n",
      "Epoch 03: train_acc=0.8317, val_acc=0.8057\n",
      "Epoch 04: train_acc=0.8780, val_acc=0.8222\n",
      "Epoch 05: train_acc=0.9044, val_acc=0.8148\n",
      "Epoch 06: train_acc=0.9404, val_acc=0.8176\n",
      "Epoch 07: train_acc=0.9585, val_acc=0.7837\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8222\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.6431, val_acc=0.7339\n",
      "Epoch 02: train_acc=0.7801, val_acc=0.7890\n",
      "Epoch 03: train_acc=0.8361, val_acc=0.8147\n",
      "Epoch 04: train_acc=0.8858, val_acc=0.8064\n",
      "Epoch 05: train_acc=0.9182, val_acc=0.8147\n",
      "Epoch 06: train_acc=0.9319, val_acc=0.8330\n",
      "Epoch 07: train_acc=0.9475, val_acc=0.8275\n",
      "Epoch 08: train_acc=0.9734, val_acc=0.8239\n",
      "Epoch 09: train_acc=0.9872, val_acc=0.8229\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8330\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.6316, val_acc=0.7330\n",
      "Epoch 02: train_acc=0.7824, val_acc=0.7596\n",
      "Epoch 03: train_acc=0.8448, val_acc=0.7835\n",
      "Epoch 04: train_acc=0.8712, val_acc=0.7862\n",
      "Epoch 05: train_acc=0.9149, val_acc=0.8018\n",
      "Epoch 06: train_acc=0.9395, val_acc=0.8064\n",
      "Epoch 07: train_acc=0.9427, val_acc=0.8229\n",
      "Epoch 08: train_acc=0.9624, val_acc=0.8046\n",
      "Epoch 09: train_acc=0.9482, val_acc=0.8165\n",
      "Epoch 10: train_acc=0.9796, val_acc=0.8211\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8229\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.6217, val_acc=0.7523\n",
      "Epoch 02: train_acc=0.7801, val_acc=0.8073\n",
      "Epoch 03: train_acc=0.8519, val_acc=0.8239\n",
      "Epoch 04: train_acc=0.8787, val_acc=0.7780\n",
      "Epoch 05: train_acc=0.9117, val_acc=0.7633\n",
      "Epoch 06: train_acc=0.9422, val_acc=0.8220\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8239\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8442\n",
      "Fold 2: 0.8222\n",
      "Fold 3: 0.8330\n",
      "Fold 4: 0.8229\n",
      "Fold 5: 0.8239\n",
      "Average Val Accuracy: 0.8292\n",
      "\n",
      "🧪 K-Fold Testing hidden_dim=384 (batch=32, lr=0.001)\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.6139, val_acc=0.7470\n",
      "Epoch 02: train_acc=0.7175, val_acc=0.5976\n",
      "Epoch 03: train_acc=0.7553, val_acc=0.7177\n",
      "Epoch 04: train_acc=0.7780, val_acc=0.7956\n",
      "Epoch 05: train_acc=0.8684, val_acc=0.8185\n",
      "Epoch 06: train_acc=0.9021, val_acc=0.8213\n",
      "Epoch 07: train_acc=0.9211, val_acc=0.8313\n",
      "Epoch 08: train_acc=0.9406, val_acc=0.7837\n",
      "Epoch 09: train_acc=0.9486, val_acc=0.7864\n",
      "Epoch 10: train_acc=0.9337, val_acc=0.8038\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8313\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.6281, val_acc=0.7351\n",
      "Epoch 02: train_acc=0.7439, val_acc=0.7525\n",
      "Epoch 03: train_acc=0.8094, val_acc=0.7846\n",
      "Epoch 04: train_acc=0.8461, val_acc=0.7305\n",
      "Epoch 05: train_acc=0.8626, val_acc=0.7965\n",
      "Epoch 06: train_acc=0.9016, val_acc=0.7663\n",
      "Epoch 07: train_acc=0.9269, val_acc=0.7699\n",
      "Epoch 08: train_acc=0.8310, val_acc=0.8029\n",
      "Epoch 09: train_acc=0.9085, val_acc=0.7974\n",
      "Epoch 10: train_acc=0.9266, val_acc=0.7883\n",
      "Epoch 11: train_acc=0.9080, val_acc=0.8158\n",
      "Epoch 12: train_acc=0.9255, val_acc=0.7736\n",
      "Epoch 13: train_acc=0.9301, val_acc=0.8258\n",
      "Epoch 14: train_acc=0.9555, val_acc=0.8121\n",
      "Epoch 15: train_acc=0.9732, val_acc=0.8295\n",
      "Epoch 16: train_acc=0.9814, val_acc=0.8332\n",
      "Epoch 17: train_acc=0.9798, val_acc=0.8277\n",
      "Epoch 18: train_acc=0.9878, val_acc=0.8048\n",
      "Epoch 19: train_acc=0.9110, val_acc=0.8231\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8332\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.5963, val_acc=0.7367\n",
      "Epoch 02: train_acc=0.7336, val_acc=0.7615\n",
      "Epoch 03: train_acc=0.7930, val_acc=0.7367\n",
      "Epoch 04: train_acc=0.8361, val_acc=0.7872\n",
      "Epoch 05: train_acc=0.8620, val_acc=0.8018\n",
      "Epoch 06: train_acc=0.8496, val_acc=0.7046\n",
      "Epoch 07: train_acc=0.8136, val_acc=0.8211\n",
      "Epoch 08: train_acc=0.8897, val_acc=0.7165\n",
      "Epoch 09: train_acc=0.9337, val_acc=0.8147\n",
      "Epoch 10: train_acc=0.9342, val_acc=0.8174\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8211\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.6288, val_acc=0.7174\n",
      "Epoch 02: train_acc=0.7758, val_acc=0.7248\n",
      "Epoch 03: train_acc=0.8251, val_acc=0.7716\n",
      "Epoch 04: train_acc=0.8590, val_acc=0.7752\n",
      "Epoch 05: train_acc=0.8840, val_acc=0.7945\n",
      "Epoch 06: train_acc=0.7368, val_acc=0.7239\n",
      "Epoch 07: train_acc=0.8446, val_acc=0.7367\n",
      "Epoch 08: train_acc=0.8547, val_acc=0.7954\n",
      "Epoch 09: train_acc=0.9021, val_acc=0.7835\n",
      "Epoch 10: train_acc=0.8980, val_acc=0.7963\n",
      "Epoch 11: train_acc=0.9276, val_acc=0.7523\n",
      "Epoch 12: train_acc=0.9459, val_acc=0.8055\n",
      "Epoch 13: train_acc=0.9729, val_acc=0.8156\n",
      "Epoch 14: train_acc=0.9803, val_acc=0.8174\n",
      "Epoch 15: train_acc=0.9819, val_acc=0.8321\n",
      "Epoch 16: train_acc=0.9927, val_acc=0.8339\n",
      "Epoch 17: train_acc=0.9869, val_acc=0.8385\n",
      "Epoch 18: train_acc=0.9924, val_acc=0.8073\n",
      "Epoch 19: train_acc=0.9885, val_acc=0.8404\n",
      "Epoch 20: train_acc=0.9991, val_acc=0.8505\n",
      "Epoch 21: train_acc=1.0000, val_acc=0.8495\n",
      "Epoch 22: train_acc=1.0000, val_acc=0.8505\n",
      "Epoch 23: train_acc=1.0000, val_acc=0.8486\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8505\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.6302, val_acc=0.7083\n",
      "Epoch 02: train_acc=0.7561, val_acc=0.7633\n",
      "Epoch 03: train_acc=0.8177, val_acc=0.7917\n",
      "Epoch 04: train_acc=0.8572, val_acc=0.7569\n",
      "Epoch 05: train_acc=0.8223, val_acc=0.8018\n",
      "Epoch 06: train_acc=0.8517, val_acc=0.7495\n",
      "Epoch 07: train_acc=0.9060, val_acc=0.8018\n",
      "Epoch 08: train_acc=0.9395, val_acc=0.8046\n",
      "Epoch 09: train_acc=0.9441, val_acc=0.7945\n",
      "Epoch 10: train_acc=0.9092, val_acc=0.8239\n",
      "Epoch 11: train_acc=0.9555, val_acc=0.7725\n",
      "Epoch 12: train_acc=0.7595, val_acc=0.8055\n",
      "Epoch 13: train_acc=0.9207, val_acc=0.8303\n",
      "Epoch 14: train_acc=0.9548, val_acc=0.7954\n",
      "Epoch 15: train_acc=0.9626, val_acc=0.8220\n",
      "Epoch 16: train_acc=0.9812, val_acc=0.8330\n",
      "Epoch 17: train_acc=0.9899, val_acc=0.8422\n",
      "Epoch 18: train_acc=0.9888, val_acc=0.8413\n",
      "Epoch 19: train_acc=0.9835, val_acc=0.8138\n",
      "Epoch 20: train_acc=0.9945, val_acc=0.8248\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8422\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8313\n",
      "Fold 2: 0.8332\n",
      "Fold 3: 0.8211\n",
      "Fold 4: 0.8505\n",
      "Fold 5: 0.8422\n",
      "Average Val Accuracy: 0.8357\n",
      "\n",
      "🧪 K-Fold Testing hidden_dim=512 (batch=32, lr=0.001)\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.5955, val_acc=0.7030\n",
      "Epoch 02: train_acc=0.7402, val_acc=0.7278\n",
      "Epoch 03: train_acc=0.7918, val_acc=0.8020\n",
      "Epoch 04: train_acc=0.7751, val_acc=0.6233\n",
      "Epoch 05: train_acc=0.5496, val_acc=0.7058\n",
      "Epoch 06: train_acc=0.7485, val_acc=0.7232\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8020\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.5822, val_acc=0.6508\n",
      "Epoch 02: train_acc=0.7262, val_acc=0.7085\n",
      "Epoch 03: train_acc=0.5572, val_acc=0.5701\n",
      "Epoch 04: train_acc=0.5132, val_acc=0.6004\n",
      "Epoch 05: train_acc=0.6693, val_acc=0.7360\n",
      "Epoch 06: train_acc=0.7789, val_acc=0.7809\n",
      "Epoch 07: train_acc=0.8214, val_acc=0.7626\n",
      "Epoch 08: train_acc=0.8762, val_acc=0.7864\n",
      "Epoch 09: train_acc=0.9032, val_acc=0.8038\n",
      "Epoch 10: train_acc=0.9213, val_acc=0.7874\n",
      "Epoch 11: train_acc=0.9505, val_acc=0.8176\n",
      "Epoch 12: train_acc=0.9601, val_acc=0.8103\n",
      "Epoch 13: train_acc=0.9686, val_acc=0.8148\n",
      "Epoch 14: train_acc=0.9649, val_acc=0.8038\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8176\n",
      "\n",
      "===== Fold 3/5 =====\n",
      "Epoch 01: train_acc=0.5713, val_acc=0.6954\n",
      "Epoch 02: train_acc=0.6921, val_acc=0.5569\n",
      "Epoch 03: train_acc=0.5177, val_acc=0.7440\n",
      "Epoch 04: train_acc=0.7072, val_acc=0.7147\n",
      "Epoch 05: train_acc=0.7309, val_acc=0.7312\n",
      "Epoch 06: train_acc=0.7070, val_acc=0.7734\n",
      "Epoch 07: train_acc=0.7845, val_acc=0.7817\n",
      "Epoch 08: train_acc=0.8439, val_acc=0.7541\n",
      "Epoch 09: train_acc=0.8182, val_acc=0.8110\n",
      "Epoch 10: train_acc=0.9072, val_acc=0.8339\n",
      "Epoch 11: train_acc=0.9315, val_acc=0.8459\n",
      "Epoch 12: train_acc=0.9459, val_acc=0.8284\n",
      "Epoch 13: train_acc=0.9619, val_acc=0.8422\n",
      "Epoch 14: train_acc=0.9686, val_acc=0.8211\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8459\n",
      "\n",
      "===== Fold 4/5 =====\n",
      "Epoch 01: train_acc=0.6073, val_acc=0.7275\n",
      "Epoch 02: train_acc=0.7538, val_acc=0.7174\n",
      "Epoch 03: train_acc=0.7950, val_acc=0.7688\n",
      "Epoch 04: train_acc=0.7178, val_acc=0.5798\n",
      "Epoch 05: train_acc=0.6382, val_acc=0.7523\n",
      "Epoch 06: train_acc=0.8058, val_acc=0.7294\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.7688\n",
      "\n",
      "===== Fold 5/5 =====\n",
      "Epoch 01: train_acc=0.6077, val_acc=0.6991\n",
      "Epoch 02: train_acc=0.7432, val_acc=0.7596\n",
      "Epoch 03: train_acc=0.7485, val_acc=0.5927\n",
      "Epoch 04: train_acc=0.7281, val_acc=0.6725\n",
      "Epoch 05: train_acc=0.7877, val_acc=0.7835\n",
      "Epoch 06: train_acc=0.8427, val_acc=0.8055\n",
      "Epoch 07: train_acc=0.8462, val_acc=0.8110\n",
      "Epoch 08: train_acc=0.8815, val_acc=0.7174\n",
      "Epoch 09: train_acc=0.8143, val_acc=0.8018\n",
      "Epoch 10: train_acc=0.8572, val_acc=0.8367\n",
      "Epoch 11: train_acc=0.8806, val_acc=0.8303\n",
      "Epoch 12: train_acc=0.9026, val_acc=0.6147\n",
      "Epoch 13: train_acc=0.9214, val_acc=0.8394\n",
      "Epoch 14: train_acc=0.9337, val_acc=0.8147\n",
      "Epoch 15: train_acc=0.9415, val_acc=0.8128\n",
      "Epoch 16: train_acc=0.9594, val_acc=0.8257\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8394\n",
      "\n",
      "===== K-Fold Results =====\n",
      "Fold 1: 0.8020\n",
      "Fold 2: 0.8176\n",
      "Fold 3: 0.8459\n",
      "Fold 4: 0.7688\n",
      "Fold 5: 0.8394\n",
      "Average Val Accuracy: 0.8147\n",
      "\n",
      "🏆 Best hidden_dim configuration (K-Fold):\n",
      "hidden_dim           96\n",
      "cv_mean_acc    0.845375\n",
      "cv_std         0.009649\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Saved var -> best_hidden_dim=96\n",
      "\n",
      "All hidden_dim results:\n",
      "    hidden_dim  cv_mean_acc    cv_std  \\\n",
      "0          96     0.845375  0.009649   \n",
      "1         128     0.845010  0.006921   \n",
      "2          64     0.842075  0.011650   \n",
      "3         192     0.840241  0.007157   \n",
      "4         384     0.835658  0.009985   \n",
      "5         256     0.829236  0.008439   \n",
      "6         512     0.814749  0.027783   \n",
      "\n",
      "                                            per_fold  \n",
      "0  [0.8615948670944088, 0.842346471127406, 0.8504...  \n",
      "1  [0.8560953253895509, 0.8368469294225481, 0.847...  \n",
      "2  [0.8560953253895509, 0.8340971585701191, 0.855...  \n",
      "3  [0.8505957836846929, 0.8350137488542622, 0.840...  \n",
      "4  [0.8313473877176902, 0.8331805682859762, 0.821...  \n",
      "5  [0.844179651695692, 0.8221814848762603, 0.8330...  \n",
      "6  [0.8020164986251146, 0.8175985334555453, 0.845...  \n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# K-Fold tuning for hidden_dim\n",
    "# ===========================\n",
    "\n",
    "hidden_dims = [64, 96, 128, 192, 256, 384, 512]\n",
    "results_hd = []\n",
    "\n",
    "for hd in hidden_dims:\n",
    "    print(f\"\\n🧪 K-Fold Testing hidden_dim={hd} (batch={best_batch_size}, lr={best_lr})\")\n",
    "    mean_acc, fold_accs, fold_models = train_kfold_config(\n",
    "        k_folds=K_FOLDS,\n",
    "        batch_size=best_batch_size,\n",
    "        lr=best_lr,\n",
    "        hidden_dim=hd,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        bidirectional=BIDIRECTIONAL,\n",
    "        dropout=DROPOUT,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        full_dataset=train_data,     # full dataset (same as before)\n",
    "        pad_idx=PAD_IDX,\n",
    "        device=device,\n",
    "        embedding_layer=embedding_layer,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        patience=PATIENCE,\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    results_hd.append({\n",
    "        \"hidden_dim\": hd,\n",
    "        \"cv_mean_acc\": float(mean_acc),\n",
    "        \"cv_std\": float(np.std(fold_accs)),\n",
    "        \"per_fold\": [float(x) for x in fold_accs],\n",
    "    })\n",
    "\n",
    "# Rank by mean CV acc (desc), tie-break by lower std\n",
    "df_hd = (\n",
    "    pd.DataFrame(results_hd)\n",
    "      .sort_values([\"cv_mean_acc\", \"cv_std\"], ascending=[False, True])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "best_hidden_dim = int(df_hd.loc[0, \"hidden_dim\"])\n",
    "\n",
    "print(\"\\n🏆 Best hidden_dim configuration (K-Fold):\")\n",
    "print(df_hd.loc[0, [\"hidden_dim\",\"cv_mean_acc\",\"cv_std\"]])\n",
    "print(f\"\\nSaved var -> best_hidden_dim={best_hidden_dim}\")\n",
    "\n",
    "print(\"\\nAll hidden_dim results:\\n\", df_hd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_batch_size = 32\n",
    "best_lr = 0.001\n",
    "best_hidden_dim = 96\n",
    "K_FOLDS = 5\n",
    "NUM_LAYERS    = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.3\n",
    "MAX_EPOCHS    = 50\n",
    "PATIENCE      = 3\n",
    "SEED          = 42\n",
    "K_FOLDS       = 5\n",
    "NUM_CLASSES   = len(LABEL.vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Regularization (No L2 No GC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_run_noclip(iterator, model, criterion, optimizer=None):\n",
    "    model.train(optimizer is not None)\n",
    "    tot_loss, tot_correct, tot_count = 0.0, 0, 0\n",
    "    for batch in iterator:\n",
    "        text, labels = batch.text, batch.label\n",
    "        logits = model(text)\n",
    "        loss = criterion(logits, labels)\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0) no clipping\n",
    "            optimizer.step()\n",
    "        tot_loss += loss.item() * labels.size(0)\n",
    "        tot_correct += (logits.argmax(1) == labels).sum().item()\n",
    "        tot_count += labels.size(0)\n",
    "    return tot_loss / max(tot_count,1), tot_correct / max(tot_count,1)\n",
    "\n",
    "def train_kfold_config_NoReg(k_folds, batch_size, lr, hidden_dim, *,\n",
    "                          num_layers, bidirectional, dropout, num_classes,\n",
    "                          full_dataset, pad_idx, device, embedding_layer,\n",
    "                          max_epochs=20, patience=3, seed=42):\n",
    "\n",
    "    set_seed(SEED)\n",
    "    all_indices = np.arange(len(full_dataset))\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "    fold_results = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(all_indices), 1):\n",
    "        print(f\"\\n===== Fold {fold}/{k_folds} (No regularization) =====\")\n",
    "\n",
    "        train_data = [full_dataset.examples[i] for i in train_idx]\n",
    "        valid_data = [full_dataset.examples[i] for i in val_idx]\n",
    "        train_data = data.Dataset(train_data, fields=full_dataset.fields)\n",
    "        valid_data = data.Dataset(valid_data, fields=full_dataset.fields)\n",
    "\n",
    "        train_iter, valid_iter = build_iters(batch_size, train_data, valid_data, device)\n",
    "\n",
    "        model = build_model(embedding_layer, pad_idx, hidden_dim, num_layers,\n",
    "                            bidirectional, dropout, num_classes, device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # 👇 L2 Regularization applied here\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        best_val_acc = -1.0\n",
    "        best_state = None\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            train_loss, train_acc = epoch_run_noclip(train_iter, model, criterion, optimizer)\n",
    "            val_loss, val_acc = epoch_run_noclip(valid_iter, model, criterion, optimizer=None)\n",
    "            print(f\"Epoch {epoch:02d}: train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                epochs_no_improve = 0\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "\n",
    "        if best_state:\n",
    "            model.load_state_dict(best_state)\n",
    "\n",
    "        print(f\"Fold {fold} best val_acc={best_val_acc:.4f}\")\n",
    "        fold_results.append(best_val_acc)\n",
    "\n",
    "    mean_acc = np.mean(fold_results)\n",
    "    print(f\"\\nAverage Val Accuracy (No regularization): {mean_acc:.4f}\")\n",
    "    return mean_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1/5 (No regularization) =====\n",
      "Epoch 01: train_acc=0.5737, val_acc=0.7314\n",
      "Epoch 02: train_acc=0.7838, val_acc=0.7874\n",
      "Epoch 03: train_acc=0.8491, val_acc=0.8268\n",
      "Epoch 04: train_acc=0.9012, val_acc=0.8543\n",
      "Epoch 05: train_acc=0.9335, val_acc=0.8524\n",
      "Epoch 06: train_acc=0.9573, val_acc=0.8552\n",
      "Epoch 07: train_acc=0.9743, val_acc=0.8552\n",
      "Epoch 08: train_acc=0.9844, val_acc=0.8488\n",
      "Epoch 09: train_acc=0.9890, val_acc=0.8341\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8552\n",
      "\n",
      "===== Fold 2/5 (No regularization) =====\n",
      "Epoch 01: train_acc=0.5801, val_acc=0.7278\n",
      "Epoch 02: train_acc=0.7794, val_acc=0.7974\n",
      "Epoch 03: train_acc=0.8278, val_acc=0.7580\n",
      "Epoch 04: train_acc=0.8776, val_acc=0.8497\n",
      "Epoch 05: train_acc=0.9236, val_acc=0.8460\n",
      "Epoch 06: train_acc=0.9553, val_acc=0.8194\n",
      "Epoch 07: train_acc=0.9723, val_acc=0.8387\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8497\n",
      "\n",
      "===== Fold 3/5 (No regularization) =====\n",
      "Epoch 01: train_acc=0.5724, val_acc=0.7578\n",
      "Epoch 02: train_acc=0.7811, val_acc=0.7872\n",
      "Epoch 03: train_acc=0.8450, val_acc=0.8385\n",
      "Epoch 04: train_acc=0.9007, val_acc=0.8376\n",
      "Epoch 05: train_acc=0.9301, val_acc=0.8404\n",
      "Epoch 06: train_acc=0.9546, val_acc=0.8404\n",
      "Epoch 07: train_acc=0.9693, val_acc=0.8505\n",
      "Epoch 08: train_acc=0.9856, val_acc=0.8468\n",
      "Epoch 09: train_acc=0.9904, val_acc=0.8523\n",
      "Epoch 10: train_acc=0.9915, val_acc=0.8385\n",
      "Epoch 11: train_acc=0.9984, val_acc=0.8477\n",
      "Epoch 12: train_acc=0.9986, val_acc=0.8486\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8523\n",
      "\n",
      "===== Fold 4/5 (No regularization) =====\n",
      "Epoch 01: train_acc=0.5873, val_acc=0.7110\n",
      "Epoch 02: train_acc=0.7928, val_acc=0.7734\n",
      "Epoch 03: train_acc=0.8576, val_acc=0.7982\n",
      "Epoch 04: train_acc=0.9042, val_acc=0.8211\n",
      "Epoch 05: train_acc=0.9367, val_acc=0.8083\n",
      "Epoch 06: train_acc=0.9594, val_acc=0.8275\n",
      "Epoch 07: train_acc=0.9746, val_acc=0.8174\n",
      "Epoch 08: train_acc=0.9768, val_acc=0.8101\n",
      "Epoch 09: train_acc=0.9872, val_acc=0.8257\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8275\n",
      "\n",
      "===== Fold 5/5 (No regularization) =====\n",
      "Epoch 01: train_acc=0.5908, val_acc=0.7431\n",
      "Epoch 02: train_acc=0.7707, val_acc=0.7917\n",
      "Epoch 03: train_acc=0.8448, val_acc=0.8376\n",
      "Epoch 04: train_acc=0.8973, val_acc=0.8303\n",
      "Epoch 05: train_acc=0.9241, val_acc=0.8431\n",
      "Epoch 06: train_acc=0.9530, val_acc=0.8385\n",
      "Epoch 07: train_acc=0.9668, val_acc=0.8330\n",
      "Epoch 08: train_acc=0.9826, val_acc=0.8229\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8431\n",
      "\n",
      "Average Val Accuracy (No regularization): 0.8456\n",
      "\n",
      "Final K-Fold Mean Accuracy with No regularization: 0.8456\n"
     ]
    }
   ],
   "source": [
    "mean_acc_no = train_kfold_config_NoReg(\n",
    "    k_folds=K_FOLDS,\n",
    "    batch_size=best_batch_size,\n",
    "    lr=best_lr,\n",
    "    hidden_dim=best_hidden_dim,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    dropout=DROPOUT,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    full_dataset=train_data,\n",
    "    pad_idx=PAD_IDX,\n",
    "    device=device,\n",
    "    embedding_layer=embedding_layer,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    seed=SEED,\n",
    ")\n",
    "print(f\"\\nFinal K-Fold Mean Accuracy with No regularization: {mean_acc_no:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 Regularization (Weight Decay = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kfold_config_L2(k_folds, batch_size, lr, hidden_dim, *,\n",
    "                          num_layers, bidirectional, dropout, num_classes,\n",
    "                          full_dataset, pad_idx, device, embedding_layer,\n",
    "                          max_epochs=20, patience=3, seed=42, weight_decay=1e-4):\n",
    "\n",
    "    set_seed(SEED)\n",
    "    all_indices = np.arange(len(full_dataset))\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "    fold_results = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(all_indices), 1):\n",
    "        print(f\"\\n===== Fold {fold}/{k_folds} (L2 regularization) =====\")\n",
    "\n",
    "        train_data = [full_dataset.examples[i] for i in train_idx]\n",
    "        valid_data = [full_dataset.examples[i] for i in val_idx]\n",
    "        train_data = data.Dataset(train_data, fields=full_dataset.fields)\n",
    "        valid_data = data.Dataset(valid_data, fields=full_dataset.fields)\n",
    "\n",
    "        train_iter, valid_iter = build_iters(batch_size, train_data, valid_data, device)\n",
    "\n",
    "        model = build_model(embedding_layer, pad_idx, hidden_dim, num_layers,\n",
    "                            bidirectional, dropout, num_classes, device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # 👇 L2 Regularization applied here\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        best_val_acc = -1.0\n",
    "        best_state = None\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            train_loss, train_acc = epoch_run_noclip(train_iter, model, criterion, optimizer)\n",
    "            val_loss, val_acc = epoch_run_noclip(valid_iter, model, criterion, optimizer=None)\n",
    "            print(f\"Epoch {epoch:02d}: train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                epochs_no_improve = 0\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "\n",
    "        if best_state:\n",
    "            model.load_state_dict(best_state)\n",
    "\n",
    "        print(f\"Fold {fold} best val_acc={best_val_acc:.4f}\")\n",
    "        fold_results.append(best_val_acc)\n",
    "\n",
    "    mean_acc = np.mean(fold_results)\n",
    "    print(f\"\\nAverage Val Accuracy (L2 regularization): {mean_acc:.4f}\")\n",
    "    return mean_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1/5 (L2 regularization) =====\n",
      "Epoch 01: train_acc=0.5714, val_acc=0.7177\n",
      "Epoch 02: train_acc=0.7757, val_acc=0.7984\n",
      "Epoch 03: train_acc=0.8367, val_acc=0.8268\n",
      "Epoch 04: train_acc=0.8892, val_acc=0.8460\n",
      "Epoch 05: train_acc=0.9264, val_acc=0.8607\n",
      "Epoch 06: train_acc=0.9564, val_acc=0.8414\n",
      "Epoch 07: train_acc=0.9729, val_acc=0.8579\n",
      "Epoch 08: train_acc=0.9833, val_acc=0.8552\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8607\n",
      "\n",
      "===== Fold 2/5 (L2 regularization) =====\n",
      "Epoch 01: train_acc=0.5735, val_acc=0.7067\n",
      "Epoch 02: train_acc=0.7670, val_acc=0.7764\n",
      "Epoch 03: train_acc=0.8347, val_acc=0.8167\n",
      "Epoch 04: train_acc=0.8805, val_acc=0.8378\n",
      "Epoch 05: train_acc=0.9234, val_acc=0.8460\n",
      "Epoch 06: train_acc=0.9539, val_acc=0.8139\n",
      "Epoch 07: train_acc=0.9723, val_acc=0.8112\n",
      "Epoch 08: train_acc=0.9766, val_acc=0.8268\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8460\n",
      "\n",
      "===== Fold 3/5 (L2 regularization) =====\n",
      "Epoch 01: train_acc=0.5697, val_acc=0.7633\n",
      "Epoch 02: train_acc=0.7737, val_acc=0.8046\n",
      "Epoch 03: train_acc=0.8464, val_acc=0.8266\n",
      "Epoch 04: train_acc=0.9007, val_acc=0.8431\n",
      "Epoch 05: train_acc=0.9376, val_acc=0.8422\n",
      "Epoch 06: train_acc=0.9567, val_acc=0.8486\n",
      "Epoch 07: train_acc=0.9613, val_acc=0.8523\n",
      "Epoch 08: train_acc=0.9759, val_acc=0.8367\n",
      "Epoch 09: train_acc=0.9888, val_acc=0.8477\n",
      "Epoch 10: train_acc=0.9922, val_acc=0.8321\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8523\n",
      "\n",
      "===== Fold 4/5 (L2 regularization) =====\n",
      "Epoch 01: train_acc=0.5848, val_acc=0.7147\n",
      "Epoch 02: train_acc=0.7856, val_acc=0.7651\n",
      "Epoch 03: train_acc=0.8530, val_acc=0.8009\n",
      "Epoch 04: train_acc=0.8893, val_acc=0.8147\n",
      "Epoch 05: train_acc=0.9280, val_acc=0.8220\n",
      "Epoch 06: train_acc=0.9551, val_acc=0.8303\n",
      "Epoch 07: train_acc=0.9700, val_acc=0.8440\n",
      "Epoch 08: train_acc=0.9828, val_acc=0.8312\n",
      "Epoch 09: train_acc=0.9849, val_acc=0.8339\n",
      "Epoch 10: train_acc=0.9851, val_acc=0.8339\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8440\n",
      "\n",
      "===== Fold 5/5 (L2 regularization) =====\n",
      "Epoch 01: train_acc=0.5915, val_acc=0.7312\n",
      "Epoch 02: train_acc=0.7650, val_acc=0.7817\n",
      "Epoch 03: train_acc=0.8313, val_acc=0.8064\n",
      "Epoch 04: train_acc=0.8902, val_acc=0.8147\n",
      "Epoch 05: train_acc=0.9264, val_acc=0.8358\n",
      "Epoch 06: train_acc=0.9521, val_acc=0.8229\n",
      "Epoch 07: train_acc=0.9709, val_acc=0.8339\n",
      "Epoch 08: train_acc=0.9833, val_acc=0.8367\n",
      "Epoch 09: train_acc=0.9927, val_acc=0.8202\n",
      "Epoch 10: train_acc=0.9823, val_acc=0.7908\n",
      "Epoch 11: train_acc=0.9597, val_acc=0.8422\n",
      "Epoch 12: train_acc=0.9966, val_acc=0.8339\n",
      "Epoch 13: train_acc=0.9972, val_acc=0.8229\n",
      "Epoch 14: train_acc=0.9991, val_acc=0.8321\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8422\n",
      "\n",
      "Average Val Accuracy (L2 regularization): 0.8490\n",
      "\n",
      "Final K-Fold Mean Accuracy with L2 regularization: 0.8490\n"
     ]
    }
   ],
   "source": [
    "mean_acc_L2 = train_kfold_config_L2(\n",
    "    k_folds=K_FOLDS,\n",
    "    batch_size=best_batch_size,\n",
    "    lr=best_lr,\n",
    "    hidden_dim=best_hidden_dim,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    dropout=DROPOUT,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    full_dataset=train_data,\n",
    "    pad_idx=PAD_IDX,\n",
    "    device=device,\n",
    "    embedding_layer=embedding_layer,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    seed=SEED,\n",
    "    weight_decay=1e-4   # L2 penalty\n",
    ")\n",
    "print(f\"\\nFinal K-Fold Mean Accuracy with L2 regularization: {mean_acc_L2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Clipping (clip_value=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kfold_config_gc(k_folds, batch_size, lr, hidden_dim, *,\n",
    "                          num_layers, bidirectional, dropout, num_classes,\n",
    "                          full_dataset, pad_idx, device, embedding_layer,\n",
    "                          max_epochs=20, patience=3, seed=42):\n",
    "    \n",
    "    set_seed(SEED)\n",
    "    all_indices = np.arange(len(full_dataset))\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "    fold_results = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(all_indices), 1):\n",
    "        print(f\"\\n===== Fold {fold}/{k_folds} (GC regularization) =====\")\n",
    "\n",
    "        train_data = [full_dataset.examples[i] for i in train_idx]\n",
    "        valid_data = [full_dataset.examples[i] for i in val_idx]\n",
    "        train_data = data.Dataset(train_data, fields=full_dataset.fields)\n",
    "        valid_data = data.Dataset(valid_data, fields=full_dataset.fields)\n",
    "\n",
    "        train_iter, valid_iter = build_iters(batch_size, train_data, valid_data, device)\n",
    "\n",
    "        model = build_model(embedding_layer, pad_idx, hidden_dim, num_layers,\n",
    "                            bidirectional, dropout, num_classes, device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # 👇 L2 Regularization applied here\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        best_val_acc = -1.0\n",
    "        best_state = None\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            train_loss, train_acc = epoch_run(train_iter, model, criterion, optimizer)\n",
    "            val_loss, val_acc = epoch_run(valid_iter, model, criterion, optimizer=None)\n",
    "            print(f\"Epoch {epoch:02d}: train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                epochs_no_improve = 0\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "\n",
    "        if best_state:\n",
    "            model.load_state_dict(best_state)\n",
    "\n",
    "        print(f\"Fold {fold} best val_acc={best_val_acc:.4f}\")\n",
    "        fold_results.append(best_val_acc)\n",
    "\n",
    "    mean_acc = np.mean(fold_results)\n",
    "    print(f\"\\nAverage Val Accuracy (GC regularization): {mean_acc:.4f}\")\n",
    "    return mean_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1/5 (GC regularization) =====\n",
      "Epoch 01: train_acc=0.5737, val_acc=0.7314\n",
      "Epoch 02: train_acc=0.7838, val_acc=0.7874\n",
      "Epoch 03: train_acc=0.8500, val_acc=0.8240\n",
      "Epoch 04: train_acc=0.9009, val_acc=0.8524\n",
      "Epoch 05: train_acc=0.9335, val_acc=0.8524\n",
      "Epoch 06: train_acc=0.9569, val_acc=0.8497\n",
      "Epoch 07: train_acc=0.9736, val_acc=0.8561\n",
      "Epoch 08: train_acc=0.9842, val_acc=0.8488\n",
      "Epoch 09: train_acc=0.9913, val_acc=0.8488\n",
      "Epoch 10: train_acc=0.9954, val_acc=0.8469\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8561\n",
      "\n",
      "===== Fold 2/5 (GC regularization) =====\n",
      "Epoch 01: train_acc=0.5753, val_acc=0.7269\n",
      "Epoch 02: train_acc=0.7762, val_acc=0.7901\n",
      "Epoch 03: train_acc=0.8354, val_acc=0.8286\n",
      "Epoch 04: train_acc=0.8881, val_acc=0.8478\n",
      "Epoch 05: train_acc=0.9298, val_acc=0.8368\n",
      "Epoch 06: train_acc=0.9564, val_acc=0.8121\n",
      "Epoch 07: train_acc=0.9684, val_acc=0.8093\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8478\n",
      "\n",
      "===== Fold 3/5 (GC regularization) =====\n",
      "Epoch 01: train_acc=0.5587, val_acc=0.7431\n",
      "Epoch 02: train_acc=0.7772, val_acc=0.7872\n",
      "Epoch 03: train_acc=0.8475, val_acc=0.8312\n",
      "Epoch 04: train_acc=0.9049, val_acc=0.8495\n",
      "Epoch 05: train_acc=0.9390, val_acc=0.8431\n",
      "Epoch 06: train_acc=0.9555, val_acc=0.8514\n",
      "Epoch 07: train_acc=0.9709, val_acc=0.8495\n",
      "Epoch 08: train_acc=0.9828, val_acc=0.8532\n",
      "Epoch 09: train_acc=0.9899, val_acc=0.8569\n",
      "Epoch 10: train_acc=0.9954, val_acc=0.8431\n",
      "Epoch 11: train_acc=0.9906, val_acc=0.8459\n",
      "Epoch 12: train_acc=0.9959, val_acc=0.8385\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8569\n",
      "\n",
      "===== Fold 4/5 (GC regularization) =====\n",
      "Epoch 01: train_acc=0.5876, val_acc=0.6982\n",
      "Epoch 02: train_acc=0.7893, val_acc=0.7752\n",
      "Epoch 03: train_acc=0.8567, val_acc=0.7899\n",
      "Epoch 04: train_acc=0.9012, val_acc=0.8174\n",
      "Epoch 05: train_acc=0.9335, val_acc=0.8055\n",
      "Epoch 06: train_acc=0.9592, val_acc=0.8321\n",
      "Epoch 07: train_acc=0.9734, val_acc=0.8321\n",
      "Epoch 08: train_acc=0.9807, val_acc=0.8339\n",
      "Epoch 09: train_acc=0.9922, val_acc=0.8220\n",
      "Epoch 10: train_acc=0.9947, val_acc=0.8294\n",
      "Epoch 11: train_acc=0.9984, val_acc=0.8248\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8339\n",
      "\n",
      "===== Fold 5/5 (GC regularization) =====\n",
      "Epoch 01: train_acc=0.5974, val_acc=0.7404\n",
      "Epoch 02: train_acc=0.7701, val_acc=0.7835\n",
      "Epoch 03: train_acc=0.8459, val_acc=0.8358\n",
      "Epoch 04: train_acc=0.8941, val_acc=0.8174\n",
      "Epoch 05: train_acc=0.9211, val_acc=0.8404\n",
      "Epoch 06: train_acc=0.9530, val_acc=0.8239\n",
      "Epoch 07: train_acc=0.9718, val_acc=0.8367\n",
      "Epoch 08: train_acc=0.9810, val_acc=0.8257\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8404\n",
      "\n",
      "Average Val Accuracy (GC regularization): 0.8470\n",
      "\n",
      "Final K-Fold Mean Accuracy with Gradient Clipping: 0.8470\n"
     ]
    }
   ],
   "source": [
    "mean_acc_gc = train_kfold_config_gc(\n",
    "    k_folds=K_FOLDS,\n",
    "    batch_size=best_batch_size,\n",
    "    lr=best_lr,\n",
    "    hidden_dim=best_hidden_dim,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    dropout=DROPOUT,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    full_dataset=train_data,\n",
    "    pad_idx=PAD_IDX,\n",
    "    device=device,\n",
    "    embedding_layer=embedding_layer,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    seed=SEED,\n",
    ")\n",
    "print(f\"\\nFinal K-Fold Mean Accuracy with Gradient Clipping: {mean_acc_gc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 Regularization (Weight Decay = 1e-4) + Gradient Clipping (Clip value = 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kfold_config_L2_gc(k_folds, batch_size, lr, hidden_dim, *,\n",
    "                          num_layers, bidirectional, dropout, num_classes,\n",
    "                          full_dataset, pad_idx, device, embedding_layer,\n",
    "                          max_epochs=20, patience=3, seed=42, weight_decay=1e-4):\n",
    "\n",
    "    set_seed(SEED)\n",
    "    all_indices = np.arange(len(full_dataset))\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "    fold_results = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(all_indices), 1):\n",
    "        print(f\"\\n===== Fold {fold}/{k_folds} (L2+GC regularization) =====\")\n",
    "\n",
    "        train_data = [full_dataset.examples[i] for i in train_idx]\n",
    "        valid_data = [full_dataset.examples[i] for i in val_idx]\n",
    "        train_data = data.Dataset(train_data, fields=full_dataset.fields)\n",
    "        valid_data = data.Dataset(valid_data, fields=full_dataset.fields)\n",
    "\n",
    "        train_iter, valid_iter = build_iters(batch_size, train_data, valid_data, device)\n",
    "\n",
    "        model = build_model(embedding_layer, pad_idx, hidden_dim, num_layers,\n",
    "                            bidirectional, dropout, num_classes, device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # 👇 L2 Regularization applied here\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        best_val_acc = -1.0\n",
    "        best_state = None\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            train_loss, train_acc = epoch_run(train_iter, model, criterion, optimizer)\n",
    "            val_loss, val_acc = epoch_run(valid_iter, model, criterion, optimizer=None)\n",
    "            print(f\"Epoch {epoch:02d}: train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                epochs_no_improve = 0\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "\n",
    "        if best_state:\n",
    "            model.load_state_dict(best_state)\n",
    "\n",
    "        print(f\"Fold {fold} best val_acc={best_val_acc:.4f}\")\n",
    "        fold_results.append(best_val_acc)\n",
    "\n",
    "    mean_acc = np.mean(fold_results)\n",
    "    print(f\"\\nAverage Val Accuracy (L2+GC regularization): {mean_acc:.4f}\")\n",
    "    return mean_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1/5 (L2+GC regularization) =====\n",
      "Epoch 01: train_acc=0.5714, val_acc=0.7177\n",
      "Epoch 02: train_acc=0.7757, val_acc=0.7984\n",
      "Epoch 03: train_acc=0.8358, val_acc=0.8258\n",
      "Epoch 04: train_acc=0.8904, val_acc=0.8515\n",
      "Epoch 05: train_acc=0.9246, val_acc=0.8570\n",
      "Epoch 06: train_acc=0.9564, val_acc=0.8515\n",
      "Epoch 07: train_acc=0.9702, val_acc=0.8405\n",
      "Epoch 08: train_acc=0.9810, val_acc=0.8488\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8570\n",
      "\n",
      "===== Fold 2/5 (L2+GC regularization) =====\n",
      "Epoch 01: train_acc=0.5735, val_acc=0.7067\n",
      "Epoch 02: train_acc=0.7670, val_acc=0.7764\n",
      "Epoch 03: train_acc=0.8354, val_acc=0.8167\n",
      "Epoch 04: train_acc=0.8812, val_acc=0.8359\n",
      "Epoch 05: train_acc=0.9234, val_acc=0.8313\n",
      "Epoch 06: train_acc=0.9530, val_acc=0.8148\n",
      "Epoch 07: train_acc=0.9695, val_acc=0.8323\n",
      "Early stopping\n",
      "Fold 2 best val_acc=0.8359\n",
      "\n",
      "===== Fold 3/5 (L2+GC regularization) =====\n",
      "Epoch 01: train_acc=0.5669, val_acc=0.7596\n",
      "Epoch 02: train_acc=0.7753, val_acc=0.7853\n",
      "Epoch 03: train_acc=0.8466, val_acc=0.8303\n",
      "Epoch 04: train_acc=0.8959, val_acc=0.8514\n",
      "Epoch 05: train_acc=0.9397, val_acc=0.8523\n",
      "Epoch 06: train_acc=0.9601, val_acc=0.8339\n",
      "Epoch 07: train_acc=0.9704, val_acc=0.8495\n",
      "Epoch 08: train_acc=0.9828, val_acc=0.8532\n",
      "Epoch 09: train_acc=0.9901, val_acc=0.8275\n",
      "Epoch 10: train_acc=0.9920, val_acc=0.7853\n",
      "Epoch 11: train_acc=0.9812, val_acc=0.8450\n",
      "Early stopping\n",
      "Fold 3 best val_acc=0.8532\n",
      "\n",
      "===== Fold 4/5 (L2+GC regularization) =====\n",
      "Epoch 01: train_acc=0.5848, val_acc=0.7147\n",
      "Epoch 02: train_acc=0.7856, val_acc=0.7651\n",
      "Epoch 03: train_acc=0.8533, val_acc=0.8009\n",
      "Epoch 04: train_acc=0.8964, val_acc=0.8174\n",
      "Epoch 05: train_acc=0.9282, val_acc=0.7972\n",
      "Epoch 06: train_acc=0.9558, val_acc=0.8358\n",
      "Epoch 07: train_acc=0.9723, val_acc=0.8312\n",
      "Epoch 08: train_acc=0.9771, val_acc=0.8349\n",
      "Epoch 09: train_acc=0.9844, val_acc=0.8422\n",
      "Epoch 10: train_acc=0.9867, val_acc=0.8358\n",
      "Epoch 11: train_acc=0.9869, val_acc=0.8385\n",
      "Epoch 12: train_acc=0.9963, val_acc=0.8468\n",
      "Epoch 13: train_acc=0.9970, val_acc=0.8367\n",
      "Epoch 14: train_acc=0.9995, val_acc=0.8367\n",
      "Epoch 15: train_acc=0.9970, val_acc=0.8440\n",
      "Early stopping\n",
      "Fold 4 best val_acc=0.8468\n",
      "\n",
      "===== Fold 5/5 (L2+GC regularization) =====\n",
      "Epoch 01: train_acc=0.5853, val_acc=0.7303\n",
      "Epoch 02: train_acc=0.7675, val_acc=0.8018\n",
      "Epoch 03: train_acc=0.8402, val_acc=0.8257\n",
      "Epoch 04: train_acc=0.8941, val_acc=0.8275\n",
      "Epoch 05: train_acc=0.9184, val_acc=0.8367\n",
      "Epoch 06: train_acc=0.9555, val_acc=0.8339\n",
      "Epoch 07: train_acc=0.9748, val_acc=0.8394\n",
      "Epoch 08: train_acc=0.9862, val_acc=0.8376\n",
      "Epoch 09: train_acc=0.9858, val_acc=0.8330\n",
      "Epoch 10: train_acc=0.9860, val_acc=0.8174\n",
      "Early stopping\n",
      "Fold 5 best val_acc=0.8394\n",
      "\n",
      "Average Val Accuracy (L2+GC regularization): 0.8465\n",
      "\n",
      "Final K-Fold Mean Accuracy with L2 regularization+Gradient Clipping: 0.8465\n"
     ]
    }
   ],
   "source": [
    "mean_acc_L2_gc = train_kfold_config_L2_gc(\n",
    "    k_folds=K_FOLDS,\n",
    "    batch_size=best_batch_size,\n",
    "    lr=best_lr,\n",
    "    hidden_dim=best_hidden_dim,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    dropout=DROPOUT,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    full_dataset=train_data,\n",
    "    pad_idx=PAD_IDX,\n",
    "    device=device,\n",
    "    embedding_layer=embedding_layer,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    seed=SEED,\n",
    "    weight_decay=1e-4   # L2 penalty\n",
    ")\n",
    "print(f\"\\nFinal K-Fold Mean Accuracy with L2 regularization+Gradient Clipping: {mean_acc_L2_gc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highest Val Accuracy from only using Weight Decay @ 0.8490"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is for Train on whole training data and Test on test data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kfold_config_L2_or_gc(k_folds, batch_size, lr, hidden_dim, *,\n",
    "                             num_layers, bidirectional, dropout, num_classes,\n",
    "                             full_dataset, pad_idx, device, embedding_layer,\n",
    "                             test_dataset,                          # ⬅️ NEW: pass test_data here\n",
    "                             max_epochs=20, patience=3, seed=42,\n",
    "                             weight_decay, clip_value,     # ⬅️ L2 + GC knobs\n",
    "                             retrain_val_ratio=0.1):                # ⬅️ small internal val for early stop\n",
    "    \"\"\"\n",
    "    K-Fold CV on training set (no test leakage) using L2 + gradient clipping.\n",
    "    After CV, retrain on full train set (with small internal val split for early stopping),\n",
    "    then evaluate once on the official test set. Returns (mean_cv_acc, test_acc).\n",
    "    \"\"\"\n",
    "    # -------- helpers --------\n",
    "    def _set_seed(s):\n",
    "        random.seed(s); np.random.seed(s)\n",
    "        torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _evaluate(iterator, model, criterion):\n",
    "        model.eval()\n",
    "        tot_loss = tot_correct = tot_count = 0\n",
    "        for batch in iterator:\n",
    "            text, labels = batch.text, batch.label\n",
    "            logits = model(text)\n",
    "            loss = criterion(logits, labels)\n",
    "            tot_loss   += loss.item() * labels.size(0)\n",
    "            tot_correct += (logits.argmax(1) == labels).sum().item()\n",
    "            tot_count  += labels.size(0)\n",
    "        return tot_loss / max(tot_count,1), tot_correct / max(tot_count,1)\n",
    "\n",
    "    def _train_epoch(it, model, criterion, optimizer, clip_value):\n",
    "        model.train()\n",
    "        tot_loss = tot_correct = tot_count = 0\n",
    "        for batch in it:\n",
    "            text, labels = batch.text, batch.label\n",
    "            logits = model(text)\n",
    "            loss = criterion(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if clip_value is not None:  # ✅ gradient clipping here\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "            tot_loss   += loss.item() * labels.size(0)\n",
    "            tot_correct += (logits.argmax(1) == labels).sum().item()\n",
    "            tot_count  += labels.size(0)\n",
    "        return tot_loss / max(tot_count,1), tot_correct / max(tot_count,1)\n",
    "\n",
    "    _set_seed(seed)\n",
    "\n",
    "    # -------- Final retrain on FULL training set (no test leakage) --------\n",
    "    # small internal val split for early stopping\n",
    "    _set_seed(seed)\n",
    "    n = len(full_dataset.examples)\n",
    "    idx = np.arange(n)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(idx)\n",
    "    cut = int((1.0 - retrain_val_ratio) * n)\n",
    "    tr_idx, va_idx = idx[:cut], idx[cut:]\n",
    "    \n",
    "    full_tr_ds = data.Dataset([full_dataset.examples[i] for i in tr_idx], fields=full_dataset.fields)\n",
    "    full_va_ds = data.Dataset([full_dataset.examples[i] for i in va_idx], fields=full_dataset.fields)\n",
    "\n",
    "    full_tr_iter, full_va_iter = build_iters(batch_size, full_tr_ds, full_va_ds, device)\n",
    "\n",
    "    final_model = build_model(embedding_layer, pad_idx, hidden_dim, num_layers,\n",
    "                              bidirectional, dropout, num_classes, device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if weight_decay is None:\n",
    "        optimizer = torch.optim.Adam(final_model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(final_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_state, best_val_acc, no_improve = None, -1.0, 0\n",
    "    fold_results = []\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        tr_loss, tr_acc = _train_epoch(full_tr_iter, final_model, criterion, optimizer, clip_value)\n",
    "        va_loss, va_acc = _evaluate(full_va_iter, final_model, criterion)\n",
    "        print(f\"[Final Retrain] Epoch {epoch:02d}: train_acc={tr_acc:.4f}, val_acc={va_acc:.4f}\")\n",
    "        fold_results.append(va_acc)\n",
    "\n",
    "        if va_acc > best_val_acc:\n",
    "            best_val_acc, no_improve = va_acc, 0\n",
    "            best_state = copy.deepcopy(final_model.state_dict())\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(\"[Final Retrain] Early stopping.\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        final_model.load_state_dict(best_state)\n",
    "\n",
    "    mean_acc = float(np.mean(fold_results))              # ✅ define the var you return\n",
    "    print(f\"\\nAverage Val Accuracy (L2+GC regularization): {mean_acc:.4f}\")\n",
    "\n",
    "    # -------- Evaluate once on TEST set (no training here) --------\n",
    "    test_iter = data.BucketIterator(\n",
    "        test_dataset, batch_size=batch_size,\n",
    "        sort_key=lambda x: len(x.text), sort_within_batch=True,\n",
    "        device=device, train=False, shuffle=False\n",
    "    )\n",
    "    test_loss, test_acc = _evaluate(test_iter, final_model, criterion)\n",
    "    print(f\"\\n[L2+GC] TEST accuracy = {test_acc:.4f}\")\n",
    "\n",
    "    return mean_acc, float(test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc_no, test_acc_no = train_kfold_config_L2_or_gc(\n",
    "    k_folds=K_FOLDS,\n",
    "    batch_size=best_batch_size,\n",
    "    lr=best_lr,\n",
    "    hidden_dim=best_hidden_dim,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    dropout=DROPOUT,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    full_dataset=train_data,    # training set only\n",
    "    test_dataset=test_data,     # <-- add this\n",
    "    pad_idx=PAD_IDX,\n",
    "    device=device,\n",
    "    embedding_layer=embedding_layer,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    seed=SEED,\n",
    "    weight_decay=None,          # L2\n",
    "    clip_value=None              # GC\n",
    ")\n",
    "print(f\"\\nTrain mean val acc={mean_acc_no:.4f} | TEST acc={test_acc_no:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 Regularization (Weight Decay = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc_L2, test_acc_L2 = train_kfold_config_L2_or_gc(\n",
    "    k_folds=K_FOLDS,\n",
    "    batch_size=best_batch_size,\n",
    "    lr=best_lr,\n",
    "    hidden_dim=best_hidden_dim,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    dropout=DROPOUT,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    full_dataset=train_data,    # training set only\n",
    "    test_dataset=test_data,     # <-- add this\n",
    "    pad_idx=PAD_IDX,\n",
    "    device=device,\n",
    "    embedding_layer=embedding_layer,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    seed=SEED,\n",
    "    weight_decay=1e-4,          # L2\n",
    "    clip_value=None              # GC\n",
    ")\n",
    "print(f\"\\nTrain mean val acc={mean_acc_L2:.4f} | TEST acc={test_acc_L2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Clipping (Clip value = 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc_gc, test_acc_gc = train_kfold_config_L2_or_gc(\n",
    "    k_folds=K_FOLDS,\n",
    "    batch_size=best_batch_size,\n",
    "    lr=best_lr,\n",
    "    hidden_dim=best_hidden_dim,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    dropout=DROPOUT,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    full_dataset=train_data,    # training set only\n",
    "    test_dataset=test_data,     # <-- add this\n",
    "    pad_idx=PAD_IDX,\n",
    "    device=device,\n",
    "    embedding_layer=embedding_layer,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    seed=SEED,\n",
    "    weight_decay=None,          # L2\n",
    "    clip_value=5.0              # GC\n",
    ")\n",
    "print(f\"\\nCTrain mean val acc={mean_acc_gc:.4f} | TEST acc={test_acc_gc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 Regularization (Weight Decay = 1e-4) + Gradient Clipping (Clip value = 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final Retrain] Epoch 01: train_acc=0.5903, val_acc=0.7509\n",
      "[Final Retrain] Epoch 02: train_acc=0.7797, val_acc=0.7949\n",
      "[Final Retrain] Epoch 03: train_acc=0.8532, val_acc=0.8260\n",
      "[Final Retrain] Epoch 04: train_acc=0.9056, val_acc=0.8443\n",
      "[Final Retrain] Epoch 05: train_acc=0.9429, val_acc=0.8462\n",
      "[Final Retrain] Epoch 06: train_acc=0.9619, val_acc=0.8516\n",
      "[Final Retrain] Epoch 07: train_acc=0.9749, val_acc=0.8462\n",
      "[Final Retrain] Epoch 08: train_acc=0.9857, val_acc=0.8553\n",
      "[Final Retrain] Epoch 09: train_acc=0.9912, val_acc=0.8297\n",
      "[Final Retrain] Epoch 10: train_acc=0.9855, val_acc=0.8516\n",
      "[Final Retrain] Epoch 11: train_acc=0.9941, val_acc=0.8626\n",
      "[Final Retrain] Epoch 12: train_acc=0.9967, val_acc=0.8388\n",
      "[Final Retrain] Epoch 13: train_acc=0.9986, val_acc=0.8590\n",
      "[Final Retrain] Epoch 14: train_acc=0.9990, val_acc=0.8516\n",
      "[Final Retrain] Early stopping.\n",
      "\n",
      "Average Val Accuracy (L2+GC regularization): 0.8363\n",
      "\n",
      "[L2+GC] TEST accuracy = 0.8840\n",
      "\n",
      "CV mean=0.8363 | TEST acc=0.8840\n"
     ]
    }
   ],
   "source": [
    "mean_acc_L2_gc, test_acc_L2_gc = train_kfold_config_L2_or_gc(\n",
    "    k_folds=K_FOLDS,\n",
    "    batch_size=best_batch_size,\n",
    "    lr=best_lr,\n",
    "    hidden_dim=best_hidden_dim,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    dropout=DROPOUT,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    full_dataset=train_data,    # training set only\n",
    "    test_dataset=test_data,     # <-- add this\n",
    "    pad_idx=PAD_IDX,\n",
    "    device=device,\n",
    "    embedding_layer=embedding_layer,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    seed=SEED,\n",
    "    weight_decay=1e-4,          # L2\n",
    "    clip_value=5.0              # GC\n",
    ")\n",
    "print(f\"\\nTrain mean val acc={mean_acc_L2_gc:.4f} | TEST acc={test_acc_L2_gc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = concat(test_acc_no, test_acc_L2, test_acc_gc, test_acc_L2_gc)\n",
    "sorted_acc = sort(acc, descending=True)\n",
    "print(\"\\n🏆 Final Test Accuracies (sorted):\")\n",
    "for i, a in enumerate(sorted_acc):\n",
    "    print(f\"Rank {i+1}: {a:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc4002-nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
