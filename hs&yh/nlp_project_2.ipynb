{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading train_5500.label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\trec\\train_5500.label: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 336k/336k [00:01<00:00, 276kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading TREC_10.label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\trec\\TREC_10.label: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23.4k/23.4k [00:00<00:00, 107kB/s] \n"
     ]
    }
   ],
   "source": [
    "# For tokenization\n",
    "TEXT = data.Field(tokenize='spacy',\n",
    "    tokenizer_language='en_core_web_sm',\n",
    "    batch_first=True,\n",
    "    fix_length=50,\n",
    "    lower=True,\n",
    "    unk_token='<unk>',\n",
    "    pad_token='<pad>'\n",
    "    )\n",
    "\n",
    "# For multi-class classification labels\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "# Load the TREC dataset\n",
    "train_data, test_data = datasets.TREC.splits(TEXT, LABEL, fine_grained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ABBR', 'LOC', 'ENTY', 'NUM', 'DESC', 'HUM'}\n"
     ]
    }
   ],
   "source": [
    "label_set = set()\n",
    "for i in train_data:\n",
    "    label_set.add(i.label)\n",
    "\n",
    "print(label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.6B.zip: 862MB [06:11, 2.32MB/s]                               \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 399999/400000 [00:10<00:00, 37352.49it/s]\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "vocab = TEXT.vocab\n",
    "vectors = vocab.vectors.clone()\n",
    "glove_vocab = set(vocab.stoi.keys())\n",
    "\n",
    "# Subword parameters\n",
    "ngram_min = 3\n",
    "ngram_max = 6\n",
    "\n",
    "# Count word frequency in training data\n",
    "word_counter = Counter()\n",
    "for example in train_data:\n",
    "    word_counter.update([w.lower().strip(string.punctuation) for w in example.text])\n",
    "\n",
    "# Threshold to consider a word â€œfrequentâ€ (adjustable)\n",
    "freq_threshold = 3\n",
    "\n",
    "# <unk> vector\n",
    "unk_vector = torch.zeros(embedding_dim)\n",
    "\n",
    "def get_subwords(word, n_min=3, n_max=6):\n",
    "    word = f\"<{word.lower()}>\"\n",
    "    subwords = []\n",
    "    for n in range(n_min, n_max+1):\n",
    "        subwords += [word[i:i+n] for i in range(len(word)-n+1)]\n",
    "    return subwords\n",
    "\n",
    "def get_word_vector(word):\n",
    "    w_clean = word.lower().strip(string.punctuation)\n",
    "    \n",
    "    if w_clean in glove_vocab:\n",
    "        return vectors[vocab.stoi[w_clean]]\n",
    "    \n",
    "    # Subword averaging\n",
    "    subwords = get_subwords(w_clean, ngram_min, ngram_max)\n",
    "    subword_vecs = [vectors[vocab.stoi[sg]] for sg in subwords if sg in glove_vocab]\n",
    "    if subword_vecs:\n",
    "        return torch.stack(subword_vecs).mean(0)\n",
    "    \n",
    "    # Random vector for frequent OOVs\n",
    "    if word_counter[w_clean] >= freq_threshold:\n",
    "        return torch.randn(embedding_dim)\n",
    "    \n",
    "    # <unk> for rare OOVs\n",
    "    return unk_vector\n",
    "\n",
    "# Build embedding matrix\n",
    "embedding_matrix = {}\n",
    "for example in train_data:\n",
    "    for w in example.text:\n",
    "        if w not in embedding_matrix:\n",
    "            embedding_matrix[w] = get_word_vector(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer created with shape: torch.Size([8536, 100])\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab = TEXT.vocab\n",
    "\n",
    "# Create tensor for nn.Embedding\n",
    "embedding_matrix_tensor = torch.zeros(len(vocab), embedding_dim)\n",
    "for word, idx in vocab.stoi.items():\n",
    "    if word in embedding_matrix:\n",
    "        embedding_matrix_tensor[idx] = embedding_matrix[word]\n",
    "\n",
    "# Create embedding layer (learnable)\n",
    "embedding_layer = nn.Embedding.from_pretrained(embedding_matrix_tensor, freeze=False)\n",
    "print(\"Embedding layer created with shape:\", embedding_layer.weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, copy, pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4416, Valid: 491, Test: 500\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1. Setup & Config\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume TEXT, LABEL, train_data, test_data, embedding_layer already exist\n",
    "train_data, valid_data = train_data.split(split_ratio=0.9, random_state=random.seed(SEED))\n",
    "print(f\"Train: {len(train_data)}, Valid: {len(valid_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "embedding_dim = embedding_layer.weight.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Simple RNN classifier (tanh)\n",
    "# -----------------------------\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim, num_layers, bidirectional, dropout, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        emb_dim = self.embedding.weight.size(1)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            nonlinearity=\"tanh\",      # simple RNN\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), num_classes)\n",
    "\n",
    "    def forward(self, text):\n",
    "        x = self.embedding(text)                     # [B, T, E]\n",
    "        outputs, hidden = self.rnn(x)                # hidden: [L*D, B, H]\n",
    "        if self.rnn.bidirectional:\n",
    "            last = torch.cat((hidden[-2], hidden[-1]), dim=1)  # [B, 2H]\n",
    "        else:\n",
    "            last = hidden[-1]                                   # [B, H]\n",
    "        return self.fc(self.dropout(last))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Train/eval 1 epoch\n",
    "# -----------------------------\n",
    "def epoch_run(iterator, model, criterion, optimizer=None):\n",
    "    model.train(optimizer is not None)\n",
    "    tot_loss, tot_correct, tot_count = 0.0, 0, 0\n",
    "    for batch in iterator:\n",
    "        text, labels = batch.text, batch.label\n",
    "        logits = model(text)\n",
    "        loss = criterion(logits, labels)\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)  # RNNs can explode\n",
    "            optimizer.step()\n",
    "        tot_loss += loss.item() * labels.size(0)\n",
    "        tot_correct += (logits.argmax(1) == labels).sum().item()\n",
    "        tot_count += labels.size(0)\n",
    "    return tot_loss / max(tot_count,1), tot_correct / max(tot_count,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Testing BATCH=32, LR=0.0005\n",
      "Epoch 01: train_acc=0.6003, val_acc=0.7943\n",
      "Epoch 02: train_acc=0.8252, val_acc=0.8411\n",
      "Epoch 03: train_acc=0.8705, val_acc=0.8839\n",
      "Epoch 04: train_acc=0.9017, val_acc=0.9063\n",
      "Epoch 05: train_acc=0.9237, val_acc=0.8900\n",
      "Epoch 06: train_acc=0.9386, val_acc=0.9022\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=32, LR=0.001\n",
      "Epoch 01: train_acc=0.6771, val_acc=0.8534\n",
      "Epoch 02: train_acc=0.8490, val_acc=0.8778\n",
      "Epoch 03: train_acc=0.8961, val_acc=0.8982\n",
      "Epoch 04: train_acc=0.9214, val_acc=0.9124\n",
      "Epoch 05: train_acc=0.9450, val_acc=0.9022\n",
      "Epoch 06: train_acc=0.9656, val_acc=0.8982\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=32, LR=0.002\n",
      "Epoch 01: train_acc=0.7124, val_acc=0.8228\n",
      "Epoch 02: train_acc=0.8594, val_acc=0.8676\n",
      "Epoch 03: train_acc=0.9128, val_acc=0.8656\n",
      "Epoch 04: train_acc=0.9402, val_acc=0.8574\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=32, LR=0.003\n",
      "Epoch 01: train_acc=0.7251, val_acc=0.8513\n",
      "Epoch 02: train_acc=0.8449, val_acc=0.8493\n",
      "Epoch 03: train_acc=0.9096, val_acc=0.8676\n",
      "Epoch 04: train_acc=0.9126, val_acc=0.8676\n",
      "Epoch 05: train_acc=0.9656, val_acc=0.8839\n",
      "Epoch 06: train_acc=0.9801, val_acc=0.8697\n",
      "Epoch 07: train_acc=0.9835, val_acc=0.8758\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=64, LR=0.0005\n",
      "Epoch 01: train_acc=0.5159, val_acc=0.6538\n",
      "Epoch 02: train_acc=0.7726, val_acc=0.8289\n",
      "Epoch 03: train_acc=0.8435, val_acc=0.8615\n",
      "Epoch 04: train_acc=0.8768, val_acc=0.8798\n",
      "Epoch 05: train_acc=0.9013, val_acc=0.8839\n",
      "Epoch 06: train_acc=0.9196, val_acc=0.8900\n",
      "Epoch 07: train_acc=0.9357, val_acc=0.8941\n",
      "Epoch 08: train_acc=0.9459, val_acc=0.9084\n",
      "Epoch 09: train_acc=0.9547, val_acc=0.8921\n",
      "Epoch 10: train_acc=0.9620, val_acc=0.8880\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=64, LR=0.001\n",
      "Epoch 01: train_acc=0.6125, val_acc=0.7984\n",
      "Epoch 02: train_acc=0.8200, val_acc=0.8187\n",
      "Epoch 03: train_acc=0.8693, val_acc=0.8798\n",
      "Epoch 04: train_acc=0.9119, val_acc=0.8982\n",
      "Epoch 05: train_acc=0.9316, val_acc=0.8798\n",
      "Epoch 06: train_acc=0.9534, val_acc=0.8839\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=64, LR=0.002\n",
      "Epoch 01: train_acc=0.6712, val_acc=0.8167\n",
      "Epoch 02: train_acc=0.8490, val_acc=0.8432\n",
      "Epoch 03: train_acc=0.8918, val_acc=0.8900\n",
      "Epoch 04: train_acc=0.9296, val_acc=0.8921\n",
      "Epoch 05: train_acc=0.9380, val_acc=0.8758\n",
      "Epoch 06: train_acc=0.9545, val_acc=0.8697\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=64, LR=0.003\n",
      "Epoch 01: train_acc=0.6789, val_acc=0.8086\n",
      "Epoch 02: train_acc=0.8347, val_acc=0.8371\n",
      "Epoch 03: train_acc=0.9031, val_acc=0.8615\n",
      "Epoch 04: train_acc=0.9468, val_acc=0.8859\n",
      "Epoch 05: train_acc=0.9629, val_acc=0.8758\n",
      "Epoch 06: train_acc=0.9819, val_acc=0.8798\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=128, LR=0.0005\n",
      "Epoch 01: train_acc=0.4287, val_acc=0.5255\n",
      "Epoch 02: train_acc=0.6606, val_acc=0.7699\n",
      "Epoch 03: train_acc=0.7702, val_acc=0.8187\n",
      "Epoch 04: train_acc=0.8277, val_acc=0.8350\n",
      "Epoch 05: train_acc=0.8551, val_acc=0.8778\n",
      "Epoch 06: train_acc=0.8807, val_acc=0.8839\n",
      "Epoch 07: train_acc=0.9001, val_acc=0.8839\n",
      "Epoch 08: train_acc=0.9169, val_acc=0.8982\n",
      "Epoch 09: train_acc=0.9287, val_acc=0.8921\n",
      "Epoch 10: train_acc=0.9361, val_acc=0.9104\n",
      "Epoch 11: train_acc=0.9504, val_acc=0.9063\n",
      "Epoch 12: train_acc=0.9567, val_acc=0.9022\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=128, LR=0.001\n",
      "Epoch 01: train_acc=0.5188, val_acc=0.7393\n",
      "Epoch 02: train_acc=0.7584, val_acc=0.8289\n",
      "Epoch 03: train_acc=0.8438, val_acc=0.8737\n",
      "Epoch 04: train_acc=0.8832, val_acc=0.8758\n",
      "Epoch 05: train_acc=0.8995, val_acc=0.8697\n",
      "Epoch 06: train_acc=0.9282, val_acc=0.8921\n",
      "Epoch 07: train_acc=0.9450, val_acc=0.8921\n",
      "Epoch 08: train_acc=0.9547, val_acc=0.8819\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=128, LR=0.002\n",
      "Epoch 01: train_acc=0.5736, val_acc=0.7658\n",
      "Epoch 02: train_acc=0.8184, val_acc=0.8452\n",
      "Epoch 03: train_acc=0.8854, val_acc=0.8697\n",
      "Epoch 04: train_acc=0.9103, val_acc=0.8697\n",
      "Epoch 05: train_acc=0.9226, val_acc=0.8737\n",
      "Epoch 06: train_acc=0.9488, val_acc=0.8758\n",
      "Epoch 07: train_acc=0.9669, val_acc=0.9002\n",
      "Epoch 08: train_acc=0.9803, val_acc=0.8961\n",
      "Epoch 09: train_acc=0.9885, val_acc=0.9022\n",
      "Epoch 10: train_acc=0.9939, val_acc=0.8880\n",
      "Epoch 11: train_acc=0.9941, val_acc=0.8859\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=128, LR=0.003\n",
      "Epoch 01: train_acc=0.5956, val_acc=0.7678\n",
      "Epoch 02: train_acc=0.8111, val_acc=0.8635\n",
      "Epoch 03: train_acc=0.8721, val_acc=0.8391\n",
      "Epoch 04: train_acc=0.9241, val_acc=0.8819\n",
      "Epoch 05: train_acc=0.9543, val_acc=0.8778\n",
      "Epoch 06: train_acc=0.9712, val_acc=0.8819\n",
      "Early stopping\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     67\u001b[39m         model.load_state_dict(best_model_state)\n\u001b[32m     69\u001b[39m     results.append({\u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m: bs, \u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: lr, \u001b[33m\"\u001b[39m\u001b[33mval_acc\u001b[39m\u001b[33m\"\u001b[39m: best_val_acc})\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m df_results = \u001b[43mpd\u001b[49m.DataFrame(results).sort_values(\u001b[33m\"\u001b[39m\u001b[33mval_acc\u001b[39m\u001b[33m\"\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     72\u001b[39m best = df_results.iloc[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Grid search with early stopping that SAVES BEST WEIGHTS\n",
    "# -----------------------------\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.3\n",
    "NUM_CLASSES = len(LABEL.vocab)\n",
    "MAX_EPOCHS = 20\n",
    "PATIENCE = 3\n",
    "\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "batch_sizes = [32, 64, 128]\n",
    "learning_rates = [5e-4, 1e-3, 2e-3, 3e-3]\n",
    "\n",
    "results = []\n",
    "\n",
    "for bs, lr in itertools.product(batch_sizes, learning_rates):\n",
    "    print(f\"\\nðŸ” Testing BATCH={bs}, LR={lr}\")\n",
    "    train_iter, valid_iter = data.BucketIterator.splits(\n",
    "        (train_data, valid_data),\n",
    "        batch_size=bs,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # fresh embedding weights each trial\n",
    "    emb_layer = nn.Embedding.from_pretrained(\n",
    "        embedding_layer.weight.data.clone(), freeze=False, padding_idx=PAD_IDX\n",
    "    )\n",
    "    model = RNNClassifier(\n",
    "        embedding_layer=emb_layer,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        bidirectional=BIDIRECTIONAL,\n",
    "        dropout=DROPOUT,\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_acc = -1.0\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        train_loss, train_acc = epoch_run(train_iter, model, criterion, optimizer)\n",
    "        val_loss, val_acc = epoch_run(valid_iter, model, criterion, optimizer=None)\n",
    "        print(f\"Epoch {epoch:02d}: train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "        # ---- your early stopping block (with saving best) ----\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())  # keep BEST weights\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    # restore best weights before logging result (so this combo truly reflects its best)\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    results.append({\"batch_size\": bs, \"lr\": lr, \"val_acc\": best_val_acc})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ† Best configuration:\n",
      "batch_size    32.000000\n",
      "lr             0.001000\n",
      "val_acc        0.912424\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "All results (sorted):\n",
      "     batch_size      lr   val_acc\n",
      "0           32  0.0010  0.912424\n",
      "1          128  0.0005  0.910387\n",
      "2           64  0.0005  0.908350\n",
      "3           32  0.0005  0.906314\n",
      "4          128  0.0020  0.902240\n",
      "5           64  0.0010  0.898167\n",
      "6          128  0.0010  0.892057\n",
      "7           64  0.0020  0.892057\n",
      "8           64  0.0030  0.885947\n",
      "9           32  0.0030  0.883910\n",
      "10         128  0.0030  0.881874\n",
      "11          32  0.0020  0.867617\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results).sort_values(\"val_acc\", ascending=False)\n",
    "best = df_results.iloc[0]\n",
    "print(\"\\nðŸ† Best configuration:\")\n",
    "print(best)\n",
    "print(\"\\nAll results (sorted):\\n\", df_results.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Fix best batch size & lr from your previous grid search result `best` ---\n",
    "best_batch_size = int(best[\"batch_size\"])\n",
    "best_lr = float(best[\"lr\"])\n",
    "print(f\"\\nâœ… Using best hyperparams from LR/Batch search -> batch_size={best_batch_size}, lr={best_lr}\")\n",
    "\n",
    "# --- 2) Hidden-dim search using fixed (batch_size, lr) ---\n",
    "\n",
    "# Search space (tweak as you like)\n",
    "hidden_dims = [64, 96, 128, 192, 256, 384, 512]\n",
    "\n",
    "results_hd = []\n",
    "\n",
    "# Rebuild iterators ONCE with best batch size\n",
    "train_iter, valid_iter = data.BucketIterator.splits(\n",
    "    (train_data, valid_data),\n",
    "    batch_size=best_batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "for hd in hidden_dims:\n",
    "    print(f\"\\nðŸ§ª Testing hidden_dim={hd}\")\n",
    "\n",
    "    # fresh embedding weights for each trial\n",
    "    emb_layer = nn.Embedding.from_pretrained(\n",
    "        embedding_layer.weight.data.clone(), freeze=False, padding_idx=PAD_IDX\n",
    "    )\n",
    "\n",
    "    model = RNNClassifier(\n",
    "        embedding_layer=emb_layer,\n",
    "        hidden_dim=hd,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        bidirectional=BIDIRECTIONAL,\n",
    "        dropout=DROPOUT,\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)\n",
    "\n",
    "    best_val_acc_hd = -1.0\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state_hd = None\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        train_loss, train_acc = epoch_run(train_iter, model, criterion, optimizer)\n",
    "        val_loss, val_acc = epoch_run(valid_iter, model, criterion, optimizer=None)\n",
    "        print(f\"Epoch {epoch:02d}: train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "        # Early stopping + save-best (your pattern)\n",
    "        if val_acc > best_val_acc_hd:\n",
    "            best_val_acc_hd = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state_hd = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    # Restore best for this hidden_dim before recording\n",
    "    if best_model_state_hd is not None:\n",
    "        model.load_state_dict(best_model_state_hd)\n",
    "\n",
    "    results_hd.append({\"hidden_dim\": hd, \"val_acc\": best_val_acc_hd})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) Pick best hidden dim, print & save variable ---\n",
    "df_hd = pd.DataFrame(results_hd).sort_values(\"val_acc\", ascending=False).reset_index(drop=True)\n",
    "best_hidden_dim = int(df_hd.loc[0, \"hidden_dim\"])\n",
    "\n",
    "print(\"\\nðŸ† Best hidden_dim configuration:\")\n",
    "print(df_hd.loc[0])\n",
    "print(\"\\nAll hidden_dim results (sorted):\")\n",
    "print(df_hd)\n",
    "\n",
    "# Variables now set for subsequent training:\n",
    "print(f\"\\nðŸ‘‰ Final choice: best_batch_size={best_batch_size}, best_lr={best_lr}, best_hidden_dim={best_hidden_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torchtext import data\n",
    "\n",
    "# -----------------------------\n",
    "# Generic helpers\n",
    "# -----------------------------\n",
    "def build_iters(batch_size, train_data, valid_data, device):\n",
    "    return data.BucketIterator.splits(\n",
    "        (train_data, valid_data),\n",
    "        batch_size=batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "def build_model(embedding_layer, pad_idx, hidden_dim, num_layers, bidirectional, dropout, num_classes, device):\n",
    "    emb_layer = nn.Embedding.from_pretrained(\n",
    "        embedding_layer.weight.data.clone(), freeze=False, padding_idx=pad_idx\n",
    "    )\n",
    "    model = RNNClassifier(\n",
    "        embedding_layer=emb_layer,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        bidirectional=bidirectional,\n",
    "        dropout=dropout,\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    return model\n",
    "\n",
    "def epoch_run(iterator, model, criterion, optimizer=None):\n",
    "    model.train(optimizer is not None)\n",
    "    tot_loss, tot_correct, tot_count = 0.0, 0, 0\n",
    "    for batch in iterator:\n",
    "        text, labels = batch.text, batch.label\n",
    "        logits = model(text)\n",
    "        loss = criterion(logits, labels)\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "        tot_loss   += loss.item() * labels.size(0)\n",
    "        tot_correct += (logits.argmax(1) == labels).sum().item()\n",
    "        tot_count  += labels.size(0)\n",
    "    return tot_loss / max(tot_count,1), tot_correct / max(tot_count,1)\n",
    "\n",
    "def train_one_config(batch_size, lr, hidden_dim, *,\n",
    "                     num_layers, bidirectional, dropout, num_classes,\n",
    "                     train_data, valid_data, pad_idx, device,\n",
    "                     max_epochs=20, patience=3):\n",
    "    \"\"\"Train one (batch_size, lr, hidden_dim) with early stopping; return best_val_acc and best_state.\"\"\"\n",
    "    train_iter, valid_iter = build_iters(batch_size, train_data, valid_data, device)\n",
    "    model = build_model(embedding_layer, pad_idx, hidden_dim, num_layers, bidirectional, dropout, num_classes, device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_acc = -1.0\n",
    "    epochs_no_improve = 0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        train_loss, train_acc = epoch_run(train_iter, model, criterion, optimizer)\n",
    "        val_loss,   val_acc   = epoch_run(valid_iter, model, criterion, optimizer=None)\n",
    "        print(f\"Epoch {epoch:02d}: train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return best_val_acc, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Testing BATCH=32, LR=0.0005\n",
      "Epoch 01: train_acc=0.6187, val_acc=0.8147\n",
      "Epoch 02: train_acc=0.8102, val_acc=0.8391\n",
      "Epoch 03: train_acc=0.8718, val_acc=0.8778\n",
      "Epoch 04: train_acc=0.8981, val_acc=0.9043\n",
      "Epoch 05: train_acc=0.9167, val_acc=0.8880\n",
      "Epoch 06: train_acc=0.9366, val_acc=0.9043\n",
      "Epoch 07: train_acc=0.9545, val_acc=0.8941\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=32, LR=0.001\n",
      "Epoch 01: train_acc=0.6769, val_acc=0.8737\n",
      "Epoch 02: train_acc=0.8406, val_acc=0.8635\n",
      "Epoch 03: train_acc=0.9051, val_acc=0.8859\n",
      "Epoch 04: train_acc=0.9282, val_acc=0.9022\n",
      "Epoch 05: train_acc=0.9452, val_acc=0.8758\n",
      "Epoch 06: train_acc=0.9654, val_acc=0.8859\n",
      "Epoch 07: train_acc=0.9758, val_acc=0.8880\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=32, LR=0.002\n",
      "Epoch 01: train_acc=0.6791, val_acc=0.8391\n",
      "Epoch 02: train_acc=0.8576, val_acc=0.8819\n",
      "Epoch 03: train_acc=0.9092, val_acc=0.8819\n",
      "Epoch 04: train_acc=0.9466, val_acc=0.8819\n",
      "Epoch 05: train_acc=0.9558, val_acc=0.9043\n",
      "Epoch 06: train_acc=0.9817, val_acc=0.8900\n",
      "Epoch 07: train_acc=0.9706, val_acc=0.8534\n",
      "Epoch 08: train_acc=0.9694, val_acc=0.8758\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=32, LR=0.003\n",
      "Epoch 01: train_acc=0.7339, val_acc=0.8248\n",
      "Epoch 02: train_acc=0.8462, val_acc=0.8493\n",
      "Epoch 03: train_acc=0.9083, val_acc=0.8656\n",
      "Epoch 04: train_acc=0.9062, val_acc=0.8819\n",
      "Epoch 05: train_acc=0.9599, val_acc=0.8819\n",
      "Epoch 06: train_acc=0.9699, val_acc=0.8697\n",
      "Epoch 07: train_acc=0.9832, val_acc=0.8676\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=64, LR=0.0005\n",
      "Epoch 01: train_acc=0.5410, val_acc=0.7291\n",
      "Epoch 02: train_acc=0.7634, val_acc=0.8167\n",
      "Epoch 03: train_acc=0.8367, val_acc=0.8737\n",
      "Epoch 04: train_acc=0.8734, val_acc=0.8778\n",
      "Epoch 05: train_acc=0.8986, val_acc=0.9043\n",
      "Epoch 06: train_acc=0.9194, val_acc=0.9002\n",
      "Epoch 07: train_acc=0.9334, val_acc=0.8839\n",
      "Epoch 08: train_acc=0.9438, val_acc=0.8982\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=64, LR=0.001\n",
      "Epoch 01: train_acc=0.6341, val_acc=0.8106\n",
      "Epoch 02: train_acc=0.8245, val_acc=0.8432\n",
      "Epoch 03: train_acc=0.8793, val_acc=0.8859\n",
      "Epoch 04: train_acc=0.9153, val_acc=0.8880\n",
      "Epoch 05: train_acc=0.9359, val_acc=0.8961\n",
      "Epoch 06: train_acc=0.9531, val_acc=0.8900\n",
      "Epoch 07: train_acc=0.9642, val_acc=0.8961\n",
      "Epoch 08: train_acc=0.9796, val_acc=0.8798\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=64, LR=0.002\n",
      "Epoch 01: train_acc=0.6843, val_acc=0.8248\n",
      "Epoch 02: train_acc=0.8548, val_acc=0.8574\n",
      "Epoch 03: train_acc=0.8952, val_acc=0.8880\n",
      "Epoch 04: train_acc=0.9355, val_acc=0.9043\n",
      "Epoch 05: train_acc=0.9581, val_acc=0.8656\n",
      "Epoch 06: train_acc=0.9423, val_acc=0.8737\n",
      "Epoch 07: train_acc=0.9611, val_acc=0.8778\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=64, LR=0.003\n",
      "Epoch 01: train_acc=0.6803, val_acc=0.8187\n",
      "Epoch 02: train_acc=0.8487, val_acc=0.8513\n",
      "Epoch 03: train_acc=0.9090, val_acc=0.8900\n",
      "Epoch 04: train_acc=0.9466, val_acc=0.8635\n",
      "Epoch 05: train_acc=0.9504, val_acc=0.8411\n",
      "Epoch 06: train_acc=0.9697, val_acc=0.8513\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=128, LR=0.0005\n",
      "Epoch 01: train_acc=0.4432, val_acc=0.5825\n",
      "Epoch 02: train_acc=0.6630, val_acc=0.7739\n",
      "Epoch 03: train_acc=0.7618, val_acc=0.8147\n",
      "Epoch 04: train_acc=0.8311, val_acc=0.8534\n",
      "Epoch 05: train_acc=0.8628, val_acc=0.8778\n",
      "Epoch 06: train_acc=0.8843, val_acc=0.8880\n",
      "Epoch 07: train_acc=0.8965, val_acc=0.8900\n",
      "Epoch 08: train_acc=0.9146, val_acc=0.8941\n",
      "Epoch 09: train_acc=0.9289, val_acc=0.8921\n",
      "Epoch 10: train_acc=0.9418, val_acc=0.8982\n",
      "Epoch 11: train_acc=0.9504, val_acc=0.8900\n",
      "Epoch 12: train_acc=0.9531, val_acc=0.9022\n",
      "Epoch 13: train_acc=0.9663, val_acc=0.9063\n",
      "Epoch 14: train_acc=0.9710, val_acc=0.9043\n",
      "Epoch 15: train_acc=0.9751, val_acc=0.9104\n",
      "Epoch 16: train_acc=0.9794, val_acc=0.8961\n",
      "Epoch 17: train_acc=0.9812, val_acc=0.8697\n",
      "Epoch 18: train_acc=0.9810, val_acc=0.8839\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=128, LR=0.001\n",
      "Epoch 01: train_acc=0.5306, val_acc=0.7393\n",
      "Epoch 02: train_acc=0.7720, val_acc=0.8371\n",
      "Epoch 03: train_acc=0.8415, val_acc=0.8758\n",
      "Epoch 04: train_acc=0.8870, val_acc=0.8961\n",
      "Epoch 05: train_acc=0.9058, val_acc=0.8900\n",
      "Epoch 06: train_acc=0.9293, val_acc=0.8982\n",
      "Epoch 07: train_acc=0.9380, val_acc=0.9043\n",
      "Epoch 08: train_acc=0.9595, val_acc=0.8961\n",
      "Epoch 09: train_acc=0.9706, val_acc=0.8982\n",
      "Epoch 10: train_acc=0.9792, val_acc=0.9043\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=128, LR=0.002\n",
      "Epoch 01: train_acc=0.5829, val_acc=0.7617\n",
      "Epoch 02: train_acc=0.8003, val_acc=0.8330\n",
      "Epoch 03: train_acc=0.8453, val_acc=0.8717\n",
      "Epoch 04: train_acc=0.8990, val_acc=0.8859\n",
      "Epoch 05: train_acc=0.9307, val_acc=0.8880\n",
      "Epoch 06: train_acc=0.9488, val_acc=0.8941\n",
      "Epoch 07: train_acc=0.9703, val_acc=0.8778\n",
      "Epoch 08: train_acc=0.9826, val_acc=0.8921\n",
      "Epoch 09: train_acc=0.9878, val_acc=0.8941\n",
      "Early stopping\n",
      "\n",
      "ðŸ” Testing BATCH=128, LR=0.003\n",
      "Epoch 01: train_acc=0.6162, val_acc=0.7862\n",
      "Epoch 02: train_acc=0.8218, val_acc=0.8737\n",
      "Epoch 03: train_acc=0.8906, val_acc=0.8635\n",
      "Epoch 04: train_acc=0.9368, val_acc=0.8961\n",
      "Epoch 05: train_acc=0.9509, val_acc=0.8819\n",
      "Epoch 06: train_acc=0.9622, val_acc=0.8921\n",
      "Epoch 07: train_acc=0.9758, val_acc=0.8921\n",
      "Early stopping\n",
      "\n",
      "ðŸ† Best (batch, lr):\n",
      "batch_size    128.000000\n",
      "lr              0.000500\n",
      "val_acc         0.910387\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Saved vars -> best_batch_size=128, best_lr=0.0005\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES   = len(LABEL.vocab)\n",
    "NUM_LAYERS    = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT       = 0.3\n",
    "MAX_EPOCHS    = 20\n",
    "PATIENCE      = 3\n",
    "\n",
    "batch_sizes     = [32, 64, 128]\n",
    "learning_rates  = [5e-4, 1e-3, 2e-3, 3e-3]\n",
    "HIDDEN_DIM_FIXED = 128   # fixed while tuning (batch, lr)\n",
    "\n",
    "results = []\n",
    "for bs, lr in itertools.product(batch_sizes, learning_rates):\n",
    "    print(f\"\\nðŸ” Testing BATCH={bs}, LR={lr}\")\n",
    "    val_acc, _ = train_one_config(\n",
    "        bs, lr, HIDDEN_DIM_FIXED,\n",
    "        num_layers=NUM_LAYERS, bidirectional=BIDIRECTIONAL, dropout=DROPOUT, num_classes=NUM_CLASSES,\n",
    "        train_data=train_data, valid_data=valid_data, pad_idx=PAD_IDX, device=device,\n",
    "        max_epochs=MAX_EPOCHS, patience=PATIENCE\n",
    "    )\n",
    "    results.append({\"batch_size\": bs, \"lr\": lr, \"val_acc\": val_acc})\n",
    "\n",
    "df_results = pd.DataFrame(results).sort_values(\"val_acc\", ascending=False).reset_index(drop=True)\n",
    "best = df_results.loc[0]\n",
    "best_batch_size = int(best[\"batch_size\"])\n",
    "best_lr        = float(best[\"lr\"])\n",
    "print(\"\\nðŸ† Best (batch, lr):\")\n",
    "print(best)\n",
    "print(f\"\\nSaved vars -> best_batch_size={best_batch_size}, best_lr={best_lr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª Testing hidden_dim=64 (batch=128, lr=0.0005)\n",
      "Epoch 01: train_acc=0.3718, val_acc=0.4847\n",
      "Epoch 02: train_acc=0.5661, val_acc=0.7006\n",
      "Epoch 03: train_acc=0.6891, val_acc=0.7780\n",
      "Epoch 04: train_acc=0.7538, val_acc=0.8086\n",
      "Epoch 05: train_acc=0.8000, val_acc=0.8411\n",
      "Epoch 06: train_acc=0.8379, val_acc=0.8554\n",
      "Epoch 07: train_acc=0.8659, val_acc=0.8778\n",
      "Epoch 08: train_acc=0.8915, val_acc=0.8819\n",
      "Epoch 09: train_acc=0.9062, val_acc=0.8900\n",
      "Epoch 10: train_acc=0.9214, val_acc=0.9002\n",
      "Epoch 11: train_acc=0.9275, val_acc=0.8941\n",
      "Epoch 12: train_acc=0.9418, val_acc=0.8900\n",
      "Epoch 13: train_acc=0.9472, val_acc=0.8941\n",
      "Early stopping\n",
      "\n",
      "ðŸ§ª Testing hidden_dim=96 (batch=128, lr=0.0005)\n",
      "Epoch 01: train_acc=0.3872, val_acc=0.5295\n",
      "Epoch 02: train_acc=0.5942, val_acc=0.6802\n",
      "Epoch 03: train_acc=0.7235, val_acc=0.7637\n",
      "Epoch 04: train_acc=0.8030, val_acc=0.8411\n",
      "Epoch 05: train_acc=0.8458, val_acc=0.8310\n",
      "Epoch 06: train_acc=0.8755, val_acc=0.8880\n",
      "Epoch 07: train_acc=0.8956, val_acc=0.8941\n",
      "Epoch 08: train_acc=0.9101, val_acc=0.8921\n",
      "Epoch 09: train_acc=0.9235, val_acc=0.9002\n",
      "Epoch 10: train_acc=0.9352, val_acc=0.8941\n",
      "Epoch 11: train_acc=0.9466, val_acc=0.8982\n",
      "Epoch 12: train_acc=0.9481, val_acc=0.8900\n",
      "Early stopping\n",
      "\n",
      "ðŸ§ª Testing hidden_dim=128 (batch=128, lr=0.0005)\n",
      "Epoch 01: train_acc=0.4300, val_acc=0.5886\n",
      "Epoch 02: train_acc=0.6784, val_acc=0.7597\n",
      "Epoch 03: train_acc=0.7724, val_acc=0.8248\n",
      "Epoch 04: train_acc=0.8265, val_acc=0.8574\n",
      "Epoch 05: train_acc=0.8544, val_acc=0.8880\n",
      "Epoch 06: train_acc=0.8813, val_acc=0.8574\n",
      "Epoch 07: train_acc=0.8836, val_acc=0.8941\n",
      "Epoch 08: train_acc=0.9112, val_acc=0.8941\n",
      "Epoch 09: train_acc=0.9303, val_acc=0.9104\n",
      "Epoch 10: train_acc=0.9373, val_acc=0.8941\n",
      "Epoch 11: train_acc=0.9441, val_acc=0.9124\n",
      "Epoch 12: train_acc=0.9563, val_acc=0.9002\n",
      "Epoch 13: train_acc=0.9615, val_acc=0.9063\n",
      "Epoch 14: train_acc=0.9703, val_acc=0.9104\n",
      "Early stopping\n",
      "\n",
      "ðŸ§ª Testing hidden_dim=192 (batch=128, lr=0.0005)\n",
      "Epoch 01: train_acc=0.4740, val_acc=0.6110\n",
      "Epoch 02: train_acc=0.6966, val_acc=0.7821\n",
      "Epoch 03: train_acc=0.7989, val_acc=0.8574\n",
      "Epoch 04: train_acc=0.8481, val_acc=0.8513\n",
      "Epoch 05: train_acc=0.8770, val_acc=0.8961\n",
      "Epoch 06: train_acc=0.8922, val_acc=0.8350\n",
      "Epoch 07: train_acc=0.9087, val_acc=0.8798\n",
      "Epoch 08: train_acc=0.9207, val_acc=0.8961\n",
      "Early stopping\n",
      "\n",
      "ðŸ§ª Testing hidden_dim=256 (batch=128, lr=0.0005)\n",
      "Epoch 01: train_acc=0.4993, val_acc=0.7210\n",
      "Epoch 02: train_acc=0.7396, val_acc=0.8147\n",
      "Epoch 03: train_acc=0.8193, val_acc=0.8554\n",
      "Epoch 04: train_acc=0.8623, val_acc=0.8859\n",
      "Epoch 05: train_acc=0.8739, val_acc=0.8839\n",
      "Epoch 06: train_acc=0.8890, val_acc=0.8941\n",
      "Epoch 07: train_acc=0.9189, val_acc=0.8921\n",
      "Epoch 08: train_acc=0.9248, val_acc=0.8839\n",
      "Epoch 09: train_acc=0.9300, val_acc=0.8921\n",
      "Early stopping\n",
      "\n",
      "ðŸ§ª Testing hidden_dim=384 (batch=128, lr=0.0005)\n",
      "Epoch 01: train_acc=0.5288, val_acc=0.7536\n",
      "Epoch 02: train_acc=0.7600, val_acc=0.8106\n",
      "Epoch 03: train_acc=0.8340, val_acc=0.8656\n",
      "Epoch 04: train_acc=0.8653, val_acc=0.8961\n",
      "Epoch 05: train_acc=0.8875, val_acc=0.8880\n",
      "Epoch 06: train_acc=0.8972, val_acc=0.8676\n",
      "Epoch 07: train_acc=0.9099, val_acc=0.8982\n",
      "Epoch 08: train_acc=0.9192, val_acc=0.8859\n",
      "Epoch 09: train_acc=0.9398, val_acc=0.8982\n",
      "Epoch 10: train_acc=0.9479, val_acc=0.8778\n",
      "Early stopping\n",
      "\n",
      "ðŸ§ª Testing hidden_dim=512 (batch=128, lr=0.0005)\n",
      "Epoch 01: train_acc=0.5365, val_acc=0.7189\n",
      "Epoch 02: train_acc=0.7577, val_acc=0.8554\n",
      "Epoch 03: train_acc=0.8383, val_acc=0.8574\n",
      "Epoch 04: train_acc=0.8671, val_acc=0.8595\n",
      "Epoch 05: train_acc=0.8877, val_acc=0.8859\n",
      "Epoch 06: train_acc=0.9024, val_acc=0.8635\n",
      "Epoch 07: train_acc=0.9076, val_acc=0.8493\n",
      "Epoch 08: train_acc=0.9106, val_acc=0.8717\n",
      "Early stopping\n",
      "\n",
      "ðŸ† Best hidden_dim configuration:\n",
      "hidden_dim    128.000000\n",
      "val_acc         0.912424\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Saved var -> best_hidden_dim=128\n",
      "\n",
      "All hidden_dim results:\n",
      "    hidden_dim   val_acc\n",
      "0         128  0.912424\n",
      "1          64  0.900204\n",
      "2          96  0.900204\n",
      "3         384  0.898167\n",
      "4         192  0.896130\n",
      "5         256  0.894094\n",
      "6         512  0.885947\n"
     ]
    }
   ],
   "source": [
    "hidden_dims = [64, 96, 128, 192, 256, 384, 512]\n",
    "results_hd = []\n",
    "\n",
    "for hd in hidden_dims:\n",
    "    print(f\"\\nðŸ§ª Testing hidden_dim={hd} (batch={best_batch_size}, lr={best_lr})\")\n",
    "    val_acc, _ = train_one_config(\n",
    "        best_batch_size, best_lr, hd,\n",
    "        num_layers=NUM_LAYERS, bidirectional=BIDIRECTIONAL, dropout=DROPOUT, num_classes=NUM_CLASSES,\n",
    "        train_data=train_data, valid_data=valid_data, pad_idx=PAD_IDX, device=device,\n",
    "        max_epochs=MAX_EPOCHS, patience=PATIENCE\n",
    "    )\n",
    "    results_hd.append({\"hidden_dim\": hd, \"val_acc\": val_acc})\n",
    "\n",
    "df_hd = pd.DataFrame(results_hd).sort_values(\"val_acc\", ascending=False).reset_index(drop=True)\n",
    "best_hidden_dim = int(df_hd.loc[0, \"hidden_dim\"])\n",
    "print(\"\\nðŸ† Best hidden_dim configuration:\")\n",
    "print(df_hd.loc[0])\n",
    "print(f\"\\nSaved var -> best_hidden_dim={best_hidden_dim}\")\n",
    "print(\"\\nAll hidden_dim results:\\n\", df_hd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------kfold------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, copy, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext import data\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# -----------------------------\n",
    "# Reproducibility\n",
    "# -----------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -----------------------------\n",
    "# Generic helpers\n",
    "# -----------------------------\n",
    "def build_iters(batch_size, train_data, valid_data, device):\n",
    "    return data.BucketIterator.splits(\n",
    "        (train_data, valid_data),\n",
    "        batch_size=batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "def build_model(embedding_layer, pad_idx, hidden_dim, num_layers, bidirectional, dropout, num_classes, device):\n",
    "    emb_layer = nn.Embedding.from_pretrained(\n",
    "        embedding_layer.weight.data.clone(), freeze=False, padding_idx=pad_idx\n",
    "    )\n",
    "    model = RNNClassifier(\n",
    "        embedding_layer=emb_layer,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        bidirectional=bidirectional,\n",
    "        dropout=dropout,\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    return model\n",
    "\n",
    "def epoch_run(iterator, model, criterion, optimizer=None):\n",
    "    model.train(optimizer is not None)\n",
    "    tot_loss, tot_correct, tot_count = 0.0, 0, 0\n",
    "    for batch in iterator:\n",
    "        text, labels = batch.text, batch.label\n",
    "        logits = model(text)\n",
    "        loss = criterion(logits, labels)\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "        tot_loss += loss.item() * labels.size(0)\n",
    "        tot_correct += (logits.argmax(1) == labels).sum().item()\n",
    "        tot_count += labels.size(0)\n",
    "    return tot_loss / max(tot_count,1), tot_correct / max(tot_count,1)\n",
    "\n",
    "# -----------------------------\n",
    "# K-Fold training function\n",
    "# -----------------------------\n",
    "def train_kfold_config(k_folds, batch_size, lr, hidden_dim, *,\n",
    "                       num_layers, bidirectional, dropout, num_classes,\n",
    "                       full_dataset, pad_idx, device, embedding_layer,\n",
    "                       max_epochs=20, patience=3, seed=42):\n",
    "\n",
    "    set_seed(seed)  # ðŸ”’ make results deterministic\n",
    "\n",
    "    # Convert torchtext dataset examples to indices for sklearn KFold\n",
    "    all_indices = np.arange(len(full_dataset))\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    fold_results = []\n",
    "    fold_models = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(all_indices), 1):\n",
    "        print(f\"\\n===== Fold {fold}/{k_folds} =====\")\n",
    "\n",
    "        # Create new Dataset splits\n",
    "        train_data = [full_dataset.examples[i] for i in train_idx]\n",
    "        valid_data = [full_dataset.examples[i] for i in val_idx]\n",
    "        train_data = data.Dataset(train_data, fields=full_dataset.fields)\n",
    "        valid_data = data.Dataset(valid_data, fields=full_dataset.fields)\n",
    "\n",
    "        # Build iterators\n",
    "        train_iter, valid_iter = build_iters(batch_size, train_data, valid_data, device)\n",
    "\n",
    "        # Build model, loss, optimizer\n",
    "        model = build_model(embedding_layer, pad_idx, hidden_dim, num_layers, bidirectional, dropout, num_classes, device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        best_val_acc = -1.0\n",
    "        epochs_no_improve = 0\n",
    "        best_state = None\n",
    "\n",
    "        # Train per fold\n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            train_loss, train_acc = epoch_run(train_iter, model, criterion, optimizer)\n",
    "            val_loss, val_acc = epoch_run(valid_iter, model, criterion, optimizer=None)\n",
    "            print(f\"Epoch {epoch:02d}: train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                epochs_no_improve = 0\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "\n",
    "        # Load best model\n",
    "        if best_state:\n",
    "            model.load_state_dict(best_state)\n",
    "\n",
    "        print(f\"Fold {fold} best val_acc={best_val_acc:.4f}\")\n",
    "        fold_results.append(best_val_acc)\n",
    "        fold_models.append(model)\n",
    "\n",
    "    # Average accuracy\n",
    "    mean_acc = np.mean(fold_results)\n",
    "    print(f\"\\n===== K-Fold Results =====\")\n",
    "    for i, acc in enumerate(fold_results, 1):\n",
    "        print(f\"Fold {i}: {acc:.4f}\")\n",
    "    print(f\"Average Val Accuracy: {mean_acc:.4f}\")\n",
    "\n",
    "    return mean_acc, fold_results, fold_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” K-Fold Testing BATCH=32, LR=0.0005\n",
      "\n",
      "===== Fold 1/5 =====\n",
      "Epoch 01: train_acc=0.5614, val_acc=0.7534\n",
      "Epoch 02: train_acc=0.8086, val_acc=0.8303\n",
      "Epoch 03: train_acc=0.8692, val_acc=0.8473\n",
      "Epoch 04: train_acc=0.8890, val_acc=0.8710\n",
      "Epoch 05: train_acc=0.9077, val_acc=0.8676\n",
      "Epoch 06: train_acc=0.9340, val_acc=0.8812\n",
      "Epoch 07: train_acc=0.9414, val_acc=0.8710\n",
      "Epoch 08: train_acc=0.9524, val_acc=0.8846\n",
      "Epoch 09: train_acc=0.9689, val_acc=0.8880\n",
      "Epoch 10: train_acc=0.9745, val_acc=0.8756\n",
      "Epoch 11: train_acc=0.9836, val_acc=0.8812\n",
      "Epoch 12: train_acc=0.9887, val_acc=0.8744\n",
      "Early stopping\n",
      "Fold 1 best val_acc=0.8880\n",
      "\n",
      "===== Fold 2/5 =====\n",
      "Epoch 01: train_acc=0.5468, val_acc=0.7271\n",
      "Epoch 02: train_acc=0.8033, val_acc=0.8188\n",
      "Epoch 03: train_acc=0.8647, val_acc=0.8437\n",
      "Epoch 04: train_acc=0.8913, val_acc=0.8505\n",
      "Epoch 05: train_acc=0.9142, val_acc=0.8686\n",
      "Epoch 06: train_acc=0.9389, val_acc=0.8652\n",
      "Epoch 07: train_acc=0.9462, val_acc=0.8664\n",
      "Epoch 08: train_acc=0.9553, val_acc=0.8766\n",
      "Epoch 09: train_acc=0.9669, val_acc=0.8754\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# K-Fold grid search for (batch_size, lr)\n",
    "# ===========================\n",
    "import itertools, numpy as np, pandas as pd\n",
    "\n",
    "# ---- Your fixed model/config bits ----\n",
    "NUM_CLASSES   = len(LABEL.vocab)\n",
    "NUM_LAYERS    = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT       = 0.3\n",
    "MAX_EPOCHS    = 20\n",
    "PATIENCE      = 3\n",
    "SEED          = 42\n",
    "K_FOLDS       = 5\n",
    "\n",
    "batch_sizes       = [32, 64, 128]\n",
    "learning_rates    = [5e-4, 1e-3, 2e-3, 3e-3]\n",
    "HIDDEN_DIM_FIXED  = 128   # fixed while tuning (batch, lr)\n",
    "\n",
    "# IMPORTANT: the full dataset for K-Fold (use your combined training set)\n",
    "FULL_DATASET = train_data   # <- if your full corpus variable has a different name, replace here\n",
    "\n",
    "results = []\n",
    "\n",
    "for bs, lr in itertools.product(batch_sizes, learning_rates):\n",
    "    print(f\"\\nðŸ” K-Fold Testing BATCH={bs}, LR={lr}\")\n",
    "    mean_acc, fold_accs, fold_models = train_kfold_config(\n",
    "        k_folds=K_FOLDS,\n",
    "        batch_size=bs,\n",
    "        lr=lr,\n",
    "        hidden_dim=HIDDEN_DIM_FIXED,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        bidirectional=BIDIRECTIONAL,\n",
    "        dropout=DROPOUT,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        full_dataset=FULL_DATASET,\n",
    "        pad_idx=PAD_IDX,\n",
    "        device=device,\n",
    "        embedding_layer=embedding_layer,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        patience=PATIENCE,\n",
    "        seed=SEED\n",
    "    )\n",
    "    results.append({\n",
    "        \"batch_size\": bs,\n",
    "        \"lr\": lr,\n",
    "        \"cv_mean_acc\": float(mean_acc),\n",
    "        \"cv_std\": float(np.std(fold_accs)),\n",
    "        \"per_fold\": [float(x) for x in fold_accs],  # optional: inspect later\n",
    "    })\n",
    "\n",
    "# Rank by mean CV acc (desc), then by lower std (tie-breaker)\n",
    "df_results = (\n",
    "    pd.DataFrame(results)\n",
    "      .sort_values([\"cv_mean_acc\", \"cv_std\"], ascending=[False, True])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "best = df_results.loc[0]\n",
    "best_batch_size = int(best[\"batch_size\"])\n",
    "best_lr         = float(best[\"lr\"])\n",
    "\n",
    "print(\"\\nðŸ† Best (batch, lr) by K-Fold:\")\n",
    "print(best[[\"batch_size\",\"lr\",\"cv_mean_acc\",\"cv_std\"]])\n",
    "print(f\"\\nSaved vars -> best_batch_size={best_batch_size}, best_lr={best_lr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# K-Fold tuning for hidden_dim\n",
    "# ===========================\n",
    "\n",
    "hidden_dims = [64, 96, 128, 192, 256, 384, 512]\n",
    "results_hd = []\n",
    "\n",
    "for hd in hidden_dims:\n",
    "    print(f\"\\nðŸ§ª K-Fold Testing hidden_dim={hd} (batch={best_batch_size}, lr={best_lr})\")\n",
    "    mean_acc, fold_accs, fold_models = train_kfold_config(\n",
    "        k_folds=K_FOLDS,\n",
    "        batch_size=best_batch_size,\n",
    "        lr=best_lr,\n",
    "        hidden_dim=hd,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        bidirectional=BIDIRECTIONAL,\n",
    "        dropout=DROPOUT,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        full_dataset=train_data,     # full dataset (same as before)\n",
    "        pad_idx=PAD_IDX,\n",
    "        device=device,\n",
    "        embedding_layer=embedding_layer,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        patience=PATIENCE,\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    results_hd.append({\n",
    "        \"hidden_dim\": hd,\n",
    "        \"cv_mean_acc\": float(mean_acc),\n",
    "        \"cv_std\": float(np.std(fold_accs)),\n",
    "        \"per_fold\": [float(x) for x in fold_accs],\n",
    "    })\n",
    "\n",
    "# Rank by mean CV acc (desc), tie-break by lower std\n",
    "df_hd = (\n",
    "    pd.DataFrame(results_hd)\n",
    "      .sort_values([\"cv_mean_acc\", \"cv_std\"], ascending=[False, True])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "best_hidden_dim = int(df_hd.loc[0, \"hidden_dim\"])\n",
    "\n",
    "print(\"\\nðŸ† Best hidden_dim configuration (K-Fold):\")\n",
    "print(df_hd.loc[0, [\"hidden_dim\",\"cv_mean_acc\",\"cv_std\"]])\n",
    "print(f\"\\nSaved var -> best_hidden_dim={best_hidden_dim}\")\n",
    "\n",
    "print(\"\\nAll hidden_dim results:\\n\", df_hd)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc4002-nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
