{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading train_5500.label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\trec\\train_5500.label: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 336k/336k [00:01<00:00, 276kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading TREC_10.label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\trec\\TREC_10.label: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23.4k/23.4k [00:00<00:00, 107kB/s] \n"
     ]
    }
   ],
   "source": [
    "# For tokenization\n",
    "TEXT = data.Field(tokenize='spacy',\n",
    "    tokenizer_language='en_core_web_sm',\n",
    "    batch_first=True,\n",
    "    fix_length=50,\n",
    "    lower=True,\n",
    "    unk_token='<unk>',\n",
    "    pad_token='<pad>'\n",
    "    )\n",
    "\n",
    "# For multi-class classification labels\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "# Load the TREC dataset\n",
    "train_data, test_data = datasets.TREC.splits(TEXT, LABEL, fine_grained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ABBR', 'LOC', 'ENTY', 'NUM', 'DESC', 'HUM'}\n"
     ]
    }
   ],
   "source": [
    "label_set = set()\n",
    "for i in train_data:\n",
    "    label_set.add(i.label)\n",
    "\n",
    "print(label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.6B.zip: 862MB [06:11, 2.32MB/s]                               \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 399999/400000 [00:10<00:00, 37352.49it/s]\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "vocab = TEXT.vocab\n",
    "vectors = vocab.vectors.clone()\n",
    "glove_vocab = set(vocab.stoi.keys())\n",
    "\n",
    "# Subword parameters\n",
    "ngram_min = 3\n",
    "ngram_max = 6\n",
    "\n",
    "# Count word frequency in training data\n",
    "word_counter = Counter()\n",
    "for example in train_data:\n",
    "    word_counter.update([w.lower().strip(string.punctuation) for w in example.text])\n",
    "\n",
    "# Threshold to consider a word ‚Äúfrequent‚Äù (adjustable)\n",
    "freq_threshold = 3\n",
    "\n",
    "# <unk> vector\n",
    "unk_vector = torch.zeros(embedding_dim)\n",
    "\n",
    "def get_subwords(word, n_min=3, n_max=6):\n",
    "    word = f\"<{word.lower()}>\"\n",
    "    subwords = []\n",
    "    for n in range(n_min, n_max+1):\n",
    "        subwords += [word[i:i+n] for i in range(len(word)-n+1)]\n",
    "    return subwords\n",
    "\n",
    "def get_word_vector(word):\n",
    "    w_clean = word.lower().strip(string.punctuation)\n",
    "    \n",
    "    if w_clean in glove_vocab:\n",
    "        return vectors[vocab.stoi[w_clean]]\n",
    "    \n",
    "    # Subword averaging\n",
    "    subwords = get_subwords(w_clean, ngram_min, ngram_max)\n",
    "    subword_vecs = [vectors[vocab.stoi[sg]] for sg in subwords if sg in glove_vocab]\n",
    "    if subword_vecs:\n",
    "        return torch.stack(subword_vecs).mean(0)\n",
    "    \n",
    "    # Random vector for frequent OOVs\n",
    "    if word_counter[w_clean] >= freq_threshold:\n",
    "        return torch.randn(embedding_dim)\n",
    "    \n",
    "    # <unk> for rare OOVs\n",
    "    return unk_vector\n",
    "\n",
    "# Build embedding matrix\n",
    "embedding_matrix = {}\n",
    "for example in train_data:\n",
    "    for w in example.text:\n",
    "        if w not in embedding_matrix:\n",
    "            embedding_matrix[w] = get_word_vector(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer created with shape: torch.Size([8536, 100])\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "vocab = TEXT.vocab\n",
    "\n",
    "# Create tensor for nn.Embedding\n",
    "embedding_matrix_tensor = torch.zeros(len(vocab), embedding_dim)\n",
    "for word, idx in vocab.stoi.items():\n",
    "    if word in embedding_matrix:\n",
    "        embedding_matrix_tensor[idx] = embedding_matrix[word]\n",
    "\n",
    "# Create embedding layer (learnable)\n",
    "embedding_layer = nn.Embedding.from_pretrained(embedding_matrix_tensor, freeze=False)\n",
    "print(\"Embedding layer created with shape:\", embedding_layer.weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4416, Valid: 491, Test: 500\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1. Setup & Config\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume TEXT, LABEL, train_data, test_data, embedding_layer already exist\n",
    "train_data, valid_data = train_data.split(split_ratio=0.9, random_state=random.seed(SEED))\n",
    "print(f\"Train: {len(train_data)}, Valid: {len(valid_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "embedding_dim = embedding_layer.weight.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2. Model Definition\n",
    "# -----------------------------\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim, num_layers, bidirectional, dropout, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        emb_dim = self.embedding.weight.size(1)\n",
    "        self.rnn = nn.GRU(\n",
    "            emb_dim, hidden_dim, num_layers=num_layers,\n",
    "            batch_first=True, bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), num_classes)\n",
    "\n",
    "    def forward(self, text):\n",
    "        x = self.embedding(text)\n",
    "        outputs, hidden = self.rnn(x)\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1]\n",
    "        out = self.dropout(hidden)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2. Model Definition\n",
    "# -----------------------------\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim, num_layers, bidirectional, dropout, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        emb_dim = self.embedding.weight.size(1)\n",
    "        self.rnn = nn.GRU(\n",
    "            emb_dim, hidden_dim, num_layers=num_layers,\n",
    "            batch_first=True, bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), num_classes)\n",
    "\n",
    "    def forward(self, text):\n",
    "        x = self.embedding(text)\n",
    "        outputs, hidden = self.rnn(x)\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1]\n",
    "        out = self.dropout(hidden)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. Training Function\n",
    "# -----------------------------\n",
    "def epoch_run(iterator, model, criterion, optimizer=None):\n",
    "    model.train(optimizer is not None)\n",
    "    total_loss, total_correct, total_count = 0, 0, 0\n",
    "\n",
    "    for batch in iterator:\n",
    "        text, labels = batch.text, batch.label\n",
    "        logits = model(text)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "\n",
    "    return total_loss / total_count, total_correct / total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing BATCH=32, LR=0.001\n",
      "Epoch 01: train_acc=0.6689, val_acc=0.8513\n",
      "Epoch 02: train_acc=0.8678, val_acc=0.8758\n",
      "Epoch 03: train_acc=0.9201, val_acc=0.9002\n",
      "Epoch 04: train_acc=0.9536, val_acc=0.8982\n",
      "Epoch 05: train_acc=0.9764, val_acc=0.9124\n",
      "Epoch 06: train_acc=0.9851, val_acc=0.9104\n",
      "Epoch 07: train_acc=0.9914, val_acc=0.9145\n",
      "Epoch 08: train_acc=0.9950, val_acc=0.8941\n",
      "Epoch 09: train_acc=0.9964, val_acc=0.9022\n",
      "\n",
      "üîç Testing BATCH=32, LR=0.002\n",
      "Epoch 01: train_acc=0.7335, val_acc=0.8656\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 4. Grid Search Loop\n",
    "# -----------------------------\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.3\n",
    "NUM_CLASSES = len(LABEL.vocab)\n",
    "EPOCHS = 10   # keep short for tuning\n",
    "PATIENT_STOP = 2\n",
    "\n",
    "batch_sizes = [32, 64, 128]\n",
    "learning_rates = [1e-3, 2e-3, 3e-3, 5e-4]\n",
    "results = []\n",
    "\n",
    "for batch_size, lr in itertools.product(batch_sizes, learning_rates):\n",
    "    print(f\"\\nüîç Testing BATCH={batch_size}, LR={lr}\")\n",
    "    # Build iterators\n",
    "    train_iter, valid_iter = data.BucketIterator.splits(\n",
    "        (train_data, valid_data),\n",
    "        batch_size=batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Rebuild embedding layer (fresh weights each trial)\n",
    "    emb_layer = nn.Embedding.from_pretrained(\n",
    "        embedding_layer.weight.data.clone(),\n",
    "        freeze=False, padding_idx=PAD_IDX\n",
    "    )\n",
    "\n",
    "    model = RNNClassifier(\n",
    "        embedding_layer=emb_layer,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        bidirectional=BIDIRECTIONAL,\n",
    "        dropout=DROPOUT,\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc = epoch_run(train_iter, model, criterion, optimizer)\n",
    "        val_loss, val_acc = epoch_run(valid_iter, model, criterion)\n",
    "        print(f\"Epoch {epoch+1:02d}: train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENT_STOP:\n",
    "                break\n",
    "\n",
    "    results.append({\n",
    "        \"batch_size\": batch_size,\n",
    "        \"lr\": lr,\n",
    "        \"val_acc\": best_val_acc\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. Display Results\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "df_results = pd.DataFrame(results)\n",
    "best_row = df_results.loc[df_results[\"val_acc\"].idxmax()]\n",
    "print(\"\\nüèÜ Best configuration:\")\n",
    "print(best_row)\n",
    "print(\"\\nAll results:\\n\", df_results.sort_values(\"val_acc\", ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc4002-nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
